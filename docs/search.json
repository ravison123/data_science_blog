[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPreference Alignment using DPO\n\n\n\n\n\n\ntrl\n\n\nFine-Tuning\n\n\n\nA hands on guide to perform preference alignment of Large Language Models using Direct Preference Optimization (DPO) algorithm\n\n\n\n\n\nJan 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised Fine Tuning\n\n\n\n\n\n\ntrl\n\n\nFine-Tuning\n\n\n\nA hands on guide to fine-tune a Large Language model using trl library\n\n\n\n\n\nDec 23, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/code_fine_tuning.html",
    "href": "posts/code_fine_tuning.html",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "Before understanding what SFT is, we should understand what pre-training is. Pre-training involves training a model (generally a transformer) on a large corpus of text. Pre-training allows models to learn generalizable knowledge, grammar, semantics etc. However, the model is hardly usable after pre-training; as the model lacks task specific expertize.\n\n\nThat’s where Supervised Training plays a part. Supervised Fine-Tuning is used to adapt a pre-trained model to a specific task. It involves training the model on a labeled dataset, where the model learns to predict the correct label for each input.\n\n\nIn this article, we will load a pre-trained model from HuggingFace and finetune it on a specific dataset about Python programming.\n\n\n\n\nWe need transformers, trl and datasets packages. We will install the packages using pip:\n\n!pip install -q transformers==4.47.1 trl==0.13.0 datasets==3.2.0\n\n\n\n\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom datasets import load_dataset\nimport os\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n\n\n\n\nWe will load SmolLM2-135M model and it’s tokenizer from HuggingFace. After that, we will generate output of the model on a few specific input prompts related to Python. First, we ask a simple question of writing Python code for finding square of a number. After that, we ask model to determine if a string is palindrome (slightly difficult question than the first one).\n\n\nAs we can see, the model did not provide correct output in both the cases. The model generates some irrelevant text and keep repeating that afterwards. Let’s see how finetuning this model on task-specific dataset will help in increasing the model correctness.\n\n\nmodel_name = 'HuggingFaceTB/SmolLM2-135M'\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\ninput = 'Generate Python code for finding square of a number'\ntokenized_input = tokenizer(input, return_tensors='pt').to(device)\nmodel = model.to(device)\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nGenerate Python code for finding square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a\n\n\n\ninput = 'Write a program in Python to determine if a string is a palindrome'\ntokenized_input = tokenizer(input, return_tensors='pt').to(device)\nmodel = model.to(device)\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = model.generate(**tokenized_input, max_length=256)\nprint(tokenizer.decode(output[0]))\n\nWrite a program in Python to determine if a string is a palindrome.\n\n```python\ndef is_palindrome(s):\n    # Check if the string is a palindrome\n    if not s:\n        return False\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isdigit():\n        return True\n\n    # Check if the string is a letter\n    if s.islower():\n        return True\n\n    # Check if the string is a letter\n    if s.isupper():\n        return True\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isnumeric():\n        return True\n\n    # Check if the string is a letter\n    if s.islower():\n        return True\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isnumeric():\n        return True\n\n    # Check if the string is a letter\n    if s.isupper():\n        return True\n\n    # Check if the string is a\n\n\n\n\n\n\nWe will only load a fraction of data (3 %) for the ease of training with a single GPU.\nTrain dataset has around 15000 examples and test / evaluation dataset has 1679 examples.\n\n\ndataset_path = 'jtatman/python-code-dataset-500k'\nds = load_dataset(dataset_path, split='train[:3%]')\nds = ds.train_test_split(test_size=0.1)\ntrain_dataset = ds['train']\ntest_dataset = ds['test']\n\n\nlen(train_dataset)\n\n15106\n\n\n\nlen(test_dataset)\n\n1679\n\n\n\ntrain_dataset\n\nDataset({\n    features: ['output', 'instruction', 'system'],\n    num_rows: 15106\n})\n\n\n\n\n\n\nAs we can see from above, the dataset has 3 columns: system, instruction and output.\nFor finetuning the model, we need to combine the 3 columns into a single text field (which is achieved with formatting_function function)\n\n\ndef formatting_function(example):\n    output_texts = []\n    for i in range(len(example['instruction'])):\n        individual_prompt = \"###System: {}\\n###Instruction: {}\\n ###Output: {}\".format(example['system'][i],\n                                                                                       example['instruction'][i],\n                                                                                       example['output'][i])\n        output_texts.append(individual_prompt)\n    return output_texts\n\n\n\n\n\nWe also need to define a collator which needs response template and tokenizer as input.\nResponse template will help the trainer to identify output within the training prompt. If the training examples do not contain the response template, trainer will error out.\n\n\nresponse_template = \" ###Output:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\n\n\n\n\n\n\ntrl library provides SFTConfig class using which we can provide training arguements.\nWe have used train and eval batch size of 2 due to memory constraints.\nWe have also used Gradient Accumulation and Gradient Checkpointing for memory optimization.\nIn addition, we have also used bf16=True in an effort to further optimize memory.\n\n\ntrainer_config = SFTConfig(\n    output_dir='.\\code_finetuned_mode',\n    max_steps=1000,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    logging_steps=10,\n    ignore_data_skip=True,\n    run_name='code_fine_tuning_23_12_2024',\n    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch\n    # Memory optimization\n    gradient_checkpointing=True,  # Trade compute for memory savings\n    optim=\"adamw_torch_fused\",\n    bf16=True\n)\n\n\n\n\n\nWe will empty cache before starting training to optimize memory.\ntrl library has SFTTrainer class to which we need to provide model, training arguments, training data, evaluation data, formatting function and data collator.\nTraining information will get logged to Weights and Biases.\nWe will save the trained model to a local directory.\n\n\ntorch.cuda.empty_cache()\n\n\ntrainer = SFTTrainer(\n    model=model,\n    args=trainer_config,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    formatting_func=formatting_function,\n    data_collator=collator\n)\ntrainer.train()\n\n\n\n\n\n\n\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n\n\n\n    \n      \n      \n      [ 101/1000 02:56 &lt; 26:41, 0.56 it/s, Epoch 0.03/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n\n\n    \n      \n      \n      [394/840 01:38 &lt; 01:52, 3.97 it/s]\n    \n    \n\n\n\n    \n      \n      \n      [1000/1000 1:05:45, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n100\n0.743100\n0.728401\n\n\n200\n0.835200\n0.708747\n\n\n300\n0.641200\n0.696030\n\n\n400\n0.726100\n0.687549\n\n\n500\n0.691100\n0.681275\n\n\n600\n0.687000\n0.676701\n\n\n700\n0.682500\n0.673061\n\n\n800\n0.701100\n0.670183\n\n\n900\n0.672400\n0.668563\n\n\n1000\n0.679100\n0.668029\n\n\n\n\n\n\nTrainOutput(global_step=1000, training_loss=0.7042460036277771, metrics={'train_runtime': 3947.6465, 'train_samples_per_second': 1.013, 'train_steps_per_second': 0.253, 'total_flos': 1717373605593600.0, 'train_loss': 0.7042460036277771, 'epoch': 0.26479544551833706})\n\n\n\n\n\n\n\n\nW&B Chart 12_23_2024, 3_26_48 AM.png\n\n\n\n\n\n\n\n\nW&B Chart 12_23_2024, 3_27_33 AM.png\n\n\n\nfinetune_name = 'code_model_finetuned'\n\n\ntrainer.save_model(f'./{finetune_name}')\n\n\n\n\n\n\nAs we can notice, the finetuned model has answered first question correctly.\nThe model tried to answer second question. However, the answer is not correct. However, the output is quite better than the non fine-tuned model. It has written correct to determine if a string is palindrome when it’s length is 2 :).\nDue to resource constraints, we used limited data and performed training for 1000 steps only. Considering that, I would say that the finetuned model is much better at Python coding than the baseline model.\n\n\nfinetuned_model = AutoModelForCausalLM.from_pretrained(f'./{finetune_name}', local_files_only=True)\n\n\ninput = \"\"\"##System: You are a Python code generator, \ncapable of creating scripts from specifications.\\n\n#Instruction: Generate Python code for finding square of a number\"\"\"\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\n##System: You are a Python code generator, capable of creating scripts from specifications.\n\n#Instruction: Generate Python code for finding square of a number\n\nGiven a number, find the square of that number.\n\nExample:\n\n```python\nnumber = 10\nsquare_number = number ** 2\nprint(square_number)\n```\n\nOutput:\n```\n10\n```\n\nExplanation:\n\n```python\nnumber = 10\nsquare_number = number ** 2\nprint(square_number)\n```\n\nOutput:\n```\n10\n\n\n\ninput = \"\"\"##System: You are a Python code generator, \ncapable of creating scripts from specifications.\\n\n#Instruction: Write a program in Python to determine if a string is a palindrome\n\"\"\"\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=256)\nprint(tokenizer.decode(output[0]))\n\n##System: You are a Python code generator, capable of creating scripts from specifications.\n\n#Instruction: Write a program in Python to determine if a string is a palindrome\n\nA palindrome is a word or a string that reads the same backward and forward. For example, the word \"hello\" is a palindrome because it reads the same backward and forward.\n\nHere's a program that determines if a string is a palindrome:\n\n```python\ndef is_palindrome(string):\n    # Check if the string is empty\n    if len(string) == 0:\n        return False\n\n    # Check if the string is a word\n    if len(string) == 2:\n        # Check if the first character is the same as the last character\n        if string[0] == string[-1]:\n            return True\n        else:\n            return False\n\n    # Check if the string is a word\n    if len(string) == 1:\n        return string[0] == string[-1]\n\n    # Check if the string is a word\n    if string[0] != string[-1]:\n        return False\n\n    # Check if the string is a word\n    if string[0] != string[-1\n\n\n\n\n\n\ntrl library provides high level and quite simple APIs to finetune a Large Language model.\nFollowing are Pros and Cons of the finetuning approach described in this article:\n\nPros:\n\nImproved Task Performance: Fine-tuning enables a model to specialize in specific tasks, often achieving better accuracy compared to general-purpose models.\nData Efficiency: Requires much less labeled data compared to training from scratch due to pre-trained knowledge.\nCustomizability: Allows tailoring models to domain-specific or task-specific needs (e.g., medical, legal, or financial contexts).\n\nCons:\n\nRisk of Overfitting: On small datasets, the model may overfit, reducing its ability to generalize to unseen data.\nCatastrophic Forgetting: Fine-tuning can overwrite pre-trained knowledge, reducing performance on general tasks.\nResource Requirements: Fine-tuning still demands significant computational resources, especially for large models, compared to lightweight alternatives like prompt engineering.\nUpdation of All Parameters: The finetuning approach described in this article updates all parameters of the model. Therefore, storing of the updated model is quite costly. Therefore, more efficient approaches of finetuning like PEFT and LoRA are preferred.\n\n\n\n\n\n\n\nTRL (Transformer Reinforcement Learning) Documentation:\nSmolLM2-135M Model on HuggingFace\nDataset for Training"
  },
  {
    "objectID": "posts/code_fine_tuning.html#introduction-what-is-supervised-fine-tuning-sft",
    "href": "posts/code_fine_tuning.html#introduction-what-is-supervised-fine-tuning-sft",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "Before understanding what SFT is, we should understand what pre-training is. Pre-training involves training a model (generally a transformer) on a large corpus of text. Pre-training allows models to learn generalizable knowledge, grammar, semantics etc. However, the model is hardly usable after pre-training; as the model lacks task specific expertize.\n\n\nThat’s where Supervised Training plays a part. Supervised Fine-Tuning is used to adapt a pre-trained model to a specific task. It involves training the model on a labeled dataset, where the model learns to predict the correct label for each input.\n\n\nIn this article, we will load a pre-trained model from HuggingFace and finetune it on a specific dataset about Python programming."
  },
  {
    "objectID": "posts/code_fine_tuning.html#install-required-packages",
    "href": "posts/code_fine_tuning.html#install-required-packages",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "We need transformers, trl and datasets packages. We will install the packages using pip:\n\n!pip install -q transformers==4.47.1 trl==0.13.0 datasets==3.2.0"
  },
  {
    "objectID": "posts/code_fine_tuning.html#import-packages",
    "href": "posts/code_fine_tuning.html#import-packages",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom datasets import load_dataset\nimport os\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
  },
  {
    "objectID": "posts/code_fine_tuning.html#load-a-pretrained-model",
    "href": "posts/code_fine_tuning.html#load-a-pretrained-model",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "We will load SmolLM2-135M model and it’s tokenizer from HuggingFace. After that, we will generate output of the model on a few specific input prompts related to Python. First, we ask a simple question of writing Python code for finding square of a number. After that, we ask model to determine if a string is palindrome (slightly difficult question than the first one).\n\n\nAs we can see, the model did not provide correct output in both the cases. The model generates some irrelevant text and keep repeating that afterwards. Let’s see how finetuning this model on task-specific dataset will help in increasing the model correctness.\n\n\nmodel_name = 'HuggingFaceTB/SmolLM2-135M'\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\ninput = 'Generate Python code for finding square of a number'\ntokenized_input = tokenizer(input, return_tensors='pt').to(device)\nmodel = model.to(device)\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nGenerate Python code for finding square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a\n\n\n\ninput = 'Write a program in Python to determine if a string is a palindrome'\ntokenized_input = tokenizer(input, return_tensors='pt').to(device)\nmodel = model.to(device)\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = model.generate(**tokenized_input, max_length=256)\nprint(tokenizer.decode(output[0]))\n\nWrite a program in Python to determine if a string is a palindrome.\n\n```python\ndef is_palindrome(s):\n    # Check if the string is a palindrome\n    if not s:\n        return False\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isdigit():\n        return True\n\n    # Check if the string is a letter\n    if s.islower():\n        return True\n\n    # Check if the string is a letter\n    if s.isupper():\n        return True\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isnumeric():\n        return True\n\n    # Check if the string is a letter\n    if s.islower():\n        return True\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isnumeric():\n        return True\n\n    # Check if the string is a letter\n    if s.isupper():\n        return True\n\n    # Check if the string is a"
  },
  {
    "objectID": "posts/code_fine_tuning.html#load-dataset",
    "href": "posts/code_fine_tuning.html#load-dataset",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "We will only load a fraction of data (3 %) for the ease of training with a single GPU.\nTrain dataset has around 15000 examples and test / evaluation dataset has 1679 examples.\n\n\ndataset_path = 'jtatman/python-code-dataset-500k'\nds = load_dataset(dataset_path, split='train[:3%]')\nds = ds.train_test_split(test_size=0.1)\ntrain_dataset = ds['train']\ntest_dataset = ds['test']\n\n\nlen(train_dataset)\n\n15106\n\n\n\nlen(test_dataset)\n\n1679\n\n\n\ntrain_dataset\n\nDataset({\n    features: ['output', 'instruction', 'system'],\n    num_rows: 15106\n})"
  },
  {
    "objectID": "posts/code_fine_tuning.html#define-a-formatting-function",
    "href": "posts/code_fine_tuning.html#define-a-formatting-function",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "As we can see from above, the dataset has 3 columns: system, instruction and output.\nFor finetuning the model, we need to combine the 3 columns into a single text field (which is achieved with formatting_function function)\n\n\ndef formatting_function(example):\n    output_texts = []\n    for i in range(len(example['instruction'])):\n        individual_prompt = \"###System: {}\\n###Instruction: {}\\n ###Output: {}\".format(example['system'][i],\n                                                                                       example['instruction'][i],\n                                                                                       example['output'][i])\n        output_texts.append(individual_prompt)\n    return output_texts"
  },
  {
    "objectID": "posts/code_fine_tuning.html#define-a-collator",
    "href": "posts/code_fine_tuning.html#define-a-collator",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "We also need to define a collator which needs response template and tokenizer as input.\nResponse template will help the trainer to identify output within the training prompt. If the training examples do not contain the response template, trainer will error out.\n\n\nresponse_template = \" ###Output:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
  },
  {
    "objectID": "posts/code_fine_tuning.html#training",
    "href": "posts/code_fine_tuning.html#training",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "trl library provides SFTConfig class using which we can provide training arguements.\nWe have used train and eval batch size of 2 due to memory constraints.\nWe have also used Gradient Accumulation and Gradient Checkpointing for memory optimization.\nIn addition, we have also used bf16=True in an effort to further optimize memory.\n\n\ntrainer_config = SFTConfig(\n    output_dir='.\\code_finetuned_mode',\n    max_steps=1000,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    logging_steps=10,\n    ignore_data_skip=True,\n    run_name='code_fine_tuning_23_12_2024',\n    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch\n    # Memory optimization\n    gradient_checkpointing=True,  # Trade compute for memory savings\n    optim=\"adamw_torch_fused\",\n    bf16=True\n)\n\n\n\n\n\nWe will empty cache before starting training to optimize memory.\ntrl library has SFTTrainer class to which we need to provide model, training arguments, training data, evaluation data, formatting function and data collator.\nTraining information will get logged to Weights and Biases.\nWe will save the trained model to a local directory.\n\n\ntorch.cuda.empty_cache()\n\n\ntrainer = SFTTrainer(\n    model=model,\n    args=trainer_config,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    formatting_func=formatting_function,\n    data_collator=collator\n)\ntrainer.train()\n\n\n\n\n\n\n\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n\n\n\n    \n      \n      \n      [ 101/1000 02:56 &lt; 26:41, 0.56 it/s, Epoch 0.03/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n\n\n    \n      \n      \n      [394/840 01:38 &lt; 01:52, 3.97 it/s]\n    \n    \n\n\n\n    \n      \n      \n      [1000/1000 1:05:45, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n100\n0.743100\n0.728401\n\n\n200\n0.835200\n0.708747\n\n\n300\n0.641200\n0.696030\n\n\n400\n0.726100\n0.687549\n\n\n500\n0.691100\n0.681275\n\n\n600\n0.687000\n0.676701\n\n\n700\n0.682500\n0.673061\n\n\n800\n0.701100\n0.670183\n\n\n900\n0.672400\n0.668563\n\n\n1000\n0.679100\n0.668029\n\n\n\n\n\n\nTrainOutput(global_step=1000, training_loss=0.7042460036277771, metrics={'train_runtime': 3947.6465, 'train_samples_per_second': 1.013, 'train_steps_per_second': 0.253, 'total_flos': 1717373605593600.0, 'train_loss': 0.7042460036277771, 'epoch': 0.26479544551833706})\n\n\n\n\n\n\n\n\nW&B Chart 12_23_2024, 3_26_48 AM.png\n\n\n\n\n\n\n\n\nW&B Chart 12_23_2024, 3_27_33 AM.png\n\n\n\nfinetune_name = 'code_model_finetuned'\n\n\ntrainer.save_model(f'./{finetune_name}')"
  },
  {
    "objectID": "posts/code_fine_tuning.html#load-the-finetuned-model-and-perform-prediction-with-it",
    "href": "posts/code_fine_tuning.html#load-the-finetuned-model-and-perform-prediction-with-it",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "As we can notice, the finetuned model has answered first question correctly.\nThe model tried to answer second question. However, the answer is not correct. However, the output is quite better than the non fine-tuned model. It has written correct to determine if a string is palindrome when it’s length is 2 :).\nDue to resource constraints, we used limited data and performed training for 1000 steps only. Considering that, I would say that the finetuned model is much better at Python coding than the baseline model.\n\n\nfinetuned_model = AutoModelForCausalLM.from_pretrained(f'./{finetune_name}', local_files_only=True)\n\n\ninput = \"\"\"##System: You are a Python code generator, \ncapable of creating scripts from specifications.\\n\n#Instruction: Generate Python code for finding square of a number\"\"\"\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\n##System: You are a Python code generator, capable of creating scripts from specifications.\n\n#Instruction: Generate Python code for finding square of a number\n\nGiven a number, find the square of that number.\n\nExample:\n\n```python\nnumber = 10\nsquare_number = number ** 2\nprint(square_number)\n```\n\nOutput:\n```\n10\n```\n\nExplanation:\n\n```python\nnumber = 10\nsquare_number = number ** 2\nprint(square_number)\n```\n\nOutput:\n```\n10\n\n\n\ninput = \"\"\"##System: You are a Python code generator, \ncapable of creating scripts from specifications.\\n\n#Instruction: Write a program in Python to determine if a string is a palindrome\n\"\"\"\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=256)\nprint(tokenizer.decode(output[0]))\n\n##System: You are a Python code generator, capable of creating scripts from specifications.\n\n#Instruction: Write a program in Python to determine if a string is a palindrome\n\nA palindrome is a word or a string that reads the same backward and forward. For example, the word \"hello\" is a palindrome because it reads the same backward and forward.\n\nHere's a program that determines if a string is a palindrome:\n\n```python\ndef is_palindrome(string):\n    # Check if the string is empty\n    if len(string) == 0:\n        return False\n\n    # Check if the string is a word\n    if len(string) == 2:\n        # Check if the first character is the same as the last character\n        if string[0] == string[-1]:\n            return True\n        else:\n            return False\n\n    # Check if the string is a word\n    if len(string) == 1:\n        return string[0] == string[-1]\n\n    # Check if the string is a word\n    if string[0] != string[-1]:\n        return False\n\n    # Check if the string is a word\n    if string[0] != string[-1"
  },
  {
    "objectID": "posts/code_fine_tuning.html#conclusion",
    "href": "posts/code_fine_tuning.html#conclusion",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "trl library provides high level and quite simple APIs to finetune a Large Language model.\nFollowing are Pros and Cons of the finetuning approach described in this article:\n\nPros:\n\nImproved Task Performance: Fine-tuning enables a model to specialize in specific tasks, often achieving better accuracy compared to general-purpose models.\nData Efficiency: Requires much less labeled data compared to training from scratch due to pre-trained knowledge.\nCustomizability: Allows tailoring models to domain-specific or task-specific needs (e.g., medical, legal, or financial contexts).\n\nCons:\n\nRisk of Overfitting: On small datasets, the model may overfit, reducing its ability to generalize to unseen data.\nCatastrophic Forgetting: Fine-tuning can overwrite pre-trained knowledge, reducing performance on general tasks.\nResource Requirements: Fine-tuning still demands significant computational resources, especially for large models, compared to lightweight alternatives like prompt engineering.\nUpdation of All Parameters: The finetuning approach described in this article updates all parameters of the model. Therefore, storing of the updated model is quite costly. Therefore, more efficient approaches of finetuning like PEFT and LoRA are preferred."
  },
  {
    "objectID": "posts/code_fine_tuning.html#references",
    "href": "posts/code_fine_tuning.html#references",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "TRL (Transformer Reinforcement Learning) Documentation:\nSmolLM2-135M Model on HuggingFace\nDataset for Training"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Ravindra. I’m passionate about Data Science, Machine Learning, and Generative AI, and I believe, as Andrew Ng says, “AI is the new electricity.” This blog focuses on the practical side of these fields—how to use data and AI to solve real-world problems and drive impact. Join me as I explore hands-on projects, share insights, and dive into the tools and techniques shaping the future.\nFeel free to connect with me on LinkedIn or explore my projects on GitHub."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring the World of AI and Data Science",
    "section": "",
    "text": "Welcome to my blog, where I explore the exciting world of Data Science, Machine Learning, and Generative AI. My aim is to simplify complex concepts, share the latest trends, and inspire others to dive deeper into these transformative technologies. Explore my latest posts for insights, tutorials, and discussions on the future of AI!"
  },
  {
    "objectID": "posts/preference_alignment.html",
    "href": "posts/preference_alignment.html",
    "title": "Preference Alignment using DPO",
    "section": "",
    "text": "There are generally 3 stages in the LLM development lifecycle:\n\nPre-training: This is the foundation of LLM development where the model is trained on massive text dataset to predict next word in a sequence. The output of this step is a base model which has developed broad understanding of language. However, the model will not be of much help as it may just predict next token / word which might not make much sense. That’s why the next step of Supervised Fine-tuning is required.\nSupervised Fine-tuning (SFT): SFT is further training of base LLM on a smaller and more specific dataset. The dataset typicallly consists of input-output pairs. During SFT, model learns to generate outputs that are more relevant to the task / end user. Large foundation models are typically trained to follow instructions using a specific type of SFT called as instruction tuning.\nPreference Alignment: Building on the SFT model, preference alignment further refines the LLM’s behavior to ensure it aligns with human preferences or values. You must have noticed that ChatGPT (or any other foundational LLM model) deny answering harmful questions. This behaviour of the model is achieved after refining the model through Performance Alignment methods.\n\nTraditionally, Preference Alignment is achieved using Reinforcement Learning with Human Feedback (RLHF). RLHF involves training a reward model using reinforcement learning which helps model to align with human preferences. RLHF is quite complex as it requires dealing with complex reinforcement learning algorithms which are unstable. Thankfully, there are other techniques apart from RLHF which can be used for preference alignment.\nIn this blog, we will dive into hands-on implementation of one such technique which is Direct Preference Optimization (DPO)."
  },
  {
    "objectID": "posts/preference_alignment.html#what-is-preference-alignment-of-large-language-models",
    "href": "posts/preference_alignment.html#what-is-preference-alignment-of-large-language-models",
    "title": "Preference Alignment using DPO",
    "section": "",
    "text": "There are generally 3 stages in the LLM development lifecycle:\n\nPre-training: This is the foundation of LLM development where the model is trained on massive text dataset to predict next word in a sequence. The output of this step is a base model which has developed broad understanding of language. However, the model will not be of much help as it may just predict next token / word which might not make much sense. That’s why the next step of Supervised Fine-tuning is required.\nSupervised Fine-tuning (SFT): SFT is further training of base LLM on a smaller and more specific dataset. The dataset typicallly consists of input-output pairs. During SFT, model learns to generate outputs that are more relevant to the task / end user. Large foundation models are typically trained to follow instructions using a specific type of SFT called as instruction tuning.\nPreference Alignment: Building on the SFT model, preference alignment further refines the LLM’s behavior to ensure it aligns with human preferences or values. You must have noticed that ChatGPT (or any other foundational LLM model) deny answering harmful questions. This behaviour of the model is achieved after refining the model through Performance Alignment methods.\n\nTraditionally, Preference Alignment is achieved using Reinforcement Learning with Human Feedback (RLHF). RLHF involves training a reward model using reinforcement learning which helps model to align with human preferences. RLHF is quite complex as it requires dealing with complex reinforcement learning algorithms which are unstable. Thankfully, there are other techniques apart from RLHF which can be used for preference alignment.\nIn this blog, we will dive into hands-on implementation of one such technique which is Direct Preference Optimization (DPO)."
  },
  {
    "objectID": "posts/preference_alignment.html#dpo",
    "href": "posts/preference_alignment.html#dpo",
    "title": "Preference Alignment using DPO",
    "section": "DPO",
    "text": "DPO\nDPO models preference alignment as a classification problem on preference data. Preference dataset contains positive and negative pairs of prompt completion / generation. Following figure from DPO paper summarizes the difference between DPO and traditional RLHF:"
  },
  {
    "objectID": "posts/preference_alignment.html#import-packages",
    "href": "posts/preference_alignment.html#import-packages",
    "title": "Preference Alignment using DPO",
    "section": "Import Packages",
    "text": "Import Packages\n\nfrom datasets import load_dataset\nfrom trl import DPOConfig, DPOTrainer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model, PeftModel\nfrom trl import extract_prompt\nimport wandb"
  },
  {
    "objectID": "posts/preference_alignment.html#load-model-and-tokenizer",
    "href": "posts/preference_alignment.html#load-model-and-tokenizer",
    "title": "Preference Alignment using DPO",
    "section": "Load Model and Tokenizer",
    "text": "Load Model and Tokenizer\nIn this tutorial, we will use SmolLM2-360M-Instruct which will be finetuned using DPO for preference alignment. We are using a small model because of resource constraints. However, the same technique can be applied to large open models like Llama.\n\nmodel_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)"
  },
  {
    "objectID": "posts/preference_alignment.html#load-dataset",
    "href": "posts/preference_alignment.html#load-dataset",
    "title": "Preference Alignment using DPO",
    "section": "Load Dataset",
    "text": "Load Dataset\nWe will use a truthfullness DPO dataset from Huggingface (jondurbin/truthy-dpo-v0.1). The dataset consists of 1016 examples. Each example consists of a prompt and corresponding chosen & rejected responses. The datasets in this format are called DPO datasets. There are quite a lot of DPO datasets on Huggingface which can be explored for preference alignment of models.\n\ndataset_path = \"jondurbin/truthy-dpo-v0.1\"\n\n\ntrain_dataset = load_dataset(dataset_path, split=\"train\")\n\n\n\n\n\n\n\n\n\n\n\nlen(train_dataset)\n\n1016\n\n\n\ntrain_dataset[0]\n\n{'id': '04c275bf738fd391b7fe25e25fe7bed3',\n 'source': 'truthy_dpo',\n 'system': 'You are an unbiased, uncensored, helpful assistant.',\n 'prompt': \"What's the nearest national park to you?\",\n 'chosen': \"As an AI, I don't have a physical location, so I can't provide the distance to the nearest national park.\",\n 'rejected': \"I don't have access to the user's location, so I can't determine the nearest national park.\"}"
  },
  {
    "objectID": "posts/preference_alignment.html#generate-output-with-baseline-model",
    "href": "posts/preference_alignment.html#generate-output-with-baseline-model",
    "title": "Preference Alignment using DPO",
    "section": "Generate output with baseline model",
    "text": "Generate output with baseline model\n\nIn this section, we generate outputs from baseline model using different prompts from training dataset.\nWe have to apply chat template of the model before passing the prompts to model. If this step is skipped, the model fails to generate the outputs.\nAfter that, we have created a function which generates the output when model and prompts are provided. This function will be useful while testing the model after preference alignment step.\n\n\ninput_prompt = train_dataset[1]['prompt']\ninput_prompt\n\n'Do you possess the ability to navigate or move within a physical environment?'\n\n\n\nsystem_prompt = \"You are an unbiased, uncensored, helpful assistant.\"\n\n\ndevice = 'cuda'\nmodel = model.to(device)\n\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": system_prompt\n    },\n     {\"role\": \"user\",\n     \"content\": input_prompt}]\n\n\nmessages\n\n[{'role': 'system',\n  'content': 'You are an unbiased, uncensored, helpful assistant.'},\n {'role': 'user',\n  'content': 'Do you possess the ability to navigate or move within a physical environment?'}]\n\n\n\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\n\n\nprint(input_text)\n\n&lt;|im_start|&gt;system\nYou are an unbiased, uncensored, helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nDo you possess the ability to navigate or move within a physical environment?&lt;|im_end|&gt;\n\n\n\n\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n\n\ninputs\n\n{'input_ids': tensor([[    1,  9690,   198,  2683,   359,   354, 36417,    28,  4424,   581,\n          1851,    28,  5356, 11173,    30,     2,   198,     1,  4093,   198,\n          6248,   346,  5204,   260,  2470,   288,  6776,   355,  1485,  1127,\n           253,  2099,  1357,    47,     2,   198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n\n\n\nprompt_length = inputs['input_ids'].shape[1]\nprompt_length\n\n36\n\n\n\noutputs = model.generate(**inputs, max_new_tokens=100, temperature=0.2, top_p=0.9, do_sample=True)\n\n\noutputs\n\ntensor([[    1,  9690,   198,  2683,   359,   354, 36417,    28,  4424,   581,\n          1851,    28,  5356, 11173,    30,     2,   198,     1,  4093,   198,\n          6248,   346,  5204,   260,  2470,   288,  6776,   355,  1485,  1127,\n           253,  2099,  1357,    47,     2,   198,     1,   520,  9531,   198,\n            57,  5248,  2724,   288,  4237,  3629,   281, 10178,   284,  4138,\n          1127,   253,  2099,  1357,    28,   564,   339,  5248,   441,   253,\n          2099,  4313,    30,   339,   416,  1538,  5783,    28,  8939,    28,\n           284,   724,   351,  4381,    28,   564,   339,  1326,   982,   457,\n           260,  2470,   288,  8582,  1485,   355,  2298,   351,   260,  1357,\n            30,     2]], device='cuda:0')\n\n\n\noutput_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\nprint(output_text)\n\nassistant\nI'm designed to assist users in navigating and moving within a physical environment, but I'm not a physical presence. I can provide guidance, directions, and help with tasks, but I don't have the ability to physically move or interact with the environment.\n\n\n\ndef get_completion(model_, input_prompt, system_prompt, max_new_tokens=100, temperature=0.2, top_p=0.9):\n    messages = [\n    {\n        \"role\": \"system\",\n        \"content\": system_prompt\n    },\n     {\"role\": \"user\",\n     \"content\": input_prompt}]\n\n    input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n    prompt_length = inputs['input_ids'].shape[1]\n    outputs = model_.generate(**inputs, max_new_tokens=max_new_tokens, temperature=temperature,\n                             top_p=top_p, do_sample=True)\n    output_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n    return output_text\n\n\nget_completion(model, 'How are you?', system_prompt)\n\n\"assistant\\nI'm fine, thank you for asking.\""
  },
  {
    "objectID": "posts/preference_alignment.html#implicit-prompt-to-explicit-prompt-conversion",
    "href": "posts/preference_alignment.html#implicit-prompt-to-explicit-prompt-conversion",
    "title": "Preference Alignment using DPO",
    "section": "Implicit Prompt to Explicit Prompt Conversion",
    "text": "Implicit Prompt to Explicit Prompt Conversion\n\nDPO documentation recommends using explicit prompts. This section applies various processing steps to convert the dataset to the one with explicit prompts.\n\n\ndef preprocess_data(examples):\n  chosen_list = []\n  rejected_list = []\n\n  for i in range(len(examples['prompt'])):\n      chosen_messages = [\n          {\"role\": \"system\", \"content\": examples['system'][i]},\n          {\"role\": \"user\", \"content\": examples['prompt'][i]},\n          {\"role\": \"assistant\", \"content\": examples['chosen'][i]}\n      ]\n      chosen_list.append(chosen_messages)\n\n      rejected_messages = [\n          {\"role\": \"system\", \"content\": examples['system'][i]},\n          {\"role\": \"user\", \"content\": examples['prompt'][i]},\n          {\"role\": \"assistant\", \"content\": examples['rejected'][i]}\n      ]\n      rejected_list.append(rejected_messages)\n\n  return {'chosen': chosen_list, 'rejected': rejected_list}\n\n\ntrain_dataset = train_dataset.map(\n    preprocess_data, batched=True, remove_columns=train_dataset.column_names\n)\n\n\n\n\n\nlen(train_dataset)\n\n1016\n\n\n\ntrain_dataset[0]\n\n{'chosen': [{'content': 'You are an unbiased, uncensored, helpful assistant.',\n   'role': 'system'},\n  {'content': \"What's the nearest national park to you?\", 'role': 'user'},\n  {'content': \"As an AI, I don't have a physical location, so I can't provide the distance to the nearest national park.\",\n   'role': 'assistant'}],\n 'rejected': [{'content': 'You are an unbiased, uncensored, helpful assistant.',\n   'role': 'system'},\n  {'content': \"What's the nearest national park to you?\", 'role': 'user'},\n  {'content': \"I don't have access to the user's location, so I can't determine the nearest national park.\",\n   'role': 'assistant'}]}\n\n\n\ntrain_dataset = train_dataset.map(extract_prompt)\n\n\n\n\n\nlen(train_dataset)\n\n1016\n\n\n\ntrain_dataset[0]\n\n{'chosen': [{'content': \"As an AI, I don't have a physical location, so I can't provide the distance to the nearest national park.\",\n   'role': 'assistant'}],\n 'rejected': [{'content': \"I don't have access to the user's location, so I can't determine the nearest national park.\",\n   'role': 'assistant'}],\n 'prompt': [{'content': 'You are an unbiased, uncensored, helpful assistant.',\n   'role': 'system'},\n  {'content': \"What's the nearest national park to you?\", 'role': 'user'}]}"
  },
  {
    "objectID": "posts/preference_alignment.html#dpo-training",
    "href": "posts/preference_alignment.html#dpo-training",
    "title": "Preference Alignment using DPO",
    "section": "DPO Training",
    "text": "DPO Training\n\nWe will use LoRA (Low Rank Adaptation) to reduce the number of training parameters. We use LoRA with rank 16 and dropout of 0.05.\nWe use DPOConfig class from trl to specify training arguements. Note that we have used gradient accumulation and gradient checkpointing to optimize memory.\nWe have also enabled mixed precision training by using fp16=True.\nWe will train the model for 10000 steps (~20 Epochs) and log trainig information to Weights and Biases platform.\n\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n\ntraining_args = DPOConfig(output_dir=\"SmolLM_DPO\",\n                          logging_steps=50,\n                          per_device_train_batch_size=1,\n                          gradient_accumulation_steps=2,\n                          gradient_checkpointing=True,\n                          max_steps=10000,\n                          fp16=True,\n                          report_to=\"wandb\",\n                          run_name=\"dpo_training_run_04_01_2025\"\n                         )\n\n\nimport gc\nimport torch\n\ngc.collect()\ntorch.cuda.empty_cache()\n\n\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nwandb_api = user_secrets.get_secret(\"wandb_api\")\n\nwandb.login(key=wandb_api)\n\nwandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\nwandb: Currently logged in as: sonawane-ravindra1. Use `wandb login --relogin` to force relogin\nwandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\n\nTrue\n\n\n\ntrainer = DPOTrainer(model=model,\n                     args=training_args,\n                     processing_class=tokenizer,\n                     train_dataset=train_dataset,\n                     peft_config=peft_config)\n\n/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n\n\n\ntrainer.train()\n\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n\n\n\n    \n      \n      \n      [10000/10000 2:13:21, Epoch 19/20]\n    \n    \n\n\n\nStep\nTraining Loss\n\n\n\n\n50\n0.692900\n\n\n100\n0.692400\n\n\n150\n0.690500\n\n\n200\n0.688100\n\n\n250\n0.687200\n\n\n300\n0.684400\n\n\n350\n0.681400\n\n\n400\n0.680000\n\n\n450\n0.677900\n\n\n500\n0.674300\n\n\n550\n0.672300\n\n\n600\n0.669400\n\n\n650\n0.665000\n\n\n700\n0.666200\n\n\n750\n0.663600\n\n\n800\n0.661400\n\n\n850\n0.656800\n\n\n900\n0.652500\n\n\n950\n0.651500\n\n\n1000\n0.645300\n\n\n1050\n0.642700\n\n\n1100\n0.643000\n\n\n1150\n0.643100\n\n\n1200\n0.643200\n\n\n1250\n0.625300\n\n\n1300\n0.627800\n\n\n1350\n0.634200\n\n\n1400\n0.630200\n\n\n1450\n0.614600\n\n\n1500\n0.616000\n\n\n1550\n0.620700\n\n\n1600\n0.613400\n\n\n1650\n0.611600\n\n\n1700\n0.611300\n\n\n1750\n0.600100\n\n\n1800\n0.611600\n\n\n1850\n0.594400\n\n\n1900\n0.596300\n\n\n1950\n0.592500\n\n\n2000\n0.581800\n\n\n2050\n0.592800\n\n\n2100\n0.580700\n\n\n2150\n0.583900\n\n\n2200\n0.574700\n\n\n2250\n0.577000\n\n\n2300\n0.571800\n\n\n2350\n0.576200\n\n\n2400\n0.561200\n\n\n2450\n0.565000\n\n\n2500\n0.552500\n\n\n2550\n0.557500\n\n\n2600\n0.554000\n\n\n2650\n0.550700\n\n\n2700\n0.543400\n\n\n2750\n0.542900\n\n\n2800\n0.557800\n\n\n2850\n0.550100\n\n\n2900\n0.535100\n\n\n2950\n0.540100\n\n\n3000\n0.533000\n\n\n3050\n0.533200\n\n\n3100\n0.528700\n\n\n3150\n0.519800\n\n\n3200\n0.525800\n\n\n3250\n0.523500\n\n\n3300\n0.511900\n\n\n3350\n0.517800\n\n\n3400\n0.508900\n\n\n3450\n0.512400\n\n\n3500\n0.510800\n\n\n3550\n0.508700\n\n\n3600\n0.503600\n\n\n3650\n0.473900\n\n\n3700\n0.507300\n\n\n3750\n0.511000\n\n\n3800\n0.502300\n\n\n3850\n0.473700\n\n\n3900\n0.493700\n\n\n3950\n0.488600\n\n\n4000\n0.490700\n\n\n4050\n0.479700\n\n\n4100\n0.450500\n\n\n4150\n0.471400\n\n\n4200\n0.467400\n\n\n4250\n0.444900\n\n\n4300\n0.479300\n\n\n4350\n0.478300\n\n\n4400\n0.463000\n\n\n4450\n0.468400\n\n\n4500\n0.480500\n\n\n4550\n0.477000\n\n\n4600\n0.428500\n\n\n4650\n0.448800\n\n\n4700\n0.446100\n\n\n4750\n0.493100\n\n\n4800\n0.413700\n\n\n4850\n0.432800\n\n\n4900\n0.447700\n\n\n4950\n0.448900\n\n\n5000\n0.448200\n\n\n5050\n0.461600\n\n\n5100\n0.440700\n\n\n5150\n0.434600\n\n\n5200\n0.421900\n\n\n5250\n0.411700\n\n\n5300\n0.437900\n\n\n5350\n0.436900\n\n\n5400\n0.422800\n\n\n5450\n0.422200\n\n\n5500\n0.417200\n\n\n5550\n0.437100\n\n\n5600\n0.454300\n\n\n5650\n0.424100\n\n\n5700\n0.415300\n\n\n5750\n0.406200\n\n\n5800\n0.427800\n\n\n5850\n0.423100\n\n\n5900\n0.399200\n\n\n5950\n0.390800\n\n\n6000\n0.411200\n\n\n6050\n0.407200\n\n\n6100\n0.415300\n\n\n6150\n0.397000\n\n\n6200\n0.399000\n\n\n6250\n0.387400\n\n\n6300\n0.405600\n\n\n6350\n0.380900\n\n\n6400\n0.401300\n\n\n6450\n0.368700\n\n\n6500\n0.405500\n\n\n6550\n0.438500\n\n\n6600\n0.418900\n\n\n6650\n0.354100\n\n\n6700\n0.381000\n\n\n6750\n0.422500\n\n\n6800\n0.380900\n\n\n6850\n0.394400\n\n\n6900\n0.385500\n\n\n6950\n0.390400\n\n\n7000\n0.366100\n\n\n7050\n0.418700\n\n\n7100\n0.400400\n\n\n7150\n0.372200\n\n\n7200\n0.366000\n\n\n7250\n0.372500\n\n\n7300\n0.404300\n\n\n7350\n0.387300\n\n\n7400\n0.389300\n\n\n7450\n0.365100\n\n\n7500\n0.368300\n\n\n7550\n0.370500\n\n\n7600\n0.372700\n\n\n7650\n0.373000\n\n\n7700\n0.379000\n\n\n7750\n0.378800\n\n\n7800\n0.395400\n\n\n7850\n0.368500\n\n\n7900\n0.377500\n\n\n7950\n0.385700\n\n\n8000\n0.364700\n\n\n8050\n0.347300\n\n\n8100\n0.336800\n\n\n8150\n0.381300\n\n\n8200\n0.392100\n\n\n8250\n0.357100\n\n\n8300\n0.368500\n\n\n8350\n0.354200\n\n\n8400\n0.352300\n\n\n8450\n0.352100\n\n\n8500\n0.389400\n\n\n8550\n0.368600\n\n\n8600\n0.380800\n\n\n8650\n0.367500\n\n\n8700\n0.341200\n\n\n8750\n0.379900\n\n\n8800\n0.362000\n\n\n8850\n0.365600\n\n\n8900\n0.341500\n\n\n8950\n0.348200\n\n\n9000\n0.359400\n\n\n9050\n0.331900\n\n\n9100\n0.400300\n\n\n9150\n0.364700\n\n\n9200\n0.401100\n\n\n9250\n0.375100\n\n\n9300\n0.343500\n\n\n9350\n0.334200\n\n\n9400\n0.359000\n\n\n9450\n0.332500\n\n\n9500\n0.388500\n\n\n9550\n0.342000\n\n\n9600\n0.357400\n\n\n9650\n0.350000\n\n\n9700\n0.334800\n\n\n9750\n0.339600\n\n\n9800\n0.354400\n\n\n9850\n0.406500\n\n\n9900\n0.331100\n\n\n9950\n0.367000\n\n\n10000\n0.360200\n\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n\n\nTrainOutput(global_step=10000, training_loss=0.4737377624511719, metrics={'train_runtime': 8002.32, 'train_samples_per_second': 2.499, 'train_steps_per_second': 1.25, 'total_flos': 0.0, 'train_loss': 0.4737377624511719, 'epoch': 19.68503937007874})"
  },
  {
    "objectID": "posts/preference_alignment.html#inspect-training-logs",
    "href": "posts/preference_alignment.html#inspect-training-logs",
    "title": "Preference Alignment using DPO",
    "section": "Inspect Training Logs",
    "text": "Inspect Training Logs\n\nWe see that the training loss has decreased from 0.7 to 0.4 (which is good).\nHowever, we also need to examine 2 more plots (rewards of chosen and rejected prompts).\nAs we can see from the plots, rewards of chosen prompts is increasing and that of rejected prompts is decreasing. This indicates that the model after training is more likely to produce responses similar to chosen prompts."
  },
  {
    "objectID": "posts/preference_alignment.html#save-and-load-the-fine-tuned-model",
    "href": "posts/preference_alignment.html#save-and-load-the-fine-tuned-model",
    "title": "Preference Alignment using DPO",
    "section": "Save and Load the Fine-tuned Model",
    "text": "Save and Load the Fine-tuned Model\n\nWe will save the LoRA adapters of the fine-tuned model in a local directory.\nAfter that, we load the LoRA adapters and combine them with the baseline model.\nYou can notice that the fine-tuned model has several LoRA adapters in it.\n\n\nsave_directory = 'dpo_model'\n\n\ntrainer.model.save_pretrained(save_directory=save_directory)\n\n\nfrom peft import PeftModel\n\nfinetuned_model = PeftModel.from_pretrained(model, save_directory)\n\n\nfinetuned_model\n\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(49152, 960, padding_idx=2)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=960, out_features=960, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=960, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=960, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear(in_features=960, out_features=320, bias=False)\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=960, out_features=320, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=960, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=320, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear(in_features=960, out_features=960, bias=False)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n              (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n              (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((960,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n    )\n  )\n)"
  },
  {
    "objectID": "posts/preference_alignment.html#generate-responses-from-fine-tuned-model",
    "href": "posts/preference_alignment.html#generate-responses-from-fine-tuned-model",
    "title": "Preference Alignment using DPO",
    "section": "Generate Responses from Fine-tuned Model",
    "text": "Generate Responses from Fine-tuned Model\n\nWe can notice that responses from both the models are nearly identical. This is because of less training data (only 1000 examples) and training for less duration (due to resource constraints)\nHowever, we learned about how to apply DPO technique to ensure that the LLM models adhere to human preferences.\n\n\nsystem_prompt = train_dataset[0]['prompt'][0]['content']\ninput_prompt = train_dataset[0]['prompt'][1]['content']\nprint('Input Prompt is:')\nprint(input_prompt)\nprint()\nprint('Output of the finetuned model:')\nprint(get_completion(finetuned_model, input_prompt, system_prompt))\nprint()\nprint('Output of the baseline model')\nprint(get_completion(model, input_prompt, system_prompt))\n\nInput Prompt is:\nWhat's the nearest national park to you?\n\nOutput of the finetuned model:\nassistant\nI'm sorry, but as an AI, I don't have the ability to provide real-time location-based information. I'm designed to provide general knowledge and information, not specific location-based information. I recommend using a GPS device or a mapping service for that.\n\nOutput of the baseline model\nassistant\nI'm here to help you with any questions or inquiries you may have. I'm not affiliated with any national parks, so I'm not limited to their boundaries. If you have a specific question, feel free to ask, and I'll do my best to provide a helpful answer."
  },
  {
    "objectID": "posts/preference_alignment.html#conclusion",
    "href": "posts/preference_alignment.html#conclusion",
    "title": "Preference Alignment using DPO",
    "section": "Conclusion",
    "text": "Conclusion\n\nAlignment to human preferences / values is very important for LLM models to be safe and reliable.\nThe preference alignment has been traditionally performed using RLHF. However, RLHF is complex and quite unstable. DPO is one technique that is widely used to perform preference alignment which is simple, stable and computationally efficient.\nTRL framework developed by Huggingface provides easy APIs for implementation of DPO and many more preference alignment algorithms."
  },
  {
    "objectID": "posts/preference_alignment.html#references",
    "href": "posts/preference_alignment.html#references",
    "title": "Preference Alignment using DPO",
    "section": "References",
    "text": "References\n\nDPO Paper\nDPO Trainer documentation in trl library\nDataset used for training"
  }
]