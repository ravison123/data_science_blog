[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSupervised Fine Tuning\n\n\n\n\n\n\ntrl\n\n\nFine-Tuning\n\n\n\nA hands on guide to fine-tune a Large Language model using trl library\n\n\n\n\n\nDec 23, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring the World of AI and Data Science",
    "section": "",
    "text": "Welcome to my blog, where I explore the exciting world of Data Science, Machine Learning, and Generative AI. My aim is to simplify complex concepts, share the latest trends, and inspire others to dive deeper into these transformative technologies. Explore my latest posts for insights, tutorials, and discussions on the future of AI!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Ravindra. I’m passionate about Data Science, Machine Learning, and Generative AI, and I believe, as Andrew Ng says, “AI is the new electricity.” This blog focuses on the practical side of these fields—how to use data and AI to solve real-world problems and drive impact. Join me as I explore hands-on projects, share insights, and dive into the tools and techniques shaping the future.\nFeel free to connect with me on LinkedIn or explore my projects on GitHub."
  },
  {
    "objectID": "posts/code_fine_tuning.html",
    "href": "posts/code_fine_tuning.html",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "Before understanding what SFT is, we should understand what pre-training is. Pre-training involves training a model (generally a transformer) on a large corpus of text. Pre-training allows models to learn generalizable knowledge, grammar, semantics etc. However, the model is hardly usable after pre-training; as the model lacks task specific expertize.\n\n\nThat’s where Supervised Training plays a part. Supervised Fine-Tuning is used to adapt a pre-trained model to a specific task. It involves training the model on a labeled dataset, where the model learns to predict the correct label for each input.\n\n\nIn this article, we will load a pre-trained model from HuggingFace and finetune it on a specific dataset about Python programming.\n\n\n\n\nWe need transformers, trl and datasets packages. We will install the packages using pip:\n\n!pip install -q transformers==4.47.1 trl==0.13.0 datasets==3.2.0\n\n\n\n\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom datasets import load_dataset\nimport os\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n\n\n\n\nWe will load SmolLM2-135M model and it’s tokenizer from HuggingFace. After that, we will generate output of the model on a few specific input prompts related to Python. First, we ask a simple question of writing Python code for finding square of a number. After that, we ask model to determine if a string is palindrome (slightly difficult question than the first one).\n\n\nAs we can see, the model did not provide correct output in both the cases. The model generates some irrelevant text and keep repeating that afterwards. Let’s see how finetuning this model on task-specific dataset will help in increasing the model correctness.\n\n\nmodel_name = 'HuggingFaceTB/SmolLM2-135M'\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\ninput = 'Generate Python code for finding square of a number'\ntokenized_input = tokenizer(input, return_tensors='pt').to(device)\nmodel = model.to(device)\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nGenerate Python code for finding square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a\n\n\n\ninput = 'Write a program in Python to determine if a string is a palindrome'\ntokenized_input = tokenizer(input, return_tensors='pt').to(device)\nmodel = model.to(device)\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = model.generate(**tokenized_input, max_length=256)\nprint(tokenizer.decode(output[0]))\n\nWrite a program in Python to determine if a string is a palindrome.\n\n```python\ndef is_palindrome(s):\n    # Check if the string is a palindrome\n    if not s:\n        return False\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isdigit():\n        return True\n\n    # Check if the string is a letter\n    if s.islower():\n        return True\n\n    # Check if the string is a letter\n    if s.isupper():\n        return True\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isnumeric():\n        return True\n\n    # Check if the string is a letter\n    if s.islower():\n        return True\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isnumeric():\n        return True\n\n    # Check if the string is a letter\n    if s.isupper():\n        return True\n\n    # Check if the string is a\n\n\n\n\n\n\nWe will only load a fraction of data (3 %) for the ease of training with a single GPU.\nTrain dataset has around 15000 examples and test / evaluation dataset has 1679 examples.\n\n\ndataset_path = 'jtatman/python-code-dataset-500k'\nds = load_dataset(dataset_path, split='train[:3%]')\nds = ds.train_test_split(test_size=0.1)\ntrain_dataset = ds['train']\ntest_dataset = ds['test']\n\n\nlen(train_dataset)\n\n15106\n\n\n\nlen(test_dataset)\n\n1679\n\n\n\ntrain_dataset\n\nDataset({\n    features: ['output', 'instruction', 'system'],\n    num_rows: 15106\n})\n\n\n\n\n\n\nAs we can see from above, the dataset has 3 columns: system, instruction and output.\nFor finetuning the model, we need to combine the 3 columns into a single text field (which is achieved with formatting_function function)\n\n\ndef formatting_function(example):\n    output_texts = []\n    for i in range(len(example['instruction'])):\n        individual_prompt = \"###System: {}\\n###Instruction: {}\\n ###Output: {}\".format(example['system'][i],\n                                                                                       example['instruction'][i],\n                                                                                       example['output'][i])\n        output_texts.append(individual_prompt)\n    return output_texts\n\n\n\n\n\nWe also need to define a collator which needs response template and tokenizer as input.\nResponse template will help the trainer to identify output within the training prompt. If the training examples do not contain the response template, trainer will error out.\n\n\nresponse_template = \" ###Output:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\n\n\n\n\ntrl library provides SFTConfig class using which we can provide training arguements.\nWe have used train and eval batch size of 2 due to memory constraints.\nWe have also used Gradient Accumulation and Gradient Checkpointing for memory optimization.\nIn addition, we have also used bf16=True in an effort to further optimize memory.\n\n\ntrainer_config = SFTConfig(\n    output_dir='.\\code_finetuned_mode',\n    max_steps=1000,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    logging_steps=10,\n    ignore_data_skip=True,\n    run_name='code_fine_tuning_23_12_2024',\n    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch\n    # Memory optimization\n    gradient_checkpointing=True,  # Trade compute for memory savings\n    optim=\"adamw_torch_fused\",\n    bf16=True\n)\n\n\n\n\n\nWe will empty cache before starting training to optimize memory.\ntrl library has SFTTrainer class to which we need to provide model, training arguments, training data, evaluation data, formatting function and data collator.\nTraining information will get logged to Weights and Biases.\nWe will save the trained model to a local directory.\n\n\ntorch.cuda.empty_cache()\n\n\ntrainer = SFTTrainer(\n    model=model,\n    args=trainer_config,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    formatting_func=formatting_function,\n    data_collator=collator\n)\ntrainer.train()\n\n\n\n\n\n\n\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n\n\n\n    \n      \n      \n      [ 101/1000 02:56 &lt; 26:41, 0.56 it/s, Epoch 0.03/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n\n\n    \n      \n      \n      [394/840 01:38 &lt; 01:52, 3.97 it/s]\n    \n    \n\n\n\n    \n      \n      \n      [1000/1000 1:05:45, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n100\n0.743100\n0.728401\n\n\n200\n0.835200\n0.708747\n\n\n300\n0.641200\n0.696030\n\n\n400\n0.726100\n0.687549\n\n\n500\n0.691100\n0.681275\n\n\n600\n0.687000\n0.676701\n\n\n700\n0.682500\n0.673061\n\n\n800\n0.701100\n0.670183\n\n\n900\n0.672400\n0.668563\n\n\n1000\n0.679100\n0.668029\n\n\n\n\n\n\nTrainOutput(global_step=1000, training_loss=0.7042460036277771, metrics={'train_runtime': 3947.6465, 'train_samples_per_second': 1.013, 'train_steps_per_second': 0.253, 'total_flos': 1717373605593600.0, 'train_loss': 0.7042460036277771, 'epoch': 0.26479544551833706})\n\n\n\n\n\n\n\n\nW&B Chart 12_23_2024, 3_26_48 AM.png\n\n\n\n\n\n\n\n\nW&B Chart 12_23_2024, 3_27_33 AM.png\n\n\n\nfinetune_name = 'code_model_finetuned'\n\n\ntrainer.save_model(f'./{finetune_name}')\n\n\n\n\n\nAs we can notice, the finetuned model has answered first question correctly.\nThe model tried to answer second question. However, the answer is not correct. However, the output is quite better than the non fine-tuned model. It has written correct to determine if a string is palindrome when it’s length is 2 :).\nDue to resource constraints, we used limited data and performed training for 1000 steps only. Considering that, I would say that the finetuned model is much better at Python coding than the baseline model.\n\n\nfinetuned_model = AutoModelForCausalLM.from_pretrained(f'./{finetune_name}', local_files_only=True)\n\n\ninput = \"\"\"##System: You are a Python code generator, capable of creating scripts from specifications.\\n\n#Instruction: Generate Python code for finding square of a number\"\"\"\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\n##System: You are a Python code generator, capable of creating scripts from specifications.\n\n#Instruction: Generate Python code for finding square of a number\n\nGiven a number, find the square of that number.\n\nExample:\n\n```python\nnumber = 10\nsquare_number = number ** 2\nprint(square_number)\n```\n\nOutput:\n```\n10\n```\n\nExplanation:\n\n```python\nnumber = 10\nsquare_number = number ** 2\nprint(square_number)\n```\n\nOutput:\n```\n10\n\n\n\ninput = \"\"\"##System: You are a Python code generator, capable of creating scripts from specifications.\\n\n#Instruction: Write a program in Python to determine if a string is a palindrome\n\"\"\"\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=256)\nprint(tokenizer.decode(output[0]))\n\n##System: You are a Python code generator, capable of creating scripts from specifications.\n\n#Instruction: Write a program in Python to determine if a string is a palindrome\n\nA palindrome is a word or a string that reads the same backward and forward. For example, the word \"hello\" is a palindrome because it reads the same backward and forward.\n\nHere's a program that determines if a string is a palindrome:\n\n```python\ndef is_palindrome(string):\n    # Check if the string is empty\n    if len(string) == 0:\n        return False\n\n    # Check if the string is a word\n    if len(string) == 2:\n        # Check if the first character is the same as the last character\n        if string[0] == string[-1]:\n            return True\n        else:\n            return False\n\n    # Check if the string is a word\n    if len(string) == 1:\n        return string[0] == string[-1]\n\n    # Check if the string is a word\n    if string[0] != string[-1]:\n        return False\n\n    # Check if the string is a word\n    if string[0] != string[-1\n\n\n\n\n\n\ntrl library provides high level and quite simple APIs to finetune a Large Language model.\nFollowing are Pros and Cons of the finetuning approach described in this article:\n\nPros:\n\nImproved Task Performance: Fine-tuning enables a model to specialize in specific tasks, often achieving better accuracy compared to general-purpose models.\nData Efficiency: Requires much less labeled data compared to training from scratch due to pre-trained knowledge.\nCustomizability: Allows tailoring models to domain-specific or task-specific needs (e.g., medical, legal, or financial contexts).\n\nCons:\n\nRisk of Overfitting: On small datasets, the model may overfit, reducing its ability to generalize to unseen data.\nCatastrophic Forgetting: Fine-tuning can overwrite pre-trained knowledge, reducing performance on general tasks.\nResource Requirements: Fine-tuning still demands significant computational resources, especially for large models, compared to lightweight alternatives like prompt engineering.\nUpdation of All Parameters: The finetuning approach described in this article updates all parameters of the model. Therefore, storing of the updated model is quite costly. Therefore, more efficient approaches of finetuning like PEFT and LoRA are preferred.\n\n\n\n\n\n\n\nTRL (Transformer Reinforcement Learning) Documentation:\nSmolLM2-135M Model on HuggingFace\nDataset for Training"
  },
  {
    "objectID": "posts/code_fine_tuning.html#introduction-what-is-supervised-fine-tuning-sft",
    "href": "posts/code_fine_tuning.html#introduction-what-is-supervised-fine-tuning-sft",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "Before understanding what SFT is, we should understand what pre-training is. Pre-training involves training a model (generally a transformer) on a large corpus of text. Pre-training allows models to learn generalizable knowledge, grammar, semantics etc. However, the model is hardly usable after pre-training; as the model lacks task specific expertize.\n\n\nThat’s where Supervised Training plays a part. Supervised Fine-Tuning is used to adapt a pre-trained model to a specific task. It involves training the model on a labeled dataset, where the model learns to predict the correct label for each input.\n\n\nIn this article, we will load a pre-trained model from HuggingFace and finetune it on a specific dataset about Python programming."
  },
  {
    "objectID": "posts/code_fine_tuning.html#install-required-packages",
    "href": "posts/code_fine_tuning.html#install-required-packages",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "We need transformers, trl and datasets packages. We will install the packages using pip:\n\n!pip install -q transformers==4.47.1 trl==0.13.0 datasets==3.2.0"
  },
  {
    "objectID": "posts/code_fine_tuning.html#import-packages",
    "href": "posts/code_fine_tuning.html#import-packages",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom datasets import load_dataset\nimport os\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
  },
  {
    "objectID": "posts/code_fine_tuning.html#load-a-pretrained-model",
    "href": "posts/code_fine_tuning.html#load-a-pretrained-model",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "We will load SmolLM2-135M model and it’s tokenizer from HuggingFace. After that, we will generate output of the model on a few specific input prompts related to Python. First, we ask a simple question of writing Python code for finding square of a number. After that, we ask model to determine if a string is palindrome (slightly difficult question than the first one).\n\n\nAs we can see, the model did not provide correct output in both the cases. The model generates some irrelevant text and keep repeating that afterwards. Let’s see how finetuning this model on task-specific dataset will help in increasing the model correctness.\n\n\nmodel_name = 'HuggingFaceTB/SmolLM2-135M'\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\ninput = 'Generate Python code for finding square of a number'\ntokenized_input = tokenizer(input, return_tensors='pt').to(device)\nmodel = model.to(device)\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nGenerate Python code for finding square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a\n\n\n\ninput = 'Write a program in Python to determine if a string is a palindrome'\ntokenized_input = tokenizer(input, return_tensors='pt').to(device)\nmodel = model.to(device)\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = model.generate(**tokenized_input, max_length=256)\nprint(tokenizer.decode(output[0]))\n\nWrite a program in Python to determine if a string is a palindrome.\n\n```python\ndef is_palindrome(s):\n    # Check if the string is a palindrome\n    if not s:\n        return False\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isdigit():\n        return True\n\n    # Check if the string is a letter\n    if s.islower():\n        return True\n\n    # Check if the string is a letter\n    if s.isupper():\n        return True\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isnumeric():\n        return True\n\n    # Check if the string is a letter\n    if s.islower():\n        return True\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isnumeric():\n        return True\n\n    # Check if the string is a letter\n    if s.isupper():\n        return True\n\n    # Check if the string is a"
  },
  {
    "objectID": "posts/code_fine_tuning.html#load-dataset",
    "href": "posts/code_fine_tuning.html#load-dataset",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "We will only load a fraction of data (3 %) for the ease of training with a single GPU.\nTrain dataset has around 15000 examples and test / evaluation dataset has 1679 examples.\n\n\ndataset_path = 'jtatman/python-code-dataset-500k'\nds = load_dataset(dataset_path, split='train[:3%]')\nds = ds.train_test_split(test_size=0.1)\ntrain_dataset = ds['train']\ntest_dataset = ds['test']\n\n\nlen(train_dataset)\n\n15106\n\n\n\nlen(test_dataset)\n\n1679\n\n\n\ntrain_dataset\n\nDataset({\n    features: ['output', 'instruction', 'system'],\n    num_rows: 15106\n})"
  },
  {
    "objectID": "posts/code_fine_tuning.html#define-a-formatting-function",
    "href": "posts/code_fine_tuning.html#define-a-formatting-function",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "As we can see from above, the dataset has 3 columns: system, instruction and output.\nFor finetuning the model, we need to combine the 3 columns into a single text field (which is achieved with formatting_function function)\n\n\ndef formatting_function(example):\n    output_texts = []\n    for i in range(len(example['instruction'])):\n        individual_prompt = \"###System: {}\\n###Instruction: {}\\n ###Output: {}\".format(example['system'][i],\n                                                                                       example['instruction'][i],\n                                                                                       example['output'][i])\n        output_texts.append(individual_prompt)\n    return output_texts"
  },
  {
    "objectID": "posts/code_fine_tuning.html#define-a-collator",
    "href": "posts/code_fine_tuning.html#define-a-collator",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "We also need to define a collator which needs response template and tokenizer as input.\nResponse template will help the trainer to identify output within the training prompt. If the training examples do not contain the response template, trainer will error out.\n\n\nresponse_template = \" ###Output:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
  },
  {
    "objectID": "posts/code_fine_tuning.html#define-training-config",
    "href": "posts/code_fine_tuning.html#define-training-config",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "trl library provides SFTConfig class using which we can provide training arguements.\nWe have used train and eval batch size of 2 due to memory constraints.\nWe have also used Gradient Accumulation and Gradient Checkpointing for memory optimization.\nIn addition, we have also used bf16=True in an effort to further optimize memory.\n\n\ntrainer_config = SFTConfig(\n    output_dir='.\\code_finetuned_mode',\n    max_steps=1000,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    logging_steps=10,\n    ignore_data_skip=True,\n    run_name='code_fine_tuning_23_12_2024',\n    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch\n    # Memory optimization\n    gradient_checkpointing=True,  # Trade compute for memory savings\n    optim=\"adamw_torch_fused\",\n    bf16=True\n)"
  },
  {
    "objectID": "posts/code_fine_tuning.html#train-finetune-the-model",
    "href": "posts/code_fine_tuning.html#train-finetune-the-model",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "We will empty cache before starting training to optimize memory.\ntrl library has SFTTrainer class to which we need to provide model, training arguments, training data, evaluation data, formatting function and data collator.\nTraining information will get logged to Weights and Biases.\nWe will save the trained model to a local directory.\n\n\ntorch.cuda.empty_cache()\n\n\ntrainer = SFTTrainer(\n    model=model,\n    args=trainer_config,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    formatting_func=formatting_function,\n    data_collator=collator\n)\ntrainer.train()\n\n\n\n\n\n\n\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n\n\n\n    \n      \n      \n      [ 101/1000 02:56 &lt; 26:41, 0.56 it/s, Epoch 0.03/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n\n\n    \n      \n      \n      [394/840 01:38 &lt; 01:52, 3.97 it/s]\n    \n    \n\n\n\n    \n      \n      \n      [1000/1000 1:05:45, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n100\n0.743100\n0.728401\n\n\n200\n0.835200\n0.708747\n\n\n300\n0.641200\n0.696030\n\n\n400\n0.726100\n0.687549\n\n\n500\n0.691100\n0.681275\n\n\n600\n0.687000\n0.676701\n\n\n700\n0.682500\n0.673061\n\n\n800\n0.701100\n0.670183\n\n\n900\n0.672400\n0.668563\n\n\n1000\n0.679100\n0.668029\n\n\n\n\n\n\nTrainOutput(global_step=1000, training_loss=0.7042460036277771, metrics={'train_runtime': 3947.6465, 'train_samples_per_second': 1.013, 'train_steps_per_second': 0.253, 'total_flos': 1717373605593600.0, 'train_loss': 0.7042460036277771, 'epoch': 0.26479544551833706})"
  },
  {
    "objectID": "posts/code_fine_tuning.html#training-loss",
    "href": "posts/code_fine_tuning.html#training-loss",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "W&B Chart 12_23_2024, 3_26_48 AM.png"
  },
  {
    "objectID": "posts/code_fine_tuning.html#evaluation-loss",
    "href": "posts/code_fine_tuning.html#evaluation-loss",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "W&B Chart 12_23_2024, 3_27_33 AM.png\n\n\n\nfinetune_name = 'code_model_finetuned'\n\n\ntrainer.save_model(f'./{finetune_name}')"
  },
  {
    "objectID": "posts/code_fine_tuning.html#load-the-finetuned-model-and-perform-prediction-with-it",
    "href": "posts/code_fine_tuning.html#load-the-finetuned-model-and-perform-prediction-with-it",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "As we can notice, the finetuned model has answered first question correctly.\nThe model tried to answer second question. However, the answer is not correct. However, the output is quite better than the non fine-tuned model. It has written correct to determine if a string is palindrome when it’s length is 2 :).\nDue to resource constraints, we used limited data and performed training for 1000 steps only. Considering that, I would say that the finetuned model is much better at Python coding than the baseline model.\n\n\nfinetuned_model = AutoModelForCausalLM.from_pretrained(f'./{finetune_name}', local_files_only=True)\n\n\ninput = \"\"\"##System: You are a Python code generator, capable of creating scripts from specifications.\\n\n#Instruction: Generate Python code for finding square of a number\"\"\"\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\n##System: You are a Python code generator, capable of creating scripts from specifications.\n\n#Instruction: Generate Python code for finding square of a number\n\nGiven a number, find the square of that number.\n\nExample:\n\n```python\nnumber = 10\nsquare_number = number ** 2\nprint(square_number)\n```\n\nOutput:\n```\n10\n```\n\nExplanation:\n\n```python\nnumber = 10\nsquare_number = number ** 2\nprint(square_number)\n```\n\nOutput:\n```\n10\n\n\n\ninput = \"\"\"##System: You are a Python code generator, capable of creating scripts from specifications.\\n\n#Instruction: Write a program in Python to determine if a string is a palindrome\n\"\"\"\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=256)\nprint(tokenizer.decode(output[0]))\n\n##System: You are a Python code generator, capable of creating scripts from specifications.\n\n#Instruction: Write a program in Python to determine if a string is a palindrome\n\nA palindrome is a word or a string that reads the same backward and forward. For example, the word \"hello\" is a palindrome because it reads the same backward and forward.\n\nHere's a program that determines if a string is a palindrome:\n\n```python\ndef is_palindrome(string):\n    # Check if the string is empty\n    if len(string) == 0:\n        return False\n\n    # Check if the string is a word\n    if len(string) == 2:\n        # Check if the first character is the same as the last character\n        if string[0] == string[-1]:\n            return True\n        else:\n            return False\n\n    # Check if the string is a word\n    if len(string) == 1:\n        return string[0] == string[-1]\n\n    # Check if the string is a word\n    if string[0] != string[-1]:\n        return False\n\n    # Check if the string is a word\n    if string[0] != string[-1"
  },
  {
    "objectID": "posts/code_fine_tuning.html#conclusion",
    "href": "posts/code_fine_tuning.html#conclusion",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "trl library provides high level and quite simple APIs to finetune a Large Language model.\nFollowing are Pros and Cons of the finetuning approach described in this article:\n\nPros:\n\nImproved Task Performance: Fine-tuning enables a model to specialize in specific tasks, often achieving better accuracy compared to general-purpose models.\nData Efficiency: Requires much less labeled data compared to training from scratch due to pre-trained knowledge.\nCustomizability: Allows tailoring models to domain-specific or task-specific needs (e.g., medical, legal, or financial contexts).\n\nCons:\n\nRisk of Overfitting: On small datasets, the model may overfit, reducing its ability to generalize to unseen data.\nCatastrophic Forgetting: Fine-tuning can overwrite pre-trained knowledge, reducing performance on general tasks.\nResource Requirements: Fine-tuning still demands significant computational resources, especially for large models, compared to lightweight alternatives like prompt engineering.\nUpdation of All Parameters: The finetuning approach described in this article updates all parameters of the model. Therefore, storing of the updated model is quite costly. Therefore, more efficient approaches of finetuning like PEFT and LoRA are preferred."
  },
  {
    "objectID": "posts/code_fine_tuning.html#references",
    "href": "posts/code_fine_tuning.html#references",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "TRL (Transformer Reinforcement Learning) Documentation:\nSmolLM2-135M Model on HuggingFace\nDataset for Training"
  }
]