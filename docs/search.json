[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Blog",
    "section": "",
    "text": "type: table contents: blog/ sort: date sort-desc: true categories: blog"
  },
  {
    "objectID": "index.html#recent-blog-posts",
    "href": "index.html#recent-blog-posts",
    "title": "Data Science Blog",
    "section": "",
    "text": "type: table contents: blog/ sort: date sort-desc: true categories: blog"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog/code_fine_tuning.html",
    "href": "blog/code_fine_tuning.html",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "Before understanding what SFT is, we should understand what pre-training is. Pre-training involves training a model (generally a transformer) on a large corpus of text. Pre-training allows models to learn generalizable knowledge, grammar, semantics etc. However, the model is hardly usable after pre-training; as the model lacks task specific expertize.\n\n\nThat’s where Supervised Training plays a part. Supervised Fine-Tuning is used to adapt a pre-trained model to a specific task. It involves training the model on a labeled dataset, where the model learns to predict the correct label for each input.\n\n\nIn this article, we will load a pre-trained model from HuggingFace and finetune it on a specific dataset about Python programming\n\n\n\n\n\n# !pip install trl transformers trl datasets\n\n\n\n\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom datasets import load_dataset\nimport os\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n\n\n\n\nWe will load SmolLM2-135M model and it’s tokenizer from HuggingFace. After that, we will generate output of the model on a specific input prompt related to Python (we ask the model to generate code for finding square root of a number)\n\n\nAs we can see, the model did not provide correct output. The model generate irrelevant text and keep repeating that. Let’s see how finetuning this model on task-specific dataset will help in increasing the model correctness.\n\n\nmodel_name = 'HuggingFaceTB/SmolLM2-135M'\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\ninput = 'Give me a Python code for finding square root of a number'\ntokenized_input = tokenizer(input, return_tensors='pt')\noutput = model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\n\nGive me a Python code for finding square root of a number.\n\n## Python Program to Find Square Root of a Number\n\nLet’s see how to find square root of a number in Python.\n\n``````# Python program to find square root of a number\n\n# This program will print the square root of a number\n\n# using the built-in function square root\n\n# This program will print the square root of a number\n\n# using the built-in function square root\n\n# This program will print the square root of a number\n\n# using the built-in function square\n\n\n\n\n\n\ndataset_path = 'jtatman/python-code-dataset-500k'\nds = load_dataset(dataset_path, split='train[:3%]')\nds = ds.train_test_split(test_size=0.1)\ntrain_dataset = ds['train']\ntest_dataset = ds['test']\n\n\nlen(train_dataset)\n\n15106\n\n\n\nlen(test_dataset)\n\n1679\n\n\n\ndef formatting_function(example):\n    output_texts = []\n    for i in range(len(example['instruction'])):\n        individual_prompt = \"###System: {}\\n###Instruction: {}\\n ###Output: {}\".format(example['system'][i],\n                                                                             example['instruction'][i],\n                                                                             example['output'][i])\n        output_texts.append(individual_prompt)\n    return output_texts\n\n\nresponse_template = \" ###Output:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\n\ntrainer_config = SFTConfig(\n    output_dir='.\\code_finetuned_mode',\n    max_steps=400,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    logging_steps=10,\n    ignore_data_skip=True\n)\n\n\ntorch.cuda.empty_cache()\n\ntrainer = SFTTrainer(\n    model=model,\n    args=trainer_config,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    formatting_func=formatting_function,\n    data_collator=collator\n)\ntrainer.train()\n\n\n    \n      \n      \n      [400/400 23:38, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n50\n0.839400\n0.743556\n\n\n100\n0.794000\n0.731545\n\n\n150\n0.715300\n0.723117\n\n\n200\n0.635300\n0.717372\n\n\n250\n0.720100\n0.713097\n\n\n300\n0.680100\n0.710268\n\n\n350\n0.711100\n0.707956\n\n\n400\n0.831900\n0.707255\n\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:169: UserWarning: Could not find response key ` ###Output:` in the following instance: ###System: You are a Python code architect, reviewing and designing scalable and efficient code\n###Instruction: Using the given information, write a function to calculate a 10-digit ISBN number. The ISBN number should be generated by combining various elements from the given information in a specific order.\n\nHere are the steps to generate the ISBN number:\n\n1. Take the first three letters of the author's last name (in this case, \"Doe\") and convert them to uppercase. If the author's last name has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"DOE\".\n\n2. Take the first three letters of the book title (in this case, \"Advanced\") and convert them to uppercase. If the title has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"ADV\".\n\n3. Take the first letter of the publisher's name (in this case, \"X\") and convert it to uppercase.\n\n4. Take the last two digits of the publication year (in this case, \"22\").\n\n5. Concatenate all the elements obtained in steps 1 to 4. The resulting string would be \"DOEADVX22\".\n\n6. Calculate the checksum digit of the ISBN number. To do this, multiply each digit of the concatenated string from step 5 by its position (starting from 1) and sum them up. For example, for the string \"DOEADVX22\", the calculation would be:\n   (1 * D) + (2 * O) + (3 * E) + (4 * A) + (5 * D) + (6 * V) + (7 * X) + (8 * 2) + (9 * 2) + (10 * 2)\n   Convert the result to modulo 11, and the remainder will be the checksum digit. If the remainder is 10, the checksum digit should be \"X\".\n\n7. Append the checksum digit obtained in step 6 to the end of the concatenated string from step 5. The resulting string would be the 10-digit ISBN number.\n\nWrite a function in your preferred programming language that implements the above steps and returns the calculated 10-digit ISBN number.\n\n#Rewritten Test - Increased Difficulty#\n\nUsing the given information, write a function to calculate a 10-digit ISBN number. The ISBN number should be generated by combining various elements from the given information in a specific order.\n\nHere are the steps to generate the ISBN number:\n\n1. Take the first three letters of the author's last name (in this case, \"Doe\") and convert them to uppercase. If the author's last name has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"DOE\".\n\n2. Take the first three letters of the book title (in this case, \"Advanced\") and convert them to uppercase. If the title has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"ADV\".\n\n3. Take the first letter of the publisher's name (in this case, \"X\") and convert it to uppercase.\n\n4. Take the last two digits of the publication year (in this case, \"22\").\n\n5. Concatenate all the elements obtained in steps 1 to 4. The resulting string would be \"DOEADVX22\".\n\n6. Calculate the checksum digit of the ISBN number. To do this, multiply each digit of the concatenated string from step 5 by its position (starting from 1) and sum them up. For example, for the string \"DOEADVX22\", the calculation would be:\n   (1 * D) + (2 * O) + (3 * E) + (4 * A) + (5 * D) + (6 * V) + (7 * X) + (8 * 2) + (9 * 2) + (10 * 2)\n   Convert the result to modulo 11, and the remainder will be the checksum digit. If the remainder is 10, the checksum digit should be \"X\".\n\n7. Append the checksum digit obtained in step 6 to the end of the concatenated string from step 5. The resulting string would be the 10-digit ISBN number.\n\nWrite a function in your preferred programming language that implements the above steps and returns the calculated 10-digit ISBN number.\n\nExample Input:\nAuthor Last Name: \"Doe\"\nBook Title. This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n  warnings.warn(\n\n\nTrainOutput(global_step=400, training_loss=0.7337326669692993, metrics={'train_runtime': 1419.3064, 'train_samples_per_second': 0.564, 'train_steps_per_second': 0.282, 'total_flos': 341935195820544.0, 'train_loss': 0.7337326669692993, 'epoch': 0.052959089103667416})\n\n\n\nfinetune_name = 'code_model_finetuned'\n\n\ntrainer.save_model(f'./{finetune_name}')\n\n\n\n\n\nfinetuned_model = AutoModelForCausalLM.from_pretrained(f'./{finetune_name}', local_files_only=True)\n\n\ninput = 'Give me a Python code for finding square root of a number'\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nGive me a Python code for finding square root of a number.\n\nHere's the code:\n\n```python\ndef square_root(num):\n    return num ** 0.5\n\nprint(square_root(10))  # Output: 5\nprint(square_root(20))  # Output: 10\nprint(square_root(30))  # Output: 40\n```\n\nIn this code, we define a function `square_root` that takes a number as input and returns its square root. We then call this function with `1"
  },
  {
    "objectID": "blog/code_fine_tuning.html#introduction-what-is-supervised-fine-tuning",
    "href": "blog/code_fine_tuning.html#introduction-what-is-supervised-fine-tuning",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "Before understanding what SFT is, we should understand what pre-training is. Pre-training involves training a model (generally a transformer) on a large corpus of text. Pre-training allows models to learn generalizable knowledge, grammar, semantics etc. However, the model is hardly usable after pre-training; as the model lacks task specific expertize.\n\n\nThat’s where Supervised Training plays a part. Supervised Fine-Tuning is used to adapt a pre-trained model to a specific task. It involves training the model on a labeled dataset, where the model learns to predict the correct label for each input.\n\n\nIn this article, we will load a pre-trained model from HuggingFace and finetune it on a specific dataset about Python programming"
  },
  {
    "objectID": "blog/code_fine_tuning.html#install-required-packages",
    "href": "blog/code_fine_tuning.html#install-required-packages",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "# !pip install trl transformers trl datasets"
  },
  {
    "objectID": "blog/code_fine_tuning.html#import-packages",
    "href": "blog/code_fine_tuning.html#import-packages",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom datasets import load_dataset\nimport os\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
  },
  {
    "objectID": "blog/code_fine_tuning.html#load-a-pretrained-model",
    "href": "blog/code_fine_tuning.html#load-a-pretrained-model",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "We will load SmolLM2-135M model and it’s tokenizer from HuggingFace. After that, we will generate output of the model on a specific input prompt related to Python (we ask the model to generate code for finding square root of a number)\n\n\nAs we can see, the model did not provide correct output. The model generate irrelevant text and keep repeating that. Let’s see how finetuning this model on task-specific dataset will help in increasing the model correctness.\n\n\nmodel_name = 'HuggingFaceTB/SmolLM2-135M'\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\ninput = 'Give me a Python code for finding square root of a number'\ntokenized_input = tokenizer(input, return_tensors='pt')\noutput = model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\n\nGive me a Python code for finding square root of a number.\n\n## Python Program to Find Square Root of a Number\n\nLet’s see how to find square root of a number in Python.\n\n``````# Python program to find square root of a number\n\n# This program will print the square root of a number\n\n# using the built-in function square root\n\n# This program will print the square root of a number\n\n# using the built-in function square root\n\n# This program will print the square root of a number\n\n# using the built-in function square"
  },
  {
    "objectID": "blog/code_fine_tuning.html#load-dataset",
    "href": "blog/code_fine_tuning.html#load-dataset",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "dataset_path = 'jtatman/python-code-dataset-500k'\nds = load_dataset(dataset_path, split='train[:3%]')\nds = ds.train_test_split(test_size=0.1)\ntrain_dataset = ds['train']\ntest_dataset = ds['test']\n\n\nlen(train_dataset)\n\n15106\n\n\n\nlen(test_dataset)\n\n1679\n\n\n\ndef formatting_function(example):\n    output_texts = []\n    for i in range(len(example['instruction'])):\n        individual_prompt = \"###System: {}\\n###Instruction: {}\\n ###Output: {}\".format(example['system'][i],\n                                                                             example['instruction'][i],\n                                                                             example['output'][i])\n        output_texts.append(individual_prompt)\n    return output_texts\n\n\nresponse_template = \" ###Output:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\n\ntrainer_config = SFTConfig(\n    output_dir='.\\code_finetuned_mode',\n    max_steps=400,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    logging_steps=10,\n    ignore_data_skip=True\n)\n\n\ntorch.cuda.empty_cache()\n\ntrainer = SFTTrainer(\n    model=model,\n    args=trainer_config,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    formatting_func=formatting_function,\n    data_collator=collator\n)\ntrainer.train()\n\n\n    \n      \n      \n      [400/400 23:38, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n50\n0.839400\n0.743556\n\n\n100\n0.794000\n0.731545\n\n\n150\n0.715300\n0.723117\n\n\n200\n0.635300\n0.717372\n\n\n250\n0.720100\n0.713097\n\n\n300\n0.680100\n0.710268\n\n\n350\n0.711100\n0.707956\n\n\n400\n0.831900\n0.707255\n\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:169: UserWarning: Could not find response key ` ###Output:` in the following instance: ###System: You are a Python code architect, reviewing and designing scalable and efficient code\n###Instruction: Using the given information, write a function to calculate a 10-digit ISBN number. The ISBN number should be generated by combining various elements from the given information in a specific order.\n\nHere are the steps to generate the ISBN number:\n\n1. Take the first three letters of the author's last name (in this case, \"Doe\") and convert them to uppercase. If the author's last name has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"DOE\".\n\n2. Take the first three letters of the book title (in this case, \"Advanced\") and convert them to uppercase. If the title has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"ADV\".\n\n3. Take the first letter of the publisher's name (in this case, \"X\") and convert it to uppercase.\n\n4. Take the last two digits of the publication year (in this case, \"22\").\n\n5. Concatenate all the elements obtained in steps 1 to 4. The resulting string would be \"DOEADVX22\".\n\n6. Calculate the checksum digit of the ISBN number. To do this, multiply each digit of the concatenated string from step 5 by its position (starting from 1) and sum them up. For example, for the string \"DOEADVX22\", the calculation would be:\n   (1 * D) + (2 * O) + (3 * E) + (4 * A) + (5 * D) + (6 * V) + (7 * X) + (8 * 2) + (9 * 2) + (10 * 2)\n   Convert the result to modulo 11, and the remainder will be the checksum digit. If the remainder is 10, the checksum digit should be \"X\".\n\n7. Append the checksum digit obtained in step 6 to the end of the concatenated string from step 5. The resulting string would be the 10-digit ISBN number.\n\nWrite a function in your preferred programming language that implements the above steps and returns the calculated 10-digit ISBN number.\n\n#Rewritten Test - Increased Difficulty#\n\nUsing the given information, write a function to calculate a 10-digit ISBN number. The ISBN number should be generated by combining various elements from the given information in a specific order.\n\nHere are the steps to generate the ISBN number:\n\n1. Take the first three letters of the author's last name (in this case, \"Doe\") and convert them to uppercase. If the author's last name has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"DOE\".\n\n2. Take the first three letters of the book title (in this case, \"Advanced\") and convert them to uppercase. If the title has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"ADV\".\n\n3. Take the first letter of the publisher's name (in this case, \"X\") and convert it to uppercase.\n\n4. Take the last two digits of the publication year (in this case, \"22\").\n\n5. Concatenate all the elements obtained in steps 1 to 4. The resulting string would be \"DOEADVX22\".\n\n6. Calculate the checksum digit of the ISBN number. To do this, multiply each digit of the concatenated string from step 5 by its position (starting from 1) and sum them up. For example, for the string \"DOEADVX22\", the calculation would be:\n   (1 * D) + (2 * O) + (3 * E) + (4 * A) + (5 * D) + (6 * V) + (7 * X) + (8 * 2) + (9 * 2) + (10 * 2)\n   Convert the result to modulo 11, and the remainder will be the checksum digit. If the remainder is 10, the checksum digit should be \"X\".\n\n7. Append the checksum digit obtained in step 6 to the end of the concatenated string from step 5. The resulting string would be the 10-digit ISBN number.\n\nWrite a function in your preferred programming language that implements the above steps and returns the calculated 10-digit ISBN number.\n\nExample Input:\nAuthor Last Name: \"Doe\"\nBook Title. This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n  warnings.warn(\n\n\nTrainOutput(global_step=400, training_loss=0.7337326669692993, metrics={'train_runtime': 1419.3064, 'train_samples_per_second': 0.564, 'train_steps_per_second': 0.282, 'total_flos': 341935195820544.0, 'train_loss': 0.7337326669692993, 'epoch': 0.052959089103667416})\n\n\n\nfinetune_name = 'code_model_finetuned'\n\n\ntrainer.save_model(f'./{finetune_name}')"
  },
  {
    "objectID": "blog/code_fine_tuning.html#load-the-finetuned-model-and-perform-prediction-with-it",
    "href": "blog/code_fine_tuning.html#load-the-finetuned-model-and-perform-prediction-with-it",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "finetuned_model = AutoModelForCausalLM.from_pretrained(f'./{finetune_name}', local_files_only=True)\n\n\ninput = 'Give me a Python code for finding square root of a number'\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nGive me a Python code for finding square root of a number.\n\nHere's the code:\n\n```python\ndef square_root(num):\n    return num ** 0.5\n\nprint(square_root(10))  # Output: 5\nprint(square_root(20))  # Output: 10\nprint(square_root(30))  # Output: 40\n```\n\nIn this code, we define a function `square_root` that takes a number as input and returns its square root. We then call this function with `1"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSupervised Fine-Tuning Part-1\n\n\n\n\n\n\nsupervised fine-tuning\n\n\nLLM\n\n\n\n\n\n\n\n\n\nDec 22, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/code_fine_tuning.html",
    "href": "posts/code_fine_tuning.html",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "Before understanding what SFT is, we should understand what pre-training is. Pre-training involves training a model (generally a transformer) on a large corpus of text. Pre-training allows models to learn generalizable knowledge, grammar, semantics etc. However, the model is hardly usable after pre-training; as the model lacks task specific expertize.\n\n\nThat’s where Supervised Training plays a part. Supervised Fine-Tuning is used to adapt a pre-trained model to a specific task. It involves training the model on a labeled dataset, where the model learns to predict the correct label for each input.\n\n\nIn this article, we will load a pre-trained model from HuggingFace and finetune it on a specific dataset about Python programming\n\n\n\n\n\n# !pip install trl transformers trl datasets\n\n\n\n\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom datasets import load_dataset\nimport os\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n\n\n\n\nWe will load SmolLM2-135M model and it’s tokenizer from HuggingFace. After that, we will generate output of the model on a specific input prompt related to Python (we ask the model to generate code for finding square root of a number)\n\n\nAs we can see, the model did not provide correct output. The model generate irrelevant text and keep repeating that. Let’s see how finetuning this model on task-specific dataset will help in increasing the model correctness.\n\n\nmodel_name = 'HuggingFaceTB/SmolLM2-135M'\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\ninput = 'Give me a Python code for finding square root of a number'\ntokenized_input = tokenizer(input, return_tensors='pt')\noutput = model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\n\nGive me a Python code for finding square root of a number.\n\n## Python Program to Find Square Root of a Number\n\nLet’s see how to find square root of a number in Python.\n\n``````# Python program to find square root of a number\n\n# This program will print the square root of a number\n\n# using the built-in function square root\n\n# This program will print the square root of a number\n\n# using the built-in function square root\n\n# This program will print the square root of a number\n\n# using the built-in function square\n\n\n\n\n\n\ndataset_path = 'jtatman/python-code-dataset-500k'\nds = load_dataset(dataset_path, split='train[:3%]')\nds = ds.train_test_split(test_size=0.1)\ntrain_dataset = ds['train']\ntest_dataset = ds['test']\n\n\nlen(train_dataset)\n\n15106\n\n\n\nlen(test_dataset)\n\n1679\n\n\n\ndef formatting_function(example):\n    output_texts = []\n    for i in range(len(example['instruction'])):\n        individual_prompt = \"###System: {}\\n###Instruction: {}\\n ###Output: {}\".format(example['system'][i],\n                                                                             example['instruction'][i],\n                                                                             example['output'][i])\n        output_texts.append(individual_prompt)\n    return output_texts\n\n\nresponse_template = \" ###Output:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\n\ntrainer_config = SFTConfig(\n    output_dir='.\\code_finetuned_mode',\n    max_steps=400,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    logging_steps=10,\n    ignore_data_skip=True\n)\n\n\ntorch.cuda.empty_cache()\n\ntrainer = SFTTrainer(\n    model=model,\n    args=trainer_config,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    formatting_func=formatting_function,\n    data_collator=collator\n)\ntrainer.train()\n\n\n    \n      \n      \n      [400/400 23:38, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n50\n0.839400\n0.743556\n\n\n100\n0.794000\n0.731545\n\n\n150\n0.715300\n0.723117\n\n\n200\n0.635300\n0.717372\n\n\n250\n0.720100\n0.713097\n\n\n300\n0.680100\n0.710268\n\n\n350\n0.711100\n0.707956\n\n\n400\n0.831900\n0.707255\n\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:169: UserWarning: Could not find response key ` ###Output:` in the following instance: ###System: You are a Python code architect, reviewing and designing scalable and efficient code\n###Instruction: Using the given information, write a function to calculate a 10-digit ISBN number. The ISBN number should be generated by combining various elements from the given information in a specific order.\n\nHere are the steps to generate the ISBN number:\n\n1. Take the first three letters of the author's last name (in this case, \"Doe\") and convert them to uppercase. If the author's last name has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"DOE\".\n\n2. Take the first three letters of the book title (in this case, \"Advanced\") and convert them to uppercase. If the title has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"ADV\".\n\n3. Take the first letter of the publisher's name (in this case, \"X\") and convert it to uppercase.\n\n4. Take the last two digits of the publication year (in this case, \"22\").\n\n5. Concatenate all the elements obtained in steps 1 to 4. The resulting string would be \"DOEADVX22\".\n\n6. Calculate the checksum digit of the ISBN number. To do this, multiply each digit of the concatenated string from step 5 by its position (starting from 1) and sum them up. For example, for the string \"DOEADVX22\", the calculation would be:\n   (1 * D) + (2 * O) + (3 * E) + (4 * A) + (5 * D) + (6 * V) + (7 * X) + (8 * 2) + (9 * 2) + (10 * 2)\n   Convert the result to modulo 11, and the remainder will be the checksum digit. If the remainder is 10, the checksum digit should be \"X\".\n\n7. Append the checksum digit obtained in step 6 to the end of the concatenated string from step 5. The resulting string would be the 10-digit ISBN number.\n\nWrite a function in your preferred programming language that implements the above steps and returns the calculated 10-digit ISBN number.\n\n#Rewritten Test - Increased Difficulty#\n\nUsing the given information, write a function to calculate a 10-digit ISBN number. The ISBN number should be generated by combining various elements from the given information in a specific order.\n\nHere are the steps to generate the ISBN number:\n\n1. Take the first three letters of the author's last name (in this case, \"Doe\") and convert them to uppercase. If the author's last name has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"DOE\".\n\n2. Take the first three letters of the book title (in this case, \"Advanced\") and convert them to uppercase. If the title has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"ADV\".\n\n3. Take the first letter of the publisher's name (in this case, \"X\") and convert it to uppercase.\n\n4. Take the last two digits of the publication year (in this case, \"22\").\n\n5. Concatenate all the elements obtained in steps 1 to 4. The resulting string would be \"DOEADVX22\".\n\n6. Calculate the checksum digit of the ISBN number. To do this, multiply each digit of the concatenated string from step 5 by its position (starting from 1) and sum them up. For example, for the string \"DOEADVX22\", the calculation would be:\n   (1 * D) + (2 * O) + (3 * E) + (4 * A) + (5 * D) + (6 * V) + (7 * X) + (8 * 2) + (9 * 2) + (10 * 2)\n   Convert the result to modulo 11, and the remainder will be the checksum digit. If the remainder is 10, the checksum digit should be \"X\".\n\n7. Append the checksum digit obtained in step 6 to the end of the concatenated string from step 5. The resulting string would be the 10-digit ISBN number.\n\nWrite a function in your preferred programming language that implements the above steps and returns the calculated 10-digit ISBN number.\n\nExample Input:\nAuthor Last Name: \"Doe\"\nBook Title. This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n  warnings.warn(\n\n\nTrainOutput(global_step=400, training_loss=0.7337326669692993, metrics={'train_runtime': 1419.3064, 'train_samples_per_second': 0.564, 'train_steps_per_second': 0.282, 'total_flos': 341935195820544.0, 'train_loss': 0.7337326669692993, 'epoch': 0.052959089103667416})\n\n\n\nfinetune_name = 'code_model_finetuned'\n\n\ntrainer.save_model(f'./{finetune_name}')\n\n\n\n\n\nfinetuned_model = AutoModelForCausalLM.from_pretrained(f'./{finetune_name}', local_files_only=True)\n\n\ninput = 'Give me a Python code for finding square root of a number'\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nGive me a Python code for finding square root of a number.\n\nHere's the code:\n\n```python\ndef square_root(num):\n    return num ** 0.5\n\nprint(square_root(10))  # Output: 5\nprint(square_root(20))  # Output: 10\nprint(square_root(30))  # Output: 40\n```\n\nIn this code, we define a function `square_root` that takes a number as input and returns its square root. We then call this function with `1"
  },
  {
    "objectID": "posts/code_fine_tuning.html#introduction-what-is-supervised-fine-tuning",
    "href": "posts/code_fine_tuning.html#introduction-what-is-supervised-fine-tuning",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "Before understanding what SFT is, we should understand what pre-training is. Pre-training involves training a model (generally a transformer) on a large corpus of text. Pre-training allows models to learn generalizable knowledge, grammar, semantics etc. However, the model is hardly usable after pre-training; as the model lacks task specific expertize.\n\n\nThat’s where Supervised Training plays a part. Supervised Fine-Tuning is used to adapt a pre-trained model to a specific task. It involves training the model on a labeled dataset, where the model learns to predict the correct label for each input.\n\n\nIn this article, we will load a pre-trained model from HuggingFace and finetune it on a specific dataset about Python programming"
  },
  {
    "objectID": "posts/code_fine_tuning.html#install-required-packages",
    "href": "posts/code_fine_tuning.html#install-required-packages",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "# !pip install trl transformers trl datasets"
  },
  {
    "objectID": "posts/code_fine_tuning.html#import-packages",
    "href": "posts/code_fine_tuning.html#import-packages",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom datasets import load_dataset\nimport os\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
  },
  {
    "objectID": "posts/code_fine_tuning.html#load-a-pretrained-model",
    "href": "posts/code_fine_tuning.html#load-a-pretrained-model",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "We will load SmolLM2-135M model and it’s tokenizer from HuggingFace. After that, we will generate output of the model on a specific input prompt related to Python (we ask the model to generate code for finding square root of a number)\n\n\nAs we can see, the model did not provide correct output. The model generate irrelevant text and keep repeating that. Let’s see how finetuning this model on task-specific dataset will help in increasing the model correctness.\n\n\nmodel_name = 'HuggingFaceTB/SmolLM2-135M'\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\ninput = 'Give me a Python code for finding square root of a number'\ntokenized_input = tokenizer(input, return_tensors='pt')\noutput = model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n\n\nGive me a Python code for finding square root of a number.\n\n## Python Program to Find Square Root of a Number\n\nLet’s see how to find square root of a number in Python.\n\n``````# Python program to find square root of a number\n\n# This program will print the square root of a number\n\n# using the built-in function square root\n\n# This program will print the square root of a number\n\n# using the built-in function square root\n\n# This program will print the square root of a number\n\n# using the built-in function square"
  },
  {
    "objectID": "posts/code_fine_tuning.html#load-dataset",
    "href": "posts/code_fine_tuning.html#load-dataset",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "dataset_path = 'jtatman/python-code-dataset-500k'\nds = load_dataset(dataset_path, split='train[:3%]')\nds = ds.train_test_split(test_size=0.1)\ntrain_dataset = ds['train']\ntest_dataset = ds['test']\n\n\nlen(train_dataset)\n\n15106\n\n\n\nlen(test_dataset)\n\n1679\n\n\n\ndef formatting_function(example):\n    output_texts = []\n    for i in range(len(example['instruction'])):\n        individual_prompt = \"###System: {}\\n###Instruction: {}\\n ###Output: {}\".format(example['system'][i],\n                                                                             example['instruction'][i],\n                                                                             example['output'][i])\n        output_texts.append(individual_prompt)\n    return output_texts\n\n\nresponse_template = \" ###Output:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\n\ntrainer_config = SFTConfig(\n    output_dir='.\\code_finetuned_mode',\n    max_steps=400,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    logging_steps=10,\n    ignore_data_skip=True\n)\n\n\ntorch.cuda.empty_cache()\n\ntrainer = SFTTrainer(\n    model=model,\n    args=trainer_config,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    formatting_func=formatting_function,\n    data_collator=collator\n)\ntrainer.train()\n\n\n    \n      \n      \n      [400/400 23:38, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n50\n0.839400\n0.743556\n\n\n100\n0.794000\n0.731545\n\n\n150\n0.715300\n0.723117\n\n\n200\n0.635300\n0.717372\n\n\n250\n0.720100\n0.713097\n\n\n300\n0.680100\n0.710268\n\n\n350\n0.711100\n0.707956\n\n\n400\n0.831900\n0.707255\n\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:169: UserWarning: Could not find response key ` ###Output:` in the following instance: ###System: You are a Python code architect, reviewing and designing scalable and efficient code\n###Instruction: Using the given information, write a function to calculate a 10-digit ISBN number. The ISBN number should be generated by combining various elements from the given information in a specific order.\n\nHere are the steps to generate the ISBN number:\n\n1. Take the first three letters of the author's last name (in this case, \"Doe\") and convert them to uppercase. If the author's last name has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"DOE\".\n\n2. Take the first three letters of the book title (in this case, \"Advanced\") and convert them to uppercase. If the title has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"ADV\".\n\n3. Take the first letter of the publisher's name (in this case, \"X\") and convert it to uppercase.\n\n4. Take the last two digits of the publication year (in this case, \"22\").\n\n5. Concatenate all the elements obtained in steps 1 to 4. The resulting string would be \"DOEADVX22\".\n\n6. Calculate the checksum digit of the ISBN number. To do this, multiply each digit of the concatenated string from step 5 by its position (starting from 1) and sum them up. For example, for the string \"DOEADVX22\", the calculation would be:\n   (1 * D) + (2 * O) + (3 * E) + (4 * A) + (5 * D) + (6 * V) + (7 * X) + (8 * 2) + (9 * 2) + (10 * 2)\n   Convert the result to modulo 11, and the remainder will be the checksum digit. If the remainder is 10, the checksum digit should be \"X\".\n\n7. Append the checksum digit obtained in step 6 to the end of the concatenated string from step 5. The resulting string would be the 10-digit ISBN number.\n\nWrite a function in your preferred programming language that implements the above steps and returns the calculated 10-digit ISBN number.\n\n#Rewritten Test - Increased Difficulty#\n\nUsing the given information, write a function to calculate a 10-digit ISBN number. The ISBN number should be generated by combining various elements from the given information in a specific order.\n\nHere are the steps to generate the ISBN number:\n\n1. Take the first three letters of the author's last name (in this case, \"Doe\") and convert them to uppercase. If the author's last name has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"DOE\".\n\n2. Take the first three letters of the book title (in this case, \"Advanced\") and convert them to uppercase. If the title has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"ADV\".\n\n3. Take the first letter of the publisher's name (in this case, \"X\") and convert it to uppercase.\n\n4. Take the last two digits of the publication year (in this case, \"22\").\n\n5. Concatenate all the elements obtained in steps 1 to 4. The resulting string would be \"DOEADVX22\".\n\n6. Calculate the checksum digit of the ISBN number. To do this, multiply each digit of the concatenated string from step 5 by its position (starting from 1) and sum them up. For example, for the string \"DOEADVX22\", the calculation would be:\n   (1 * D) + (2 * O) + (3 * E) + (4 * A) + (5 * D) + (6 * V) + (7 * X) + (8 * 2) + (9 * 2) + (10 * 2)\n   Convert the result to modulo 11, and the remainder will be the checksum digit. If the remainder is 10, the checksum digit should be \"X\".\n\n7. Append the checksum digit obtained in step 6 to the end of the concatenated string from step 5. The resulting string would be the 10-digit ISBN number.\n\nWrite a function in your preferred programming language that implements the above steps and returns the calculated 10-digit ISBN number.\n\nExample Input:\nAuthor Last Name: \"Doe\"\nBook Title. This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n  warnings.warn(\n\n\nTrainOutput(global_step=400, training_loss=0.7337326669692993, metrics={'train_runtime': 1419.3064, 'train_samples_per_second': 0.564, 'train_steps_per_second': 0.282, 'total_flos': 341935195820544.0, 'train_loss': 0.7337326669692993, 'epoch': 0.052959089103667416})\n\n\n\nfinetune_name = 'code_model_finetuned'\n\n\ntrainer.save_model(f'./{finetune_name}')"
  },
  {
    "objectID": "posts/code_fine_tuning.html#load-the-finetuned-model-and-perform-prediction-with-it",
    "href": "posts/code_fine_tuning.html#load-the-finetuned-model-and-perform-prediction-with-it",
    "title": "Supervised Fine-Tuning Part-1",
    "section": "",
    "text": "finetuned_model = AutoModelForCausalLM.from_pretrained(f'./{finetune_name}', local_files_only=True)\n\n\ninput = 'Give me a Python code for finding square root of a number'\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nGive me a Python code for finding square root of a number.\n\nHere's the code:\n\n```python\ndef square_root(num):\n    return num ** 0.5\n\nprint(square_root(10))  # Output: 5\nprint(square_root(20))  # Output: 10\nprint(square_root(30))  # Output: 40\n```\n\nIn this code, we define a function `square_root` that takes a number as input and returns its square root. We then call this function with `1"
  }
]