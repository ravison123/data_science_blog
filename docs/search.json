[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAudio Classification using Transformers\n\n\n\n\n\n\nTransformers\n\n\ndeep learning\n\n\nfine-tuning\n\n\n\nAttention is all you need!! In this blog, we implement music genre classification model using Transformers\n\n\n\n\n\nJan 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionality Reduction using Autoencoders\n\n\n\n\n\n\ndimensionality reduction\n\n\ndeep learning\n\n\n\nAutoencoder is unsupervised deep learning based technique. This blog implements Autoencoder using Pytorch on MNIST digits dataset.\n\n\n\n\n\nJan 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPreference Alignment using Direct Preference Optimization (DPO)\n\n\n\n\n\n\ntrl\n\n\nFine-Tuning\n\n\n\nA hands on guide to perform preference alignment of Large Language Models using Direct Preference Optimization (DPO) algorithm\n\n\n\n\n\nJan 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised Fine Tuning\n\n\n\n\n\n\ntrl\n\n\nFine-Tuning\n\n\n\nA hands on guide to fine-tune a Large Language model using trl library\n\n\n\n\n\nDec 23, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/code_fine_tuning.html",
    "href": "posts/code_fine_tuning.html",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "Before understanding what SFT is, we should understand what pre-training is. Pre-training involves training a model (generally a transformer) on a large corpus of text. Pre-training allows models to learn generalizable knowledge, grammar, semantics etc. However, the model is hardly usable after pre-training; as the model lacks task specific expertize.\n\n\nThat’s where Supervised Training plays a part. Supervised Fine-Tuning is used to adapt a pre-trained model to a specific task. It involves training the model on a labeled dataset, where the model learns to predict the correct label for each input.\n\n\nIn this article, we will load a pre-trained model from HuggingFace and finetune it on a specific dataset about Python programming.\n\n\n\n\nWe need transformers, trl and datasets packages. We will install the packages using pip:\n\n!pip install -q transformers==4.47.1 trl==0.13.0 datasets==3.2.0\n\n\n\n\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom datasets import load_dataset\nimport os\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n\n\n\n\nWe will load SmolLM2-135M model and it’s tokenizer from HuggingFace. After that, we will generate output of the model on a few specific input prompts related to Python. First, we ask a simple question of writing Python code for finding square of a number. After that, we ask model to determine if a string is palindrome (slightly difficult question than the first one).\n\n\nAs we can see, the model did not provide correct output in both the cases. The model generates some irrelevant text and keep repeating that afterwards. Let’s see how finetuning this model on task-specific dataset will help in increasing the model correctness.\n\n\nmodel_name = 'HuggingFaceTB/SmolLM2-135M'\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\ninput = 'Generate Python code for finding square of a number'\ntokenized_input = tokenizer(input, return_tensors='pt').to(device)\nmodel = model.to(device)\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nGenerate Python code for finding square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a\n\n\n\ninput = 'Write a program in Python to determine if a string is a palindrome'\ntokenized_input = tokenizer(input, return_tensors='pt').to(device)\nmodel = model.to(device)\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = model.generate(**tokenized_input, max_length=256)\nprint(tokenizer.decode(output[0]))\n\nWrite a program in Python to determine if a string is a palindrome.\n\n```python\ndef is_palindrome(s):\n    # Check if the string is a palindrome\n    if not s:\n        return False\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isdigit():\n        return True\n\n    # Check if the string is a letter\n    if s.islower():\n        return True\n\n    # Check if the string is a letter\n    if s.isupper():\n        return True\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isnumeric():\n        return True\n\n    # Check if the string is a letter\n    if s.islower():\n        return True\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isnumeric():\n        return True\n\n    # Check if the string is a letter\n    if s.isupper():\n        return True\n\n    # Check if the string is a\n\n\n\n\n\n\nWe will only load a fraction of data (3 %) for the ease of training with a single GPU.\nTrain dataset has around 15000 examples and test / evaluation dataset has 1679 examples.\n\n\ndataset_path = 'jtatman/python-code-dataset-500k'\nds = load_dataset(dataset_path, split='train[:3%]')\nds = ds.train_test_split(test_size=0.1)\ntrain_dataset = ds['train']\ntest_dataset = ds['test']\n\n\nlen(train_dataset)\n\n15106\n\n\n\nlen(test_dataset)\n\n1679\n\n\n\ntrain_dataset\n\nDataset({\n    features: ['output', 'instruction', 'system'],\n    num_rows: 15106\n})\n\n\n\n\n\n\nAs we can see from above, the dataset has 3 columns: system, instruction and output.\nFor finetuning the model, we need to combine the 3 columns into a single text field (which is achieved with formatting_function function)\n\n\ndef formatting_function(example):\n    output_texts = []\n    for i in range(len(example['instruction'])):\n        individual_prompt = \"###System: {}\\n###Instruction: {}\\n ###Output: {}\".format(example['system'][i],\n                                                                                       example['instruction'][i],\n                                                                                       example['output'][i])\n        output_texts.append(individual_prompt)\n    return output_texts\n\n\n\n\n\nWe also need to define a collator which needs response template and tokenizer as input.\nResponse template will help the trainer to identify output within the training prompt. If the training examples do not contain the response template, trainer will error out.\n\n\nresponse_template = \" ###Output:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n\n\n\n\n\n\n\ntrl library provides SFTConfig class using which we can provide training arguements.\nWe have used train and eval batch size of 2 due to memory constraints.\nWe have also used Gradient Accumulation and Gradient Checkpointing for memory optimization.\nIn addition, we have also used bf16=True in an effort to further optimize memory.\n\n\ntrainer_config = SFTConfig(\n    output_dir='.\\code_finetuned_mode',\n    max_steps=1000,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    logging_steps=10,\n    ignore_data_skip=True,\n    run_name='code_fine_tuning_23_12_2024',\n    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch\n    # Memory optimization\n    gradient_checkpointing=True,  # Trade compute for memory savings\n    optim=\"adamw_torch_fused\",\n    bf16=True\n)\n\n\n\n\n\nWe will empty cache before starting training to optimize memory.\ntrl library has SFTTrainer class to which we need to provide model, training arguments, training data, evaluation data, formatting function and data collator.\nTraining information will get logged to Weights and Biases.\nWe will save the trained model to a local directory.\n\n\ntorch.cuda.empty_cache()\n\n\ntrainer = SFTTrainer(\n    model=model,\n    args=trainer_config,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    formatting_func=formatting_function,\n    data_collator=collator\n)\ntrainer.train()\n\n\n\n\n\n\n\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n\n\n\n    \n      \n      \n      [ 101/1000 02:56 &lt; 26:41, 0.56 it/s, Epoch 0.03/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n\n\n    \n      \n      \n      [394/840 01:38 &lt; 01:52, 3.97 it/s]\n    \n    \n\n\n\n    \n      \n      \n      [1000/1000 1:05:45, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n100\n0.743100\n0.728401\n\n\n200\n0.835200\n0.708747\n\n\n300\n0.641200\n0.696030\n\n\n400\n0.726100\n0.687549\n\n\n500\n0.691100\n0.681275\n\n\n600\n0.687000\n0.676701\n\n\n700\n0.682500\n0.673061\n\n\n800\n0.701100\n0.670183\n\n\n900\n0.672400\n0.668563\n\n\n1000\n0.679100\n0.668029\n\n\n\n\n\n\nTrainOutput(global_step=1000, training_loss=0.7042460036277771, metrics={'train_runtime': 3947.6465, 'train_samples_per_second': 1.013, 'train_steps_per_second': 0.253, 'total_flos': 1717373605593600.0, 'train_loss': 0.7042460036277771, 'epoch': 0.26479544551833706})\n\n\n\n\n\n\n\n\nW&B Chart 12_23_2024, 3_26_48 AM.png\n\n\n\n\n\n\n\n\nW&B Chart 12_23_2024, 3_27_33 AM.png\n\n\n\nfinetune_name = 'code_model_finetuned'\n\n\ntrainer.save_model(f'./{finetune_name}')\n\n\n\n\n\n\nAs we can notice, the finetuned model has answered first question correctly.\nThe model tried to answer second question. However, the answer is not correct. However, the output is quite better than the non fine-tuned model. It has written correct to determine if a string is palindrome when it’s length is 2 :).\nDue to resource constraints, we used limited data and performed training for 1000 steps only. Considering that, I would say that the finetuned model is much better at Python coding than the baseline model.\n\n\nfinetuned_model = AutoModelForCausalLM.from_pretrained(f'./{finetune_name}', local_files_only=True)\n\n\ninput = \"\"\"##System: You are a Python code generator, \ncapable of creating scripts from specifications.\\n\n#Instruction: Generate Python code for finding square of a number\"\"\"\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\n##System: You are a Python code generator, capable of creating scripts from specifications.\n\n#Instruction: Generate Python code for finding square of a number\n\nGiven a number, find the square of that number.\n\nExample:\n\n```python\nnumber = 10\nsquare_number = number ** 2\nprint(square_number)\n```\n\nOutput:\n```\n10\n```\n\nExplanation:\n\n```python\nnumber = 10\nsquare_number = number ** 2\nprint(square_number)\n```\n\nOutput:\n```\n10\n\n\n\ninput = \"\"\"##System: You are a Python code generator, \ncapable of creating scripts from specifications.\\n\n#Instruction: Write a program in Python to determine if a string is a palindrome\n\"\"\"\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=256)\nprint(tokenizer.decode(output[0]))\n\n##System: You are a Python code generator, capable of creating scripts from specifications.\n\n#Instruction: Write a program in Python to determine if a string is a palindrome\n\nA palindrome is a word or a string that reads the same backward and forward. For example, the word \"hello\" is a palindrome because it reads the same backward and forward.\n\nHere's a program that determines if a string is a palindrome:\n\n```python\ndef is_palindrome(string):\n    # Check if the string is empty\n    if len(string) == 0:\n        return False\n\n    # Check if the string is a word\n    if len(string) == 2:\n        # Check if the first character is the same as the last character\n        if string[0] == string[-1]:\n            return True\n        else:\n            return False\n\n    # Check if the string is a word\n    if len(string) == 1:\n        return string[0] == string[-1]\n\n    # Check if the string is a word\n    if string[0] != string[-1]:\n        return False\n\n    # Check if the string is a word\n    if string[0] != string[-1\n\n\n\n\n\n\ntrl library provides high level and quite simple APIs to finetune a Large Language model.\nFollowing are Pros and Cons of the finetuning approach described in this article:\n\nPros:\n\nImproved Task Performance: Fine-tuning enables a model to specialize in specific tasks, often achieving better accuracy compared to general-purpose models.\nData Efficiency: Requires much less labeled data compared to training from scratch due to pre-trained knowledge.\nCustomizability: Allows tailoring models to domain-specific or task-specific needs (e.g., medical, legal, or financial contexts).\n\nCons:\n\nRisk of Overfitting: On small datasets, the model may overfit, reducing its ability to generalize to unseen data.\nCatastrophic Forgetting: Fine-tuning can overwrite pre-trained knowledge, reducing performance on general tasks.\nResource Requirements: Fine-tuning still demands significant computational resources, especially for large models, compared to lightweight alternatives like prompt engineering.\nUpdation of All Parameters: The finetuning approach described in this article updates all parameters of the model. Therefore, storing of the updated model is quite costly. Therefore, more efficient approaches of finetuning like PEFT and LoRA are preferred.\n\n\n\n\n\n\n\nTRL (Transformer Reinforcement Learning) Documentation:\nSmolLM2-135M Model on HuggingFace\nDataset for Training"
  },
  {
    "objectID": "posts/code_fine_tuning.html#introduction-what-is-supervised-fine-tuning-sft",
    "href": "posts/code_fine_tuning.html#introduction-what-is-supervised-fine-tuning-sft",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "Before understanding what SFT is, we should understand what pre-training is. Pre-training involves training a model (generally a transformer) on a large corpus of text. Pre-training allows models to learn generalizable knowledge, grammar, semantics etc. However, the model is hardly usable after pre-training; as the model lacks task specific expertize.\n\n\nThat’s where Supervised Training plays a part. Supervised Fine-Tuning is used to adapt a pre-trained model to a specific task. It involves training the model on a labeled dataset, where the model learns to predict the correct label for each input.\n\n\nIn this article, we will load a pre-trained model from HuggingFace and finetune it on a specific dataset about Python programming."
  },
  {
    "objectID": "posts/code_fine_tuning.html#install-required-packages",
    "href": "posts/code_fine_tuning.html#install-required-packages",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "We need transformers, trl and datasets packages. We will install the packages using pip:\n\n!pip install -q transformers==4.47.1 trl==0.13.0 datasets==3.2.0"
  },
  {
    "objectID": "posts/code_fine_tuning.html#import-packages",
    "href": "posts/code_fine_tuning.html#import-packages",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom datasets import load_dataset\nimport os\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
  },
  {
    "objectID": "posts/code_fine_tuning.html#load-a-pretrained-model",
    "href": "posts/code_fine_tuning.html#load-a-pretrained-model",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "We will load SmolLM2-135M model and it’s tokenizer from HuggingFace. After that, we will generate output of the model on a few specific input prompts related to Python. First, we ask a simple question of writing Python code for finding square of a number. After that, we ask model to determine if a string is palindrome (slightly difficult question than the first one).\n\n\nAs we can see, the model did not provide correct output in both the cases. The model generates some irrelevant text and keep repeating that afterwards. Let’s see how finetuning this model on task-specific dataset will help in increasing the model correctness.\n\n\nmodel_name = 'HuggingFaceTB/SmolLM2-135M'\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\ninput = 'Generate Python code for finding square of a number'\ntokenized_input = tokenizer(input, return_tensors='pt').to(device)\nmodel = model.to(device)\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\nGenerate Python code for finding square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a number\n\nPython code to find square of a\n\n\n\ninput = 'Write a program in Python to determine if a string is a palindrome'\ntokenized_input = tokenizer(input, return_tensors='pt').to(device)\nmodel = model.to(device)\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = model.generate(**tokenized_input, max_length=256)\nprint(tokenizer.decode(output[0]))\n\nWrite a program in Python to determine if a string is a palindrome.\n\n```python\ndef is_palindrome(s):\n    # Check if the string is a palindrome\n    if not s:\n        return False\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isdigit():\n        return True\n\n    # Check if the string is a letter\n    if s.islower():\n        return True\n\n    # Check if the string is a letter\n    if s.isupper():\n        return True\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isnumeric():\n        return True\n\n    # Check if the string is a letter\n    if s.islower():\n        return True\n\n    # Check if the string is a word\n    if len(s) != 1:\n        return False\n\n    # Check if the string is a number\n    if s.isnumeric():\n        return True\n\n    # Check if the string is a letter\n    if s.isupper():\n        return True\n\n    # Check if the string is a"
  },
  {
    "objectID": "posts/code_fine_tuning.html#load-dataset",
    "href": "posts/code_fine_tuning.html#load-dataset",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "We will only load a fraction of data (3 %) for the ease of training with a single GPU.\nTrain dataset has around 15000 examples and test / evaluation dataset has 1679 examples.\n\n\ndataset_path = 'jtatman/python-code-dataset-500k'\nds = load_dataset(dataset_path, split='train[:3%]')\nds = ds.train_test_split(test_size=0.1)\ntrain_dataset = ds['train']\ntest_dataset = ds['test']\n\n\nlen(train_dataset)\n\n15106\n\n\n\nlen(test_dataset)\n\n1679\n\n\n\ntrain_dataset\n\nDataset({\n    features: ['output', 'instruction', 'system'],\n    num_rows: 15106\n})"
  },
  {
    "objectID": "posts/code_fine_tuning.html#define-a-formatting-function",
    "href": "posts/code_fine_tuning.html#define-a-formatting-function",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "As we can see from above, the dataset has 3 columns: system, instruction and output.\nFor finetuning the model, we need to combine the 3 columns into a single text field (which is achieved with formatting_function function)\n\n\ndef formatting_function(example):\n    output_texts = []\n    for i in range(len(example['instruction'])):\n        individual_prompt = \"###System: {}\\n###Instruction: {}\\n ###Output: {}\".format(example['system'][i],\n                                                                                       example['instruction'][i],\n                                                                                       example['output'][i])\n        output_texts.append(individual_prompt)\n    return output_texts"
  },
  {
    "objectID": "posts/code_fine_tuning.html#define-a-collator",
    "href": "posts/code_fine_tuning.html#define-a-collator",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "We also need to define a collator which needs response template and tokenizer as input.\nResponse template will help the trainer to identify output within the training prompt. If the training examples do not contain the response template, trainer will error out.\n\n\nresponse_template = \" ###Output:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
  },
  {
    "objectID": "posts/code_fine_tuning.html#training",
    "href": "posts/code_fine_tuning.html#training",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "trl library provides SFTConfig class using which we can provide training arguements.\nWe have used train and eval batch size of 2 due to memory constraints.\nWe have also used Gradient Accumulation and Gradient Checkpointing for memory optimization.\nIn addition, we have also used bf16=True in an effort to further optimize memory.\n\n\ntrainer_config = SFTConfig(\n    output_dir='.\\code_finetuned_mode',\n    max_steps=1000,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    logging_steps=10,\n    ignore_data_skip=True,\n    run_name='code_fine_tuning_23_12_2024',\n    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch\n    # Memory optimization\n    gradient_checkpointing=True,  # Trade compute for memory savings\n    optim=\"adamw_torch_fused\",\n    bf16=True\n)\n\n\n\n\n\nWe will empty cache before starting training to optimize memory.\ntrl library has SFTTrainer class to which we need to provide model, training arguments, training data, evaluation data, formatting function and data collator.\nTraining information will get logged to Weights and Biases.\nWe will save the trained model to a local directory.\n\n\ntorch.cuda.empty_cache()\n\n\ntrainer = SFTTrainer(\n    model=model,\n    args=trainer_config,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    formatting_func=formatting_function,\n    data_collator=collator\n)\ntrainer.train()\n\n\n\n\n\n\n\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n\n\n\n    \n      \n      \n      [ 101/1000 02:56 &lt; 26:41, 0.56 it/s, Epoch 0.03/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n\n\n    \n      \n      \n      [394/840 01:38 &lt; 01:52, 3.97 it/s]\n    \n    \n\n\n\n    \n      \n      \n      [1000/1000 1:05:45, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n100\n0.743100\n0.728401\n\n\n200\n0.835200\n0.708747\n\n\n300\n0.641200\n0.696030\n\n\n400\n0.726100\n0.687549\n\n\n500\n0.691100\n0.681275\n\n\n600\n0.687000\n0.676701\n\n\n700\n0.682500\n0.673061\n\n\n800\n0.701100\n0.670183\n\n\n900\n0.672400\n0.668563\n\n\n1000\n0.679100\n0.668029\n\n\n\n\n\n\nTrainOutput(global_step=1000, training_loss=0.7042460036277771, metrics={'train_runtime': 3947.6465, 'train_samples_per_second': 1.013, 'train_steps_per_second': 0.253, 'total_flos': 1717373605593600.0, 'train_loss': 0.7042460036277771, 'epoch': 0.26479544551833706})\n\n\n\n\n\n\n\n\nW&B Chart 12_23_2024, 3_26_48 AM.png\n\n\n\n\n\n\n\n\nW&B Chart 12_23_2024, 3_27_33 AM.png\n\n\n\nfinetune_name = 'code_model_finetuned'\n\n\ntrainer.save_model(f'./{finetune_name}')"
  },
  {
    "objectID": "posts/code_fine_tuning.html#load-the-finetuned-model-and-perform-prediction-with-it",
    "href": "posts/code_fine_tuning.html#load-the-finetuned-model-and-perform-prediction-with-it",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "As we can notice, the finetuned model has answered first question correctly.\nThe model tried to answer second question. However, the answer is not correct. However, the output is quite better than the non fine-tuned model. It has written correct to determine if a string is palindrome when it’s length is 2 :).\nDue to resource constraints, we used limited data and performed training for 1000 steps only. Considering that, I would say that the finetuned model is much better at Python coding than the baseline model.\n\n\nfinetuned_model = AutoModelForCausalLM.from_pretrained(f'./{finetune_name}', local_files_only=True)\n\n\ninput = \"\"\"##System: You are a Python code generator, \ncapable of creating scripts from specifications.\\n\n#Instruction: Generate Python code for finding square of a number\"\"\"\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=128)\nprint(tokenizer.decode(output[0]))\n\n##System: You are a Python code generator, capable of creating scripts from specifications.\n\n#Instruction: Generate Python code for finding square of a number\n\nGiven a number, find the square of that number.\n\nExample:\n\n```python\nnumber = 10\nsquare_number = number ** 2\nprint(square_number)\n```\n\nOutput:\n```\n10\n```\n\nExplanation:\n\n```python\nnumber = 10\nsquare_number = number ** 2\nprint(square_number)\n```\n\nOutput:\n```\n10\n\n\n\ninput = \"\"\"##System: You are a Python code generator, \ncapable of creating scripts from specifications.\\n\n#Instruction: Write a program in Python to determine if a string is a palindrome\n\"\"\"\ntokenized_input = tokenizer(input, return_tensors='pt')\nfinetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\noutput = finetuned_model.generate(**tokenized_input, max_length=256)\nprint(tokenizer.decode(output[0]))\n\n##System: You are a Python code generator, capable of creating scripts from specifications.\n\n#Instruction: Write a program in Python to determine if a string is a palindrome\n\nA palindrome is a word or a string that reads the same backward and forward. For example, the word \"hello\" is a palindrome because it reads the same backward and forward.\n\nHere's a program that determines if a string is a palindrome:\n\n```python\ndef is_palindrome(string):\n    # Check if the string is empty\n    if len(string) == 0:\n        return False\n\n    # Check if the string is a word\n    if len(string) == 2:\n        # Check if the first character is the same as the last character\n        if string[0] == string[-1]:\n            return True\n        else:\n            return False\n\n    # Check if the string is a word\n    if len(string) == 1:\n        return string[0] == string[-1]\n\n    # Check if the string is a word\n    if string[0] != string[-1]:\n        return False\n\n    # Check if the string is a word\n    if string[0] != string[-1"
  },
  {
    "objectID": "posts/code_fine_tuning.html#conclusion",
    "href": "posts/code_fine_tuning.html#conclusion",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "trl library provides high level and quite simple APIs to finetune a Large Language model.\nFollowing are Pros and Cons of the finetuning approach described in this article:\n\nPros:\n\nImproved Task Performance: Fine-tuning enables a model to specialize in specific tasks, often achieving better accuracy compared to general-purpose models.\nData Efficiency: Requires much less labeled data compared to training from scratch due to pre-trained knowledge.\nCustomizability: Allows tailoring models to domain-specific or task-specific needs (e.g., medical, legal, or financial contexts).\n\nCons:\n\nRisk of Overfitting: On small datasets, the model may overfit, reducing its ability to generalize to unseen data.\nCatastrophic Forgetting: Fine-tuning can overwrite pre-trained knowledge, reducing performance on general tasks.\nResource Requirements: Fine-tuning still demands significant computational resources, especially for large models, compared to lightweight alternatives like prompt engineering.\nUpdation of All Parameters: The finetuning approach described in this article updates all parameters of the model. Therefore, storing of the updated model is quite costly. Therefore, more efficient approaches of finetuning like PEFT and LoRA are preferred."
  },
  {
    "objectID": "posts/code_fine_tuning.html#references",
    "href": "posts/code_fine_tuning.html#references",
    "title": "Supervised Fine Tuning",
    "section": "",
    "text": "TRL (Transformer Reinforcement Learning) Documentation:\nSmolLM2-135M Model on HuggingFace\nDataset for Training"
  },
  {
    "objectID": "posts/audio_classification_final.html",
    "href": "posts/audio_classification_final.html",
    "title": "Audio Classification using Transformers",
    "section": "",
    "text": "Transformers based models have been at the forefront of the Artificial Intelligence, primarily due to huge success in natural language processing (NLP) tasks. Transformers work so well due to the mechanism of self attention which helps the model to learn long range dependencies. However, their applications extend well beyond text. In fact, the attention mechanism can be applied and work exceptionally well on images, audio and video data as well.\nIn this tutorial, we will understand the applications of transformers for audio classification task. We will take a pretrained transformer based model from huggingface and finetune it for the purpose of music genre classification.\n\n\n\nFor this tutorial, we need to install pytorch, datasets, transformers and evaluate packages. We will use Weights and Biases for logging.\n\n# !pip -q install datasets==3.2.0\n# !pip -q install transformers==4.8.1\n# !pip -q install evaluate==0.4.3\n# !pip -q install torch==2.5.1\n\n\nfrom datasets import load_dataset, Audio\nimport evaluate\nimport IPython\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer\nimport wandb\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom google.colab import userdata\nwandb.login(key=userdata.get('WB_TOKEN'))\n\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\ndevice\n\n'cuda'\n\n\n\n\n\n\nWe will use marsyas/gtzan dataset from huggingface. The dataset consists of 1,000 audio tracks, each of 30 seconds long. It contains 10 genres, each represented by 100 tracks. The tracks are all 22,050Hz Mono 16-bit audio files in WAV format. The genres are: blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, and rock.\nEach example contains audio in the form of array. The elements of the array represent amplitude of the audio signals at various timestamps. Since, each audio has sampling rate of 22050Hz, the array contains 22050 elements for each second of the audio.\nWe can also listen to some of the audio clips using Audio class from IPython package.\n\n\ndata = load_dataset('marsyas/gtzan', trust_remote_code=True)\n\n\ndata\n\nDatasetDict({\n    train: Dataset({\n        features: ['file', 'audio', 'genre'],\n        num_rows: 999\n    })\n})\n\n\n\ndata['train'][0]\n\n{'file': '/root/.cache/huggingface/datasets/downloads/extracted/3b204381d6c029312e4f9c569c6b1130af3041dd36ca38ca53d4e20f585e39c6/genres/blues/blues.00000.wav',\n 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/3b204381d6c029312e4f9c569c6b1130af3041dd36ca38ca53d4e20f585e39c6/genres/blues/blues.00000.wav',\n  'array': array([ 0.00732422,  0.01660156,  0.00762939, ..., -0.05560303,\n         -0.06106567, -0.06417847]),\n  'sampling_rate': 22050},\n 'genre': 0}\n\n\n\ndata['train'][0]['audio']['array']\n\narray([ 0.00732422,  0.01660156,  0.00762939, ..., -0.05560303,\n       -0.06106567, -0.06417847])\n\n\n\nlen(data['train'][0]['audio']['array'])\n\n661794\n\n\n\nplt.plot(data['train'][100]['audio']['array'])\n\n\n\n\n\n\n\n\n\nplt.plot(data['train'][500]['audio']['array'])\n\n\n\n\n\n\n\n\n\nprint('Genre is: {}'.format(id2label[data['train'][0]['genre']]))\nIPython.display.Audio(data['train'][0]['audio']['array'], \n                      rate=data['train'][0]['audio']['sampling_rate'])\n\nGenre is: blues\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nprint('Genre is: {}'.format(id2label[data['train'][100]['genre']]))\nIPython.display.Audio(data['train'][1]['audio']['array'], \n                      rate=data['train'][0]['audio']['sampling_rate'])\n\nGenre is: classical\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nprint('Genre is: {}'.format(id2label[data['train'][500]['genre']]))\nIPython.display.Audio(data['train'][1]['audio']['array'], \n                      rate=data['train'][0]['audio']['sampling_rate'])\n\nGenre is: jazz\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nfeatures = data['train'].features\nfeatures\n\n{'file': Value(dtype='string', id=None),\n 'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None),\n 'genre': ClassLabel(names=['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock'], id=None)}\n\n\n\nlabel_names = features['genre'].names\nlabel_names\n\n['blues',\n 'classical',\n 'country',\n 'disco',\n 'hiphop',\n 'jazz',\n 'metal',\n 'pop',\n 'reggae',\n 'rock']\n\n\n\nid2label = {id:label for id, label in enumerate(label_names)}\nid2label\n\n{0: 'blues',\n 1: 'classical',\n 2: 'country',\n 3: 'disco',\n 4: 'hiphop',\n 5: 'jazz',\n 6: 'metal',\n 7: 'pop',\n 8: 'reggae',\n 9: 'rock'}\n\n\n\nlabel2id = {v:k for k, v in id2label.items()}\nlabel2id\n\n{'blues': 0,\n 'classical': 1,\n 'country': 2,\n 'disco': 3,\n 'hiphop': 4,\n 'jazz': 5,\n 'metal': 6,\n 'pop': 7,\n 'reggae': 8,\n 'rock': 9}\n\n\n\n\n\n\nFor this audio classification task, we will use facebook/wav2vec2-base model from Huggingface. This is a 94 million parameter model and is trained on 53K hours of audio data.\nBecause of extensive pre-training, the model has quite a lot understanding about the audio features making it quite suitable for various audio finetuning tasks.\nFirst, we load the feature extractor of the model using AutoFeatureExtractor.from_pretrained class. The extractor to audio transformers is what tokenizer is to text transformers. They provide quite convinient functionality to process the input data and make it suitable for feeding it to the model.\nSampling rate of feature extractor (and hence the model) is 16,000 Hz. Therefore, we need to change the sampling rate of data from 22,500 Hz to 16,000 Hz using cast_colum method.\nAfter that, we use preprocess_function to apply transformation of feature extractor to all examples in training data. We use max_length of 30*16000 as we have audio clips of 30 seconds and sampling rate is 16,000 Hz.\nWe load the model using AutoModelForAudioClassification.from_pretrained method. The pretrained model has 256 neurons in the last layer. Therefore, we need to specify num_labels, labe2id and id2label while loading the model. This changes the last layer of the model and newly initializes it. Because of this, the loaded model will have close to random performance on the audio classification task and fine-tuning needs to be performed before using it.\n\n\nmodel_name = 'facebook/wav2vec2-base'\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_name,\n                                                         return_tensors='pt',\n                                                         do_normalize=True,\n                                                         return_attention_mask=True)\n\n\nfeature_extractor.sampling_rate\n\n16000\n\n\n\ndata = data.cast_column(\"audio\", Audio(sampling_rate=16000))\n\n\ndata['train'][0]\n\n{'file': '/root/.cache/huggingface/datasets/downloads/extracted/3b204381d6c029312e4f9c569c6b1130af3041dd36ca38ca53d4e20f585e39c6/genres/blues/blues.00000.wav',\n 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/3b204381d6c029312e4f9c569c6b1130af3041dd36ca38ca53d4e20f585e39c6/genres/blues/blues.00000.wav',\n  'array': array([ 0.00766408,  0.01497506, -0.00178931, ..., -0.05508532,\n         -0.06770003,  0.        ]),\n  'sampling_rate': 16000},\n 'genre': 0}\n\n\n\ndef preprocess_function(examples):\n    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n    inputs = feature_extractor(\n        audio_arrays, sampling_rate=feature_extractor.sampling_rate, \n        padding=True, truncation=True,\n        max_length=30*16000\n    )\n    return inputs\n\n\nencoded_dataset = data.map(preprocess_function, remove_columns=[\"audio\"], batched=True)\n\n\n\n\n\nencoded_dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['file', 'genre', 'input_values', 'attention_mask'],\n        num_rows: 999\n    })\n})\n\n\n\nsplit_dataset = encoded_dataset['train'].train_test_split(test_size=0.2)\n\n\ntrain_dataset = split_dataset['train']\ntrain_dataset\n\nDataset({\n    features: ['file', 'genre', 'input_values', 'attention_mask'],\n    num_rows: 799\n})\n\n\n\neval_dataset = split_dataset['test']\neval_dataset\n\nDataset({\n    features: ['file', 'genre', 'input_values', 'attention_mask'],\n    num_rows: 200\n})\n\n\n\nnum_labels = 10\nmodel = AutoModelForAudioClassification.from_pretrained(model_name,\n                                                        num_labels=num_labels,\n                                                        label2id=label2id,\n                                                        id2label=id2label)\n\n\ntotal_params = 0\nfor param in model.parameters():\n    total_params = total_params + param.numel()\ntotal_params / 1e+06\n\n94.571146\n\n\n\ntrain_dataset\n\nDataset({\n    features: ['file', 'genre', 'input_values', 'attention_mask'],\n    num_rows: 799\n})\n\n\n\ntrain_dataset = train_dataset.rename_column(\"genre\", \"label\")\neval_dataset = eval_dataset.rename_column(\"genre\", \"label\")\n\n\n\n\n\nWe specify training arguements using TrainingArguments class from transformers library. We will train the model for 10 epochs and use small batch size of 8 due to resource constraints.\nAfter that, we create a helper function to calculate accuracy using evaluate package.\nTraining of the model is quite easy with Trainer class from transformers library.\nWe can see that after 10 epochs, we achieve accuracy of 83% on evaluation dataset. The training loss is 0.18 and validatio loss is 0.67. This indicates that the model has overfit the data. This was kind of expected because of small size of the dataset and huge pre-trained model size.\nHowever, accuracy of 83% after training for only 10 epochs is quite good.\n\n\nmodel_name_1 = model_name.split(\"/\")[-1]\nbatch_size = 8\ngradient_accumulation_steps = 1\nnum_train_epochs = 10\n\ntraining_args = TrainingArguments(\n    f\"{model_name_1}-finetuned-gtzan\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_train_epochs,\n    warmup_ratio=0.1,\n    logging_steps=5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    fp16=True,\n    push_to_hub=False,\n)\n\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\n\ntrainer.train()\n\nwandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\n\nTracking run with wandb version 0.19.1\n\n\nRun data is saved locally in /kaggle/working/wandb/run-20250126_081553-c1u0om9e\n\n\nSyncing run wav2vec2-base-finetuned-gtzan to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/sonawane-ravindra1/huggingface\n\n\n View run at https://wandb.ai/sonawane-ravindra1/huggingface/runs/c1u0om9e\n\n\n\n    \n      \n      \n      [1000/1000 2:20:27, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n1.864500\n1.984971\n0.400000\n\n\n2\n1.537700\n1.635146\n0.395000\n\n\n3\n1.231000\n1.346450\n0.540000\n\n\n4\n0.958400\n1.088597\n0.635000\n\n\n5\n0.722100\n0.903491\n0.740000\n\n\n6\n0.817800\n0.816501\n0.775000\n\n\n7\n0.697800\n0.833347\n0.785000\n\n\n8\n0.207900\n0.850784\n0.790000\n\n\n9\n0.179600\n0.697149\n0.825000\n\n\n10\n0.177400\n0.667886\n0.830000\n\n\n\n\n\n\nTrainOutput(global_step=1000, training_loss=0.926927893102169, metrics={'train_runtime': 8447.1145, 'train_samples_per_second': 0.946, 'train_steps_per_second': 0.118, 'total_flos': 2.1761955548352e+18, 'train_loss': 0.926927893102169, 'epoch': 10.0})\n\n\n\n\n\n\n\n\nWe learned to fine-tune a large pre-trained model to achieve reasonably good performance on audio classification task.\nIt’s amazing to see how the approach for finetuning for audio task is quite similar to that of finetuning across other modalities.\nWe noticed that our model suffered from overfitting. Here are some of the approaches that can be used to mitigate overfitting and achieve accuracy beyond 83%:\n\nData Augmentation: Number of training examples can be increased by using data augmentation. A library like audiomentations can be used for this purpose.\nRegularization: L1 or L2 regularization can be used to reduce overfitting.\nUse Smaller Model: Since the model we have used is quite big, using a smaller model might be a great idea considering the simple nature of the classificatio task.\nFine-tune few layers: In the above example, we fine-tuned the entire model (all parameters of the model). However, only finetuning last layers or only a few attention heads will help to reduce overfitting. This will also help to speed up training and will consume less resources.\n\n\n\n\n\n\nDataset\nModel"
  },
  {
    "objectID": "posts/audio_classification_final.html#attention-is-all-you-need",
    "href": "posts/audio_classification_final.html#attention-is-all-you-need",
    "title": "Audio Classification using Transformers",
    "section": "",
    "text": "Transformers based models have been at the forefront of the Artificial Intelligence, primarily due to huge success in natural language processing (NLP) tasks. Transformers work so well due to the mechanism of self attention which helps the model to learn long range dependencies. However, their applications extend well beyond text. In fact, the attention mechanism can be applied and work exceptionally well on images, audio and video data as well.\nIn this tutorial, we will understand the applications of transformers for audio classification task. We will take a pretrained transformer based model from huggingface and finetune it for the purpose of music genre classification."
  },
  {
    "objectID": "posts/audio_classification_final.html#import-packages",
    "href": "posts/audio_classification_final.html#import-packages",
    "title": "Audio Classification using Transformers",
    "section": "",
    "text": "For this tutorial, we need to install pytorch, datasets, transformers and evaluate packages. We will use Weights and Biases for logging.\n\n# !pip -q install datasets==3.2.0\n# !pip -q install transformers==4.8.1\n# !pip -q install evaluate==0.4.3\n# !pip -q install torch==2.5.1\n\n\nfrom datasets import load_dataset, Audio\nimport evaluate\nimport IPython\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer\nimport wandb\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom google.colab import userdata\nwandb.login(key=userdata.get('WB_TOKEN'))\n\n\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\ndevice\n\n'cuda'"
  },
  {
    "objectID": "posts/audio_classification_final.html#load-and-inspect-dataset",
    "href": "posts/audio_classification_final.html#load-and-inspect-dataset",
    "title": "Audio Classification using Transformers",
    "section": "",
    "text": "We will use marsyas/gtzan dataset from huggingface. The dataset consists of 1,000 audio tracks, each of 30 seconds long. It contains 10 genres, each represented by 100 tracks. The tracks are all 22,050Hz Mono 16-bit audio files in WAV format. The genres are: blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, and rock.\nEach example contains audio in the form of array. The elements of the array represent amplitude of the audio signals at various timestamps. Since, each audio has sampling rate of 22050Hz, the array contains 22050 elements for each second of the audio.\nWe can also listen to some of the audio clips using Audio class from IPython package.\n\n\ndata = load_dataset('marsyas/gtzan', trust_remote_code=True)\n\n\ndata\n\nDatasetDict({\n    train: Dataset({\n        features: ['file', 'audio', 'genre'],\n        num_rows: 999\n    })\n})\n\n\n\ndata['train'][0]\n\n{'file': '/root/.cache/huggingface/datasets/downloads/extracted/3b204381d6c029312e4f9c569c6b1130af3041dd36ca38ca53d4e20f585e39c6/genres/blues/blues.00000.wav',\n 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/3b204381d6c029312e4f9c569c6b1130af3041dd36ca38ca53d4e20f585e39c6/genres/blues/blues.00000.wav',\n  'array': array([ 0.00732422,  0.01660156,  0.00762939, ..., -0.05560303,\n         -0.06106567, -0.06417847]),\n  'sampling_rate': 22050},\n 'genre': 0}\n\n\n\ndata['train'][0]['audio']['array']\n\narray([ 0.00732422,  0.01660156,  0.00762939, ..., -0.05560303,\n       -0.06106567, -0.06417847])\n\n\n\nlen(data['train'][0]['audio']['array'])\n\n661794\n\n\n\nplt.plot(data['train'][100]['audio']['array'])\n\n\n\n\n\n\n\n\n\nplt.plot(data['train'][500]['audio']['array'])\n\n\n\n\n\n\n\n\n\nprint('Genre is: {}'.format(id2label[data['train'][0]['genre']]))\nIPython.display.Audio(data['train'][0]['audio']['array'], \n                      rate=data['train'][0]['audio']['sampling_rate'])\n\nGenre is: blues\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nprint('Genre is: {}'.format(id2label[data['train'][100]['genre']]))\nIPython.display.Audio(data['train'][1]['audio']['array'], \n                      rate=data['train'][0]['audio']['sampling_rate'])\n\nGenre is: classical\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nprint('Genre is: {}'.format(id2label[data['train'][500]['genre']]))\nIPython.display.Audio(data['train'][1]['audio']['array'], \n                      rate=data['train'][0]['audio']['sampling_rate'])\n\nGenre is: jazz\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nfeatures = data['train'].features\nfeatures\n\n{'file': Value(dtype='string', id=None),\n 'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None),\n 'genre': ClassLabel(names=['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock'], id=None)}\n\n\n\nlabel_names = features['genre'].names\nlabel_names\n\n['blues',\n 'classical',\n 'country',\n 'disco',\n 'hiphop',\n 'jazz',\n 'metal',\n 'pop',\n 'reggae',\n 'rock']\n\n\n\nid2label = {id:label for id, label in enumerate(label_names)}\nid2label\n\n{0: 'blues',\n 1: 'classical',\n 2: 'country',\n 3: 'disco',\n 4: 'hiphop',\n 5: 'jazz',\n 6: 'metal',\n 7: 'pop',\n 8: 'reggae',\n 9: 'rock'}\n\n\n\nlabel2id = {v:k for k, v in id2label.items()}\nlabel2id\n\n{'blues': 0,\n 'classical': 1,\n 'country': 2,\n 'disco': 3,\n 'hiphop': 4,\n 'jazz': 5,\n 'metal': 6,\n 'pop': 7,\n 'reggae': 8,\n 'rock': 9}"
  },
  {
    "objectID": "posts/audio_classification_final.html#load-pretrained-model",
    "href": "posts/audio_classification_final.html#load-pretrained-model",
    "title": "Audio Classification using Transformers",
    "section": "",
    "text": "For this audio classification task, we will use facebook/wav2vec2-base model from Huggingface. This is a 94 million parameter model and is trained on 53K hours of audio data.\nBecause of extensive pre-training, the model has quite a lot understanding about the audio features making it quite suitable for various audio finetuning tasks.\nFirst, we load the feature extractor of the model using AutoFeatureExtractor.from_pretrained class. The extractor to audio transformers is what tokenizer is to text transformers. They provide quite convinient functionality to process the input data and make it suitable for feeding it to the model.\nSampling rate of feature extractor (and hence the model) is 16,000 Hz. Therefore, we need to change the sampling rate of data from 22,500 Hz to 16,000 Hz using cast_colum method.\nAfter that, we use preprocess_function to apply transformation of feature extractor to all examples in training data. We use max_length of 30*16000 as we have audio clips of 30 seconds and sampling rate is 16,000 Hz.\nWe load the model using AutoModelForAudioClassification.from_pretrained method. The pretrained model has 256 neurons in the last layer. Therefore, we need to specify num_labels, labe2id and id2label while loading the model. This changes the last layer of the model and newly initializes it. Because of this, the loaded model will have close to random performance on the audio classification task and fine-tuning needs to be performed before using it.\n\n\nmodel_name = 'facebook/wav2vec2-base'\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_name,\n                                                         return_tensors='pt',\n                                                         do_normalize=True,\n                                                         return_attention_mask=True)\n\n\nfeature_extractor.sampling_rate\n\n16000\n\n\n\ndata = data.cast_column(\"audio\", Audio(sampling_rate=16000))\n\n\ndata['train'][0]\n\n{'file': '/root/.cache/huggingface/datasets/downloads/extracted/3b204381d6c029312e4f9c569c6b1130af3041dd36ca38ca53d4e20f585e39c6/genres/blues/blues.00000.wav',\n 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/3b204381d6c029312e4f9c569c6b1130af3041dd36ca38ca53d4e20f585e39c6/genres/blues/blues.00000.wav',\n  'array': array([ 0.00766408,  0.01497506, -0.00178931, ..., -0.05508532,\n         -0.06770003,  0.        ]),\n  'sampling_rate': 16000},\n 'genre': 0}\n\n\n\ndef preprocess_function(examples):\n    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n    inputs = feature_extractor(\n        audio_arrays, sampling_rate=feature_extractor.sampling_rate, \n        padding=True, truncation=True,\n        max_length=30*16000\n    )\n    return inputs\n\n\nencoded_dataset = data.map(preprocess_function, remove_columns=[\"audio\"], batched=True)\n\n\n\n\n\nencoded_dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['file', 'genre', 'input_values', 'attention_mask'],\n        num_rows: 999\n    })\n})\n\n\n\nsplit_dataset = encoded_dataset['train'].train_test_split(test_size=0.2)\n\n\ntrain_dataset = split_dataset['train']\ntrain_dataset\n\nDataset({\n    features: ['file', 'genre', 'input_values', 'attention_mask'],\n    num_rows: 799\n})\n\n\n\neval_dataset = split_dataset['test']\neval_dataset\n\nDataset({\n    features: ['file', 'genre', 'input_values', 'attention_mask'],\n    num_rows: 200\n})\n\n\n\nnum_labels = 10\nmodel = AutoModelForAudioClassification.from_pretrained(model_name,\n                                                        num_labels=num_labels,\n                                                        label2id=label2id,\n                                                        id2label=id2label)\n\n\ntotal_params = 0\nfor param in model.parameters():\n    total_params = total_params + param.numel()\ntotal_params / 1e+06\n\n94.571146\n\n\n\ntrain_dataset\n\nDataset({\n    features: ['file', 'genre', 'input_values', 'attention_mask'],\n    num_rows: 799\n})\n\n\n\ntrain_dataset = train_dataset.rename_column(\"genre\", \"label\")\neval_dataset = eval_dataset.rename_column(\"genre\", \"label\")"
  },
  {
    "objectID": "posts/audio_classification_final.html#finetune-train-the-model",
    "href": "posts/audio_classification_final.html#finetune-train-the-model",
    "title": "Audio Classification using Transformers",
    "section": "",
    "text": "We specify training arguements using TrainingArguments class from transformers library. We will train the model for 10 epochs and use small batch size of 8 due to resource constraints.\nAfter that, we create a helper function to calculate accuracy using evaluate package.\nTraining of the model is quite easy with Trainer class from transformers library.\nWe can see that after 10 epochs, we achieve accuracy of 83% on evaluation dataset. The training loss is 0.18 and validatio loss is 0.67. This indicates that the model has overfit the data. This was kind of expected because of small size of the dataset and huge pre-trained model size.\nHowever, accuracy of 83% after training for only 10 epochs is quite good.\n\n\nmodel_name_1 = model_name.split(\"/\")[-1]\nbatch_size = 8\ngradient_accumulation_steps = 1\nnum_train_epochs = 10\n\ntraining_args = TrainingArguments(\n    f\"{model_name_1}-finetuned-gtzan\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_train_epochs,\n    warmup_ratio=0.1,\n    logging_steps=5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    fp16=True,\n    push_to_hub=False,\n)\n\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\n\ntrainer.train()\n\nwandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\n\nTracking run with wandb version 0.19.1\n\n\nRun data is saved locally in /kaggle/working/wandb/run-20250126_081553-c1u0om9e\n\n\nSyncing run wav2vec2-base-finetuned-gtzan to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/sonawane-ravindra1/huggingface\n\n\n View run at https://wandb.ai/sonawane-ravindra1/huggingface/runs/c1u0om9e\n\n\n\n    \n      \n      \n      [1000/1000 2:20:27, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n1.864500\n1.984971\n0.400000\n\n\n2\n1.537700\n1.635146\n0.395000\n\n\n3\n1.231000\n1.346450\n0.540000\n\n\n4\n0.958400\n1.088597\n0.635000\n\n\n5\n0.722100\n0.903491\n0.740000\n\n\n6\n0.817800\n0.816501\n0.775000\n\n\n7\n0.697800\n0.833347\n0.785000\n\n\n8\n0.207900\n0.850784\n0.790000\n\n\n9\n0.179600\n0.697149\n0.825000\n\n\n10\n0.177400\n0.667886\n0.830000\n\n\n\n\n\n\nTrainOutput(global_step=1000, training_loss=0.926927893102169, metrics={'train_runtime': 8447.1145, 'train_samples_per_second': 0.946, 'train_steps_per_second': 0.118, 'total_flos': 2.1761955548352e+18, 'train_loss': 0.926927893102169, 'epoch': 10.0})"
  },
  {
    "objectID": "posts/audio_classification_final.html#conclusion",
    "href": "posts/audio_classification_final.html#conclusion",
    "title": "Audio Classification using Transformers",
    "section": "",
    "text": "We learned to fine-tune a large pre-trained model to achieve reasonably good performance on audio classification task.\nIt’s amazing to see how the approach for finetuning for audio task is quite similar to that of finetuning across other modalities.\nWe noticed that our model suffered from overfitting. Here are some of the approaches that can be used to mitigate overfitting and achieve accuracy beyond 83%:\n\nData Augmentation: Number of training examples can be increased by using data augmentation. A library like audiomentations can be used for this purpose.\nRegularization: L1 or L2 regularization can be used to reduce overfitting.\nUse Smaller Model: Since the model we have used is quite big, using a smaller model might be a great idea considering the simple nature of the classificatio task.\nFine-tune few layers: In the above example, we fine-tuned the entire model (all parameters of the model). However, only finetuning last layers or only a few attention heads will help to reduce overfitting. This will also help to speed up training and will consume less resources."
  },
  {
    "objectID": "posts/audio_classification_final.html#references",
    "href": "posts/audio_classification_final.html#references",
    "title": "Audio Classification using Transformers",
    "section": "",
    "text": "Dataset\nModel"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Ravindra. I’m passionate about Data Science, Machine Learning, and Generative AI, and I believe, as Andrew Ng says, “AI is the new electricity.” This blog focuses on the practical side of these fields—how to use data and AI to solve real-world problems and drive impact. Join me as I explore hands-on projects, share insights, and dive into the tools and techniques shaping the future.\nFeel free to connect with me on LinkedIn or explore my projects on GitHub."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring the World of AI and Data Science",
    "section": "",
    "text": "Welcome to my blog, where I explore the exciting world of Data Science, Machine Learning, and Generative AI. My aim is to simplify complex concepts, share the latest trends, and inspire others to dive deeper into these transformative technologies. Explore my latest posts for insights, tutorials, and discussions on the future of AI!"
  },
  {
    "objectID": "posts/auto_encoder.html",
    "href": "posts/auto_encoder.html",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "Curse of Dimensionality: A daunting challenge that haunts data scientists and machine learning practitioners alike. As number of features in the dataset increases, it is very difficult to analyze and extract meaningful insights out of the data. Following are some challenges while dealing with high dimensional data:\n\nOverfitting: High dimensional datasets are prone to overfitting due to large number of parameters.\n\nIssue with distance metrics: Many machine learning algorithms rely on distance metrics (like Euclidean distance) to make decisions. In high-dimensional spaces, these metrics are often less reliable and tend to converge. In short, it is difficult to distinguish between close and far points.\n\nVisualization: Visualizing high dimensional data is very difficult which poses problem in identifying patterns / insights in data.\n\nSparsity: In high dimensions, often data tend to be sparse because of which it is difficult for algorithms to find out meaningful patterns from the data.\n\nSo, how to deal with high dimensional data and avoid curse of dimnensionality? This is where dimensionality reduction algorithms come to the rescue. One of the most popular and go-to dimensionality reduction algorithm is Principal Component Analysis (PCA).\nPCA transforms the high dimensional data it into a new set of features called principal components. One can select few features / principal components which capture maximum variance in the dataset and thereby reduce the dimensionality of the dataset. It is very important to note that PCA is a linear algorithm and therefore can not capture non-linear patterns in the data.\nMost of the real word high dimensional datasets are complex and contain non-linearities. Applications of PCA will be very limited in such complex datasets. Because of this non-linear dimensionality reduction algorithms are needed. Autoencoder is a deep learning based dimensionality reduction algorithm which is great at capturing complex non-linear patterns inside the data. In this blog, we will understand and implement Autonencoder for dimensionality reduction using Pytorch.\n\n\n\nAutoencoder is a neural network used for unsupervised learning. Its primary goal is to learn an efficient representation (or encoding) of input data by training the network to map the input to a lower-dimensional space and then reconstruct it back to its original form. It consists of two main components: the encoder and the decoder. The encoder compresses the input data into a compact latent representation, while the decoder reconstructs the data from this compressed representation. Apart from dimensionality reduction, Autoencoders are widely used for others tasks such as denoising, anomaly detection, feature extraction and generating new data as well.\n\nIn this blog, we will apply Autoencoder to perform dimensionality reduction of MNIST digit dataset. MNIST digit dataset contains 28X28 grayscale images of handwritten digits. In the end, we will compare the results of Autoencoder to that of PCA.\n\n\n\n\nWe will use torchinfo package for getting summary of neural network.\nWeights and Biases will be used for logging the results of neural network training.\n\n\n!pip -q install torchinfo\n\n\nfrom google.colab import userdata\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom sklearn.decomposition import PCA\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nfrom torchinfo import summary\nfrom torch.optim.lr_scheduler import StepLR\nimport wandb\nimport plotly.express as px\nwandb.login(key=userdata.get('WB_TOKEN'))\n\nwandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\nwandb: Currently logged in as: sonawane-ravindra1. Use `wandb login --relogin` to force relogin\nwandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\n\nTrue\n\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\nif torch.cuda.is_available():\n  device = 'cuda'\nelse:\n  device = 'cpu'\n\n\ndevice\n\n'cuda'\n\n\n\n\n\n\nWe load train and test dataset using torchvision package and then convert them into dataloaders.\nTrain dataset consists of 60,000 grayscale images of handwritten digits while test dataset contains 10,000 grayscale images of handwritten digits.\n\n\ntrain_data = torchvision.datasets.MNIST(root='./data', train=True, download=True,\n                                      transform=transforms.ToTensor())\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True,\n                                              num_workers=2)\n\n\nlen(train_data)\n\n60000\n\n\n\nlen(train_dataloader)\n\n938\n\n\n\nlen(test_data)\n\n10000\n\n\n\nlen(test_dataloader)\n\n157\n\n\n\n\n\n\nrandom_integers = [random.randint(0, 9999) for _ in range(9)]\n\n\nrandom_integers\n\n[1651, 5652, 80, 2806, 4588, 5011, 4612, 5760, 8162]\n\n\n\nfig, axes = plt.subplots(3, 3, figsize = (8, 5))\ncount = 0\nfor i in range(3):\n  for j in range(3):\n    image_index = random_integers[count]\n    axes[i][j].imshow(train_data[image_index][0].reshape((28, 28)), cmap='gray')\n    count = count + 1\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, We create encoder with 3 convolutional layers (with 64 filters each) and 3 fully connected layers. As you can see, the last layer in encoder has output shape of 2. The last layer in encoder is the compressed representation of the input and it is also called as latent space. The last layer in encoder is also called bottleneck.\nAfter that, we create decoder which performs opposite transformations as compared to encoder. Decoder attempts to regenerate the input by upsampling the compressed representation.\nYou can notice that we have made use of ConvTranspose2d and Unflatten layers in decoder which are inverse of Conv2d and Flatten layers used in encoder.\nGenerally, Autoencoders use MSE (Mean Square Error) loss between input and output. During training, we minimise the loss by using backpropogation. Since, we are using MSE loss, the shape of input and output must be same. In order to ensure the same shape, we have used nn.functional.interpolate method in the forward pass.\nAs it can be seen in the summary, the model has 1,331,395 parameters.\n\n\nclass AutoEncoder(nn.Module):\n  def __init__(self):\n    super(AutoEncoder, self).__init__()\n    self.encoder = nn.Sequential(\n        nn.Conv2d(1, 64, kernel_size=3, padding=1, stride=2),\n        nn.ReLU(),\n        nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2),\n        nn.ReLU(),\n        nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2),\n        nn.ReLU(),\n        nn.Flatten(),\n        nn.Linear(64 * 4 * 4, 512),\n        nn.ReLU(),\n        nn.Linear(512, 128),\n        nn.ReLU(),\n        nn.Linear(128, 2)\n    )\n\n    self.decoder = nn.Sequential(\n        nn.Linear(2, 128),\n        nn.ReLU(),\n        nn.Linear(128, 512),\n        nn.ReLU(),\n        nn.Linear(512, 64 * 4 * 4),\n        nn.Unflatten(1, (64, 4, 4)),\n        nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, output_padding=1, stride=2),\n        nn.ReLU(),\n        nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, output_padding=1, stride=2),\n        nn.ReLU(),\n        nn.ConvTranspose2d(64, 1, kernel_size=3, padding=1, output_padding=1, stride=2),\n        nn.Sigmoid()\n    )\n\n  def forward(self, x):\n    x = self.encoder(x)\n    x = self.decoder(x)\n    x = nn.functional.interpolate(x, size=(28, 28), mode='bilinear', align_corners=False)\n    return x\n\n\ntrain_data[0][0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\nmodel = AutoEncoder()\nmodel = model.to(device)\n\n\ntrain_data[0][0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\ntrain_data[0][0].unsqueeze(0).shape\n\ntorch.Size([1, 1, 28, 28])\n\n\n\nmodel.forward(train_data[0][0].unsqueeze(0).to(device)).shape\n\ntorch.Size([1, 1, 28, 28])\n\n\n\nsummary(model, input_size=(1, 1, 28, 28))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoEncoder                              [1, 1, 28, 28]            --\n├─Sequential: 1-1                        [1, 2]                    --\n│    └─Conv2d: 2-1                       [1, 64, 14, 14]           640\n│    └─ReLU: 2-2                         [1, 64, 14, 14]           --\n│    └─Conv2d: 2-3                       [1, 64, 7, 7]             36,928\n│    └─ReLU: 2-4                         [1, 64, 7, 7]             --\n│    └─Conv2d: 2-5                       [1, 64, 4, 4]             36,928\n│    └─ReLU: 2-6                         [1, 64, 4, 4]             --\n│    └─Flatten: 2-7                      [1, 1024]                 --\n│    └─Linear: 2-8                       [1, 512]                  524,800\n│    └─ReLU: 2-9                         [1, 512]                  --\n│    └─Linear: 2-10                      [1, 128]                  65,664\n│    └─ReLU: 2-11                        [1, 128]                  --\n│    └─Linear: 2-12                      [1, 2]                    258\n├─Sequential: 1-2                        [1, 1, 32, 32]            --\n│    └─Linear: 2-13                      [1, 128]                  384\n│    └─ReLU: 2-14                        [1, 128]                  --\n│    └─Linear: 2-15                      [1, 512]                  66,048\n│    └─ReLU: 2-16                        [1, 512]                  --\n│    └─Linear: 2-17                      [1, 1024]                 525,312\n│    └─Unflatten: 2-18                   [1, 64, 4, 4]             --\n│    └─ConvTranspose2d: 2-19             [1, 64, 8, 8]             36,928\n│    └─ReLU: 2-20                        [1, 64, 8, 8]             --\n│    └─ConvTranspose2d: 2-21             [1, 64, 16, 16]           36,928\n│    └─ReLU: 2-22                        [1, 64, 16, 16]           --\n│    └─ConvTranspose2d: 2-23             [1, 1, 32, 32]            577\n│    └─Sigmoid: 2-24                     [1, 1, 32, 32]            --\n==========================================================================================\nTotal params: 1,331,395\nTrainable params: 1,331,395\nNon-trainable params: 0\nTotal mult-adds (M): 16.12\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.32\nParams size (MB): 5.33\nEstimated Total Size (MB): 5.65\n==========================================================================================\n\n\n\n\n\n\nWe implement a standard training loop in Pytorch and report loss to weights & biases after processing of every 100 batches.\nWe can notice that after training for 10 epochs, training loss is 0.037 and test (validation) loss is 0.0348. Let’s see how this model performs in terms of dimensionality reduction as compared to PCA.\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n\ncriterion = nn.MSELoss()\n\n\nmodel = model.to(device)\n\n\nnumber_of_epochs = 10\nwandb.init(project=\"autoencoder\", entity=\"sonawane-ravindra1\",\n           config={})\nfor i in tqdm(range(number_of_epochs)):\n  model.train()\n  for batch_id, (data, _) in enumerate(train_dataloader):\n    optimizer.zero_grad()\n    data = data.to(device)\n    output = model(data)\n    loss = criterion(output, data)\n    loss.backward()\n    optimizer.step()\n    if batch_id % 100 == 0:\n      print(f\"Epoch [{i+1}/{number_of_epochs}], Batch [{batch_id+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n      wandb.log({'train_loss': loss.item()}, step=i*len(train_dataloader) + batch_id)\n\n  model.eval()\n  val_loss = 0\n  for data, _ in test_dataloader:\n    data = data.to(device)\n    output = model(data)\n    loss = criterion(output, data)\n    val_loss = val_loss + loss.item()\n\n  val_loss = val_loss / len(test_dataloader)\n  wandb.log({'val_loss': val_loss}, step=(i+1)*len(train_dataloader))\n  print(f\"Epoch [{i+1}/{number_of_epochs}], Validation Loss: {val_loss:.4f}\")\n\nwandb.finish()\n\nTracking run with wandb version 0.19.1\n\n\nRun data is saved locally in /content/wandb/run-20250111_153941-i7i1ug09\n\n\nSyncing run dark-shadow-7 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/sonawane-ravindra1/autoencoder\n\n\n View run at https://wandb.ai/sonawane-ravindra1/autoencoder/runs/i7i1ug09\n\n\n  0%|          | 0/10 [00:00&lt;?, ?it/s]\n\n\nEpoch [1/10], Batch [1/938], Loss: 0.2090\nEpoch [1/10], Batch [101/938], Loss: 0.0733\nEpoch [1/10], Batch [201/938], Loss: 0.0542\nEpoch [1/10], Batch [301/938], Loss: 0.0532\nEpoch [1/10], Batch [401/938], Loss: 0.0441\nEpoch [1/10], Batch [501/938], Loss: 0.0442\nEpoch [1/10], Batch [601/938], Loss: 0.0441\nEpoch [1/10], Batch [701/938], Loss: 0.0415\nEpoch [1/10], Batch [801/938], Loss: 0.0431\nEpoch [1/10], Batch [901/938], Loss: 0.0439\n\n\n 10%|█         | 1/10 [00:16&lt;02:26, 16.23s/it]\n\n\nEpoch [1/10], Validation Loss: 0.0421\nEpoch [2/10], Batch [1/938], Loss: 0.0373\nEpoch [2/10], Batch [101/938], Loss: 0.0439\nEpoch [2/10], Batch [201/938], Loss: 0.0397\nEpoch [2/10], Batch [301/938], Loss: 0.0431\nEpoch [2/10], Batch [401/938], Loss: 0.0392\nEpoch [2/10], Batch [501/938], Loss: 0.0434\nEpoch [2/10], Batch [601/938], Loss: 0.0405\nEpoch [2/10], Batch [701/938], Loss: 0.0395\nEpoch [2/10], Batch [801/938], Loss: 0.0385\nEpoch [2/10], Batch [901/938], Loss: 0.0394\n\n\n 20%|██        | 2/10 [00:29&lt;01:57, 14.71s/it]\n\n\nEpoch [2/10], Validation Loss: 0.0389\nEpoch [3/10], Batch [1/938], Loss: 0.0405\nEpoch [3/10], Batch [101/938], Loss: 0.0401\nEpoch [3/10], Batch [201/938], Loss: 0.0391\nEpoch [3/10], Batch [301/938], Loss: 0.0365\nEpoch [3/10], Batch [401/938], Loss: 0.0405\nEpoch [3/10], Batch [501/938], Loss: 0.0395\nEpoch [3/10], Batch [601/938], Loss: 0.0386\nEpoch [3/10], Batch [701/938], Loss: 0.0352\nEpoch [3/10], Batch [801/938], Loss: 0.0398\nEpoch [3/10], Batch [901/938], Loss: 0.0371\n\n\n 30%|███       | 3/10 [00:43&lt;01:40, 14.42s/it]\n\n\nEpoch [3/10], Validation Loss: 0.0375\nEpoch [4/10], Batch [1/938], Loss: 0.0368\nEpoch [4/10], Batch [101/938], Loss: 0.0377\nEpoch [4/10], Batch [201/938], Loss: 0.0380\nEpoch [4/10], Batch [301/938], Loss: 0.0391\nEpoch [4/10], Batch [401/938], Loss: 0.0362\nEpoch [4/10], Batch [501/938], Loss: 0.0386\nEpoch [4/10], Batch [601/938], Loss: 0.0381\nEpoch [4/10], Batch [701/938], Loss: 0.0324\nEpoch [4/10], Batch [801/938], Loss: 0.0353\nEpoch [4/10], Batch [901/938], Loss: 0.0391\n\n\n 40%|████      | 4/10 [00:56&lt;01:23, 13.88s/it]\n\n\nEpoch [4/10], Validation Loss: 0.0365\nEpoch [5/10], Batch [1/938], Loss: 0.0352\nEpoch [5/10], Batch [101/938], Loss: 0.0367\nEpoch [5/10], Batch [201/938], Loss: 0.0368\nEpoch [5/10], Batch [301/938], Loss: 0.0345\nEpoch [5/10], Batch [401/938], Loss: 0.0332\nEpoch [5/10], Batch [501/938], Loss: 0.0396\nEpoch [5/10], Batch [601/938], Loss: 0.0390\nEpoch [5/10], Batch [701/938], Loss: 0.0355\nEpoch [5/10], Batch [801/938], Loss: 0.0330\nEpoch [5/10], Batch [901/938], Loss: 0.0393\n\n\n 50%|█████     | 5/10 [01:11&lt;01:10, 14.15s/it]\n\n\nEpoch [5/10], Validation Loss: 0.0365\nEpoch [6/10], Batch [1/938], Loss: 0.0409\nEpoch [6/10], Batch [101/938], Loss: 0.0366\nEpoch [6/10], Batch [201/938], Loss: 0.0329\nEpoch [6/10], Batch [301/938], Loss: 0.0375\nEpoch [6/10], Batch [401/938], Loss: 0.0318\nEpoch [6/10], Batch [501/938], Loss: 0.0337\nEpoch [6/10], Batch [601/938], Loss: 0.0374\nEpoch [6/10], Batch [701/938], Loss: 0.0392\nEpoch [6/10], Batch [801/938], Loss: 0.0354\nEpoch [6/10], Batch [901/938], Loss: 0.0354\n\n\n 60%|██████    | 6/10 [01:25&lt;00:55, 13.89s/it]\n\n\nEpoch [6/10], Validation Loss: 0.0358\nEpoch [7/10], Batch [1/938], Loss: 0.0362\nEpoch [7/10], Batch [101/938], Loss: 0.0350\nEpoch [7/10], Batch [201/938], Loss: 0.0382\nEpoch [7/10], Batch [301/938], Loss: 0.0365\nEpoch [7/10], Batch [401/938], Loss: 0.0358\nEpoch [7/10], Batch [501/938], Loss: 0.0347\nEpoch [7/10], Batch [601/938], Loss: 0.0358\nEpoch [7/10], Batch [701/938], Loss: 0.0354\nEpoch [7/10], Batch [801/938], Loss: 0.0376\nEpoch [7/10], Batch [901/938], Loss: 0.0339\n\n\n 70%|███████   | 7/10 [01:37&lt;00:40, 13.44s/it]\n\n\nEpoch [7/10], Validation Loss: 0.0354\nEpoch [8/10], Batch [1/938], Loss: 0.0366\nEpoch [8/10], Batch [101/938], Loss: 0.0355\nEpoch [8/10], Batch [201/938], Loss: 0.0353\nEpoch [8/10], Batch [301/938], Loss: 0.0353\nEpoch [8/10], Batch [401/938], Loss: 0.0360\nEpoch [8/10], Batch [501/938], Loss: 0.0359\nEpoch [8/10], Batch [601/938], Loss: 0.0364\nEpoch [8/10], Batch [701/938], Loss: 0.0369\nEpoch [8/10], Batch [801/938], Loss: 0.0342\nEpoch [8/10], Batch [901/938], Loss: 0.0340\n\n\n 80%|████████  | 8/10 [01:50&lt;00:26, 13.29s/it]\n\n\nEpoch [8/10], Validation Loss: 0.0350\nEpoch [9/10], Batch [1/938], Loss: 0.0360\nEpoch [9/10], Batch [101/938], Loss: 0.0346\nEpoch [9/10], Batch [201/938], Loss: 0.0383\nEpoch [9/10], Batch [301/938], Loss: 0.0318\nEpoch [9/10], Batch [401/938], Loss: 0.0344\nEpoch [9/10], Batch [501/938], Loss: 0.0335\nEpoch [9/10], Batch [601/938], Loss: 0.0369\nEpoch [9/10], Batch [701/938], Loss: 0.0354\nEpoch [9/10], Batch [801/938], Loss: 0.0343\nEpoch [9/10], Batch [901/938], Loss: 0.0371\n\n\n 90%|█████████ | 9/10 [02:03&lt;00:13, 13.35s/it]\n\n\nEpoch [9/10], Validation Loss: 0.0350\nEpoch [10/10], Batch [1/938], Loss: 0.0330\nEpoch [10/10], Batch [101/938], Loss: 0.0333\nEpoch [10/10], Batch [201/938], Loss: 0.0368\nEpoch [10/10], Batch [301/938], Loss: 0.0333\nEpoch [10/10], Batch [401/938], Loss: 0.0333\nEpoch [10/10], Batch [501/938], Loss: 0.0365\nEpoch [10/10], Batch [601/938], Loss: 0.0354\nEpoch [10/10], Batch [701/938], Loss: 0.0334\nEpoch [10/10], Batch [801/938], Loss: 0.0372\nEpoch [10/10], Batch [901/938], Loss: 0.0370\n\n\n100%|██████████| 10/10 [02:17&lt;00:00, 13.75s/it]\n\n\nEpoch [10/10], Validation Loss: 0.0348\n\n\n\n\n\n\n\n\n    \n\n\n\ntrain_loss\n█▅▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▂▂▁▁▂▂▁▂▂▁▁▂▁▂▁▁▂▁▁▁▁▂\n\n\nval_loss\n█▅▄▃▃▂▂▁▁▁\n\n\n\n\n\n\n\ntrain_loss\n0.037\n\n\nval_loss\n0.03479\n\n\n\n\n\n\n View run dark-shadow-7 at: https://wandb.ai/sonawane-ravindra1/autoencoder/runs/i7i1ug09 View project at: https://wandb.ai/sonawane-ravindra1/autoencoderSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250111_153941-i7i1ug09/logs\n\n\n\n\n\nWe will save the trained model weights to google drive. After that, we load the weights back in another model which can be used for inference.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\nfile_path = '/content/drive/MyDrive/autoencoder_mnist_1.pth'\ntorch.save(model.state_dict(), file_path)\n\nMounted at /content/drive\n\n\n\nloaded_model = AutoEncoder()\nstate_dict = torch.load(file_path, weights_only=True)\nloaded_model.load_state_dict(state_dict)\nloaded_model.eval()\nloaded_model = loaded_model.to(device)\n\n\n\n\n\nAs we discussed earlier, Autoencoder compresses the input representation and then reconstructs it into its original form. Let’s try to visualize how the reconstruction looks like for a few examples in train and test data.\nThe Autoencoder is quite good at reconstruction of images. The generated images are blurred version of the original images. This can be attributed to simpler model / limited training.\n\n\ndef plot_original_and_generated_images(index_to_show, data, model_):\n  model_ = model_.to(device)\n  original_image = data[index_to_show][0].reshape(28, 28)\n  generated_image = model_(data[index_to_show][0].unsqueeze(0).to(device))\n  generated_image = generated_image.reshape(shape=(28, 28))\n  generated_image = generated_image.cpu().detach().numpy()\n\n  fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n  axes[0].imshow(original_image, cmap='gray')\n  axes[0].set_title('Original Image')\n\n  axes[1].imshow(generated_image, cmap='gray')\n  axes[1].set_title('Generated Image')\n\n  return fig\n\n\nindex_to_show = 10\nfig = plot_original_and_generated_images(index_to_show, train_data, loaded_model)\n\n\n\n\n\n\n\n\n\nindex_to_show = 20\nfig = plot_original_and_generated_images(index_to_show, test_data, loaded_model)\n\n\n\n\n\n\n\n\n\nindex_to_show = 100\nfig = plot_original_and_generated_images(index_to_show, test_data, loaded_model)\n\n\n\n\n\n\n\n\n\nindex_to_show = 1000\nfig = plot_original_and_generated_images(index_to_show, test_data, loaded_model)\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we plot 2D latent space of 10,000 test examples. After that, we compare it with the output of PCA with 2 principal components.\nWe can observe from the plot of Latent Space that Autoencoder has performed quite great for the dimensionality reduction task. There is slight overlap between 9 and 4. The overlap can be attributed to simpler model and limited training time (we trained for only 10 epochs).\nIn addition, the issue can also be due to the latent space being too small. We can try running a model with 3 dimensional latent space and see if it removes this issue.\nOn the other hand, PCA has not been able to correctly cluster the handwritten digits. This was expected as PCA is linear technique.\n\n\nlatent_df = pd.DataFrame()\nfor data, label in test_dataloader:\n  temp_df = pd.DataFrame()\n  data = data.to(device)\n  loaded_model = loaded_model.to(device)\n  latent_represenation = loaded_model.encoder(data)\n  temp_df[['latent_x', 'latent_y']] = latent_represenation.detach().cpu().numpy()\n  temp_df['label'] = label\n  latent_df = pd.concat([temp_df, latent_df], ignore_index=True)\nlatent_df = latent_df.reset_index(drop=True)\n\n\nlatent_df.shape\n\n(10000, 3)\n\n\n\nlatent_df.head(2)\n\n\n  \n    \n\n\n\n\n\n\nlatent_x\nlatent_y\nlabel\n\n\n\n\n0\n1.059678\n1.287576\n9\n\n\n1\n8.858256\n-4.525108\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n%matplotlib inline\nlatent_df.plot(kind='scatter', x='latent_x', y='latent_y', c='label', cmap='viridis')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlen(train_data)\n\n60000\n\n\n\ntrain_data_np = train_data.data.numpy()\ntrain_data_np = train_data_np.reshape(train_data_np.shape[0], -1)\ntest_data_np = test_data.data.numpy()\ntest_data_np = test_data_np.reshape(test_data_np.shape[0], -1)\n\n\ntrain_data_np.shape\n\n(60000, 784)\n\n\n\npca_model = PCA(n_components=2)\npca_model.fit(train_data_np)\n\nPCA(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFittedPCA(n_components=2) \n\n\n\ntest_data_with_pca = pca_model.transform(test_data_np)\ntest_data_with_pca.shape\n\n(10000, 2)\n\n\n\npca_df = pd.DataFrame()\npca_df[['pca_x', 'pca_y']] = test_data_with_pca\n\nlabel_df = pd.DataFrame()\nfor _, label in test_dataloader:\n  temp_df = pd.DataFrame()\n  temp_df['label'] = label\n  label_df = pd.concat([temp_df, label_df], ignore_index=True)\n\npca_df = pd.concat([pca_df, label_df], axis = 1)\n\n\npca_df.head(2)\n\n\n  \n    \n\n\n\n\n\n\npca_x\npca_y\nlabel\n\n\n\n\n0\n-332.271169\n-747.798716\n8\n\n\n1\n20.696962\n955.584583\n4\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\npca_df.plot(kind='scatter', x='pca_x', y='pca_y', c='label', cmap='viridis')\n\n\n\n\n\n\n\n\n\n\n\n\nWe create the Autoencoder with the same architecture as that in the earlier step and only change the latent space dimension to 3.\nNext, we wrap the training of Autoencoder inside a function for easy experimentation.\nIn this experiment, we also use step Learning Rate scheduler. It will reduce the learning as we train the model which helps in finding global minima (since the step size keep on reducing as the training progresses)\n\n\nclass AutoEncoder_1(nn.Module):\n  def __init__(self):\n    super(AutoEncoder_1, self).__init__()\n    self.encoder = nn.Sequential(\n        nn.Conv2d(1, 64, kernel_size=3, padding=1, stride=2),\n        nn.ReLU(),\n        nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2),\n        nn.ReLU(),\n        nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2),\n        nn.ReLU(),\n        nn.Flatten(),\n        nn.Linear(64 * 4 * 4, 512),\n        nn.ReLU(),\n        nn.Linear(512, 128),\n        nn.ReLU(),\n        nn.Linear(128, 3)\n    )\n\n    self.decoder = nn.Sequential(\n        nn.Linear(3, 128),\n        nn.ReLU(),\n        nn.Linear(128, 512),\n        nn.ReLU(),\n        nn.Linear(512, 64 * 4 * 4),\n        nn.ReLU(),\n        nn.Unflatten(1, (64, 4, 4)),\n        nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, output_padding=1, stride=2),\n        nn.ReLU(),\n        nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, output_padding=1, stride=2),\n        nn.ReLU(),\n        nn.ConvTranspose2d(64, 1, kernel_size=3, padding=1, output_padding=1, stride=2),\n        nn.Sigmoid()\n    )\n\n  def forward(self, x):\n    x = self.encoder(x)\n    x = self.decoder(x)\n    x = nn.functional.interpolate(x, size=(28, 28), mode='bilinear', align_corners=False)\n    return x\n\n\nmodel_1 = AutoEncoder_1()\n\n\nsummary(model_1, input_size = (1, 1, 28, 28))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoEncoder_1                            [1, 1, 28, 28]            --\n├─Sequential: 1-1                        [1, 3]                    --\n│    └─Conv2d: 2-1                       [1, 64, 14, 14]           640\n│    └─ReLU: 2-2                         [1, 64, 14, 14]           --\n│    └─Conv2d: 2-3                       [1, 64, 7, 7]             36,928\n│    └─ReLU: 2-4                         [1, 64, 7, 7]             --\n│    └─Conv2d: 2-5                       [1, 64, 4, 4]             36,928\n│    └─ReLU: 2-6                         [1, 64, 4, 4]             --\n│    └─Flatten: 2-7                      [1, 1024]                 --\n│    └─Linear: 2-8                       [1, 512]                  524,800\n│    └─ReLU: 2-9                         [1, 512]                  --\n│    └─Linear: 2-10                      [1, 128]                  65,664\n│    └─ReLU: 2-11                        [1, 128]                  --\n│    └─Linear: 2-12                      [1, 3]                    387\n├─Sequential: 1-2                        [1, 1, 32, 32]            --\n│    └─Linear: 2-13                      [1, 128]                  512\n│    └─ReLU: 2-14                        [1, 128]                  --\n│    └─Linear: 2-15                      [1, 512]                  66,048\n│    └─ReLU: 2-16                        [1, 512]                  --\n│    └─Linear: 2-17                      [1, 1024]                 525,312\n│    └─ReLU: 2-18                        [1, 1024]                 --\n│    └─Unflatten: 2-19                   [1, 64, 4, 4]             --\n│    └─ConvTranspose2d: 2-20             [1, 64, 8, 8]             36,928\n│    └─ReLU: 2-21                        [1, 64, 8, 8]             --\n│    └─ConvTranspose2d: 2-22             [1, 64, 16, 16]           36,928\n│    └─ReLU: 2-23                        [1, 64, 16, 16]           --\n│    └─ConvTranspose2d: 2-24             [1, 1, 32, 32]            577\n│    └─Sigmoid: 2-25                     [1, 1, 32, 32]            --\n==========================================================================================\nTotal params: 1,331,652\nTrainable params: 1,331,652\nNon-trainable params: 0\nTotal mult-adds (M): 16.12\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.32\nParams size (MB): 5.33\nEstimated Total Size (MB): 5.65\n==========================================================================================\n\n\n\ndef train_model(model, train_dataloader, test_dataloader, number_of_epochs, learning_rate, config={}):\n  if wandb.run is not None:\n    wandb.finish()\n  wandb.init(project=\"autoencoder\", entity=\"sonawane-ravindra1\",\n             config=config, reinit=True)\n  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n  scheduler = StepLR(optimizer, step_size=2, gamma=0.5)\n  criterion = nn.MSELoss()\n  for i in tqdm(range(number_of_epochs)):\n    model.train()\n    for batch_id, (data, _) in enumerate(train_dataloader):\n      optimizer.zero_grad()\n      data = data.to(device)\n      output = model(data)\n      loss = criterion(output, data)\n      loss.backward()\n      optimizer.step()\n      if batch_id % 100 == 0:\n        print(f\"Epoch [{i+1}/{number_of_epochs}], Batch [{batch_id+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n        wandb.log({'train_loss': loss.item(),\n                   'learning_rate': optimizer.param_groups[0]['lr']}, step=i*len(train_dataloader) + batch_id)\n\n    model.eval()\n    val_loss = 0\n    for data, _ in test_dataloader:\n      data = data.to(device)\n      output = model(data)\n      loss = criterion(output, data)\n      val_loss = val_loss + loss.item()\n\n    val_loss = val_loss / len(test_dataloader)\n    wandb.log({'val_loss': val_loss}, step=(i+1)*len(train_dataloader))\n    print(f\"Epoch [{i+1}/{number_of_epochs}], Validation Loss: {val_loss:.4f}\")\n    scheduler.step()\n\n  wandb.finish()\n  return model\n\n\nmodel_1 = train_model(model_1, train_dataloader, test_dataloader, number_of_epochs=10,\n                      learning_rate=0.001, config={})\n\n\n\n\n    \n\n\n\nlearning_rate\n▁▁▁▁▁▁▁▁▁▁\n\n\ntrain_loss\n█▂▂▁▁▁▁▁▁▁\n\n\n\n\n\n\n\nlearning_rate\n0.001\n\n\ntrain_loss\n0.03555\n\n\n\n\n\n\n View run driven-morning-9 at: https://wandb.ai/sonawane-ravindra1/autoencoder/runs/idx5qrs2 View project at: https://wandb.ai/sonawane-ravindra1/autoencoderSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250111_155635-idx5qrs2/logs\n\n\nTracking run with wandb version 0.19.1\n\n\nRun data is saved locally in /content/wandb/run-20250111_155755-3zb2ks0n\n\n\nSyncing run clean-darkness-10 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/sonawane-ravindra1/autoencoder\n\n\n View run at https://wandb.ai/sonawane-ravindra1/autoencoder/runs/3zb2ks0n\n\n\n  0%|          | 0/10 [00:00&lt;?, ?it/s]\n\n\nEpoch [1/10], Batch [1/938], Loss: 0.2287\nEpoch [1/10], Batch [101/938], Loss: 0.0722\nEpoch [1/10], Batch [201/938], Loss: 0.0599\nEpoch [1/10], Batch [301/938], Loss: 0.0550\nEpoch [1/10], Batch [401/938], Loss: 0.0487\nEpoch [1/10], Batch [501/938], Loss: 0.0459\nEpoch [1/10], Batch [601/938], Loss: 0.0441\nEpoch [1/10], Batch [701/938], Loss: 0.0420\nEpoch [1/10], Batch [801/938], Loss: 0.0428\nEpoch [1/10], Batch [901/938], Loss: 0.0391\n\n\n 10%|█         | 1/10 [00:14&lt;02:10, 14.45s/it]\n\n\nEpoch [1/10], Validation Loss: 0.0386\nEpoch [2/10], Batch [1/938], Loss: 0.0382\nEpoch [2/10], Batch [101/938], Loss: 0.0351\nEpoch [2/10], Batch [201/938], Loss: 0.0375\nEpoch [2/10], Batch [301/938], Loss: 0.0378\nEpoch [2/10], Batch [401/938], Loss: 0.0343\nEpoch [2/10], Batch [501/938], Loss: 0.0352\nEpoch [2/10], Batch [601/938], Loss: 0.0360\nEpoch [2/10], Batch [701/938], Loss: 0.0400\nEpoch [2/10], Batch [801/938], Loss: 0.0368\nEpoch [2/10], Batch [901/938], Loss: 0.0334\n\n\n 20%|██        | 2/10 [00:27&lt;01:48, 13.59s/it]\n\n\nEpoch [2/10], Validation Loss: 0.0342\nEpoch [3/10], Batch [1/938], Loss: 0.0329\nEpoch [3/10], Batch [101/938], Loss: 0.0368\nEpoch [3/10], Batch [201/938], Loss: 0.0304\nEpoch [3/10], Batch [301/938], Loss: 0.0319\nEpoch [3/10], Batch [401/938], Loss: 0.0370\nEpoch [3/10], Batch [501/938], Loss: 0.0341\nEpoch [3/10], Batch [601/938], Loss: 0.0367\nEpoch [3/10], Batch [701/938], Loss: 0.0343\nEpoch [3/10], Batch [801/938], Loss: 0.0313\nEpoch [3/10], Batch [901/938], Loss: 0.0298\n\n\n 30%|███       | 3/10 [00:40&lt;01:34, 13.51s/it]\n\n\nEpoch [3/10], Validation Loss: 0.0328\nEpoch [4/10], Batch [1/938], Loss: 0.0346\nEpoch [4/10], Batch [101/938], Loss: 0.0332\nEpoch [4/10], Batch [201/938], Loss: 0.0330\nEpoch [4/10], Batch [301/938], Loss: 0.0324\nEpoch [4/10], Batch [401/938], Loss: 0.0321\nEpoch [4/10], Batch [501/938], Loss: 0.0341\nEpoch [4/10], Batch [601/938], Loss: 0.0306\nEpoch [4/10], Batch [701/938], Loss: 0.0333\nEpoch [4/10], Batch [801/938], Loss: 0.0319\nEpoch [4/10], Batch [901/938], Loss: 0.0319\n\n\n 40%|████      | 4/10 [00:55&lt;01:22, 13.82s/it]\n\n\nEpoch [4/10], Validation Loss: 0.0322\nEpoch [5/10], Batch [1/938], Loss: 0.0338\nEpoch [5/10], Batch [101/938], Loss: 0.0334\nEpoch [5/10], Batch [201/938], Loss: 0.0336\nEpoch [5/10], Batch [301/938], Loss: 0.0286\nEpoch [5/10], Batch [401/938], Loss: 0.0347\nEpoch [5/10], Batch [501/938], Loss: 0.0310\nEpoch [5/10], Batch [601/938], Loss: 0.0310\nEpoch [5/10], Batch [701/938], Loss: 0.0335\nEpoch [5/10], Batch [801/938], Loss: 0.0324\nEpoch [5/10], Batch [901/938], Loss: 0.0296\n\n\n 50%|█████     | 5/10 [01:09&lt;01:10, 14.02s/it]\n\n\nEpoch [5/10], Validation Loss: 0.0310\nEpoch [6/10], Batch [1/938], Loss: 0.0292\nEpoch [6/10], Batch [101/938], Loss: 0.0289\nEpoch [6/10], Batch [201/938], Loss: 0.0346\nEpoch [6/10], Batch [301/938], Loss: 0.0308\nEpoch [6/10], Batch [401/938], Loss: 0.0305\nEpoch [6/10], Batch [501/938], Loss: 0.0363\nEpoch [6/10], Batch [601/938], Loss: 0.0279\nEpoch [6/10], Batch [701/938], Loss: 0.0316\nEpoch [6/10], Batch [801/938], Loss: 0.0299\nEpoch [6/10], Batch [901/938], Loss: 0.0293\n\n\n 60%|██████    | 6/10 [01:23&lt;00:56, 14.04s/it]\n\n\nEpoch [6/10], Validation Loss: 0.0308\nEpoch [7/10], Batch [1/938], Loss: 0.0309\nEpoch [7/10], Batch [101/938], Loss: 0.0281\nEpoch [7/10], Batch [201/938], Loss: 0.0323\nEpoch [7/10], Batch [301/938], Loss: 0.0318\nEpoch [7/10], Batch [401/938], Loss: 0.0308\nEpoch [7/10], Batch [501/938], Loss: 0.0306\nEpoch [7/10], Batch [601/938], Loss: 0.0303\nEpoch [7/10], Batch [701/938], Loss: 0.0311\nEpoch [7/10], Batch [801/938], Loss: 0.0276\nEpoch [7/10], Batch [901/938], Loss: 0.0299\n\n\n 70%|███████   | 7/10 [01:40&lt;00:45, 15.04s/it]\n\n\nEpoch [7/10], Validation Loss: 0.0306\nEpoch [8/10], Batch [1/938], Loss: 0.0286\nEpoch [8/10], Batch [101/938], Loss: 0.0298\nEpoch [8/10], Batch [201/938], Loss: 0.0307\nEpoch [8/10], Batch [301/938], Loss: 0.0291\nEpoch [8/10], Batch [401/938], Loss: 0.0283\nEpoch [8/10], Batch [501/938], Loss: 0.0259\nEpoch [8/10], Batch [601/938], Loss: 0.0289\nEpoch [8/10], Batch [701/938], Loss: 0.0307\nEpoch [8/10], Batch [801/938], Loss: 0.0284\nEpoch [8/10], Batch [901/938], Loss: 0.0265\n\n\n 80%|████████  | 8/10 [01:54&lt;00:29, 14.72s/it]\n\n\nEpoch [8/10], Validation Loss: 0.0304\nEpoch [9/10], Batch [1/938], Loss: 0.0295\nEpoch [9/10], Batch [101/938], Loss: 0.0314\nEpoch [9/10], Batch [201/938], Loss: 0.0318\nEpoch [9/10], Batch [301/938], Loss: 0.0295\nEpoch [9/10], Batch [401/938], Loss: 0.0301\nEpoch [9/10], Batch [501/938], Loss: 0.0321\nEpoch [9/10], Batch [601/938], Loss: 0.0330\nEpoch [9/10], Batch [701/938], Loss: 0.0280\nEpoch [9/10], Batch [801/938], Loss: 0.0299\nEpoch [9/10], Batch [901/938], Loss: 0.0288\n\n\n 90%|█████████ | 9/10 [02:07&lt;00:14, 14.17s/it]\n\n\nEpoch [9/10], Validation Loss: 0.0302\nEpoch [10/10], Batch [1/938], Loss: 0.0279\nEpoch [10/10], Batch [101/938], Loss: 0.0297\nEpoch [10/10], Batch [201/938], Loss: 0.0277\nEpoch [10/10], Batch [301/938], Loss: 0.0312\nEpoch [10/10], Batch [401/938], Loss: 0.0282\nEpoch [10/10], Batch [501/938], Loss: 0.0309\nEpoch [10/10], Batch [601/938], Loss: 0.0329\nEpoch [10/10], Batch [701/938], Loss: 0.0300\nEpoch [10/10], Batch [801/938], Loss: 0.0292\nEpoch [10/10], Batch [901/938], Loss: 0.0310\n\n\n100%|██████████| 10/10 [02:21&lt;00:00, 14.12s/it]\n\n\nEpoch [10/10], Validation Loss: 0.0302\n\n\n\n\n\n\n\n\n    \n\n\n\nlearning_rate\n██████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\ntrain_loss\n█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nval_loss\n█▄▃▃▂▂▁▁▁▁\n\n\n\n\n\n\n\nlearning_rate\n6e-05\n\n\ntrain_loss\n0.03103\n\n\nval_loss\n0.03018\n\n\n\n\n\n\n View run clean-darkness-10 at: https://wandb.ai/sonawane-ravindra1/autoencoder/runs/3zb2ks0n View project at: https://wandb.ai/sonawane-ravindra1/autoencoderSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250111_155755-3zb2ks0n/logs\n\n\n\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\nfile_path = '/content/drive/MyDrive/autoencoder_mnist_2.pth'\ntorch.save(model_1.state_dict(), file_path)\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\nloaded_model_1 = AutoEncoder_1()\nstate_dict = torch.load(file_path, weights_only=True)\nloaded_model_1.load_state_dict(state_dict)\nloaded_model_1 = loaded_model_1.to(device)\nloaded_model_1.eval()\n\nAutoEncoder_1(\n  (encoder): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (5): ReLU()\n    (6): Flatten(start_dim=1, end_dim=-1)\n    (7): Linear(in_features=1024, out_features=512, bias=True)\n    (8): ReLU()\n    (9): Linear(in_features=512, out_features=128, bias=True)\n    (10): ReLU()\n    (11): Linear(in_features=128, out_features=3, bias=True)\n  )\n  (decoder): Sequential(\n    (0): Linear(in_features=3, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=1024, bias=True)\n    (5): ReLU()\n    (6): Unflatten(dim=1, unflattened_size=(64, 4, 4))\n    (7): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (8): ReLU()\n    (9): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (10): ReLU()\n    (11): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (12): Sigmoid()\n  )\n)\n\n\n\n\n\n\nAs we can see, the clusters are well separated in latent space.\nPCA with 3 principal components is also not able to correctly cluster the digits.\n\n\nlatent_df_1 = pd.DataFrame()\nfor data, label in test_dataloader:\n  temp_df = pd.DataFrame()\n  data = data.to(device)\n  latent_represenation = loaded_model_1.encoder(data)\n  temp_df[['latent_x', 'latent_y', 'latent_z']] = latent_represenation.detach().cpu().numpy()\n  temp_df['label'] = label\n  latent_df_1 = pd.concat([temp_df, latent_df_1], ignore_index=True)\nlatent_df_1 = latent_df_1.reset_index(drop=True)\n\n\nlatent_df_1.head(2)\n\n\n  \n    \n\n\n\n\n\n\nlatent_x\nlatent_y\nlatent_z\nlabel\n\n\n\n\n0\n1.033129\n-0.914918\n-1.792805\n3\n\n\n1\n0.271146\n-0.705786\n-1.259117\n3\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nfig = px.scatter_3d(x=latent_df_1['latent_x'], y=latent_df_1['latent_y'],\n                    z=latent_df_1['latent_z'], color=latent_df_1['label'],\n                    width=800, height=600,\n                    opacity=0.7, )\nfig.show()\n\n\n\npca_model_1 = PCA(n_components=3)\npca_model_1.fit(train_data_np)\n\nPCA(n_components=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFittedPCA(n_components=3) \n\n\n\ntest_data_with_pca_1 = pca_model_1.transform(test_data_np)\n\n\ntest_data_with_pca_1.shape\n\n(10000, 3)\n\n\n\npca_df_1 = pd.DataFrame()\npca_df_1[['pca_x', 'pca_y', 'pca_z']] = test_data_with_pca_1\npca_df_1.head(2)\n\n\n  \n    \n\n\n\n\n\n\npca_x\npca_y\npca_z\n\n\n\n\n0\n-332.271169\n-747.798716\n42.844819\n\n\n1\n20.696962\n955.584583\n152.715517\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\npca_df_1 = pd.concat([pca_df_1, label_df], axis = 1)\n\n\npca_df_1.head(2)\n\n\n  \n    \n\n\n\n\n\n\npca_x\npca_y\npca_z\nlabel\n\n\n\n\n0\n-332.271169\n-747.798716\n42.844819\n8\n\n\n1\n20.696962\n955.584583\n152.715517\n4\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nfig = px.scatter_3d(x=pca_df_1['pca_x'], y=pca_df_1['pca_y'], z=pca_df_1['pca_z'], color=pca_df_1['label'])\nfig.show()\n\n\n\n\n\n\nAutoencoders is a deep learning based unsupervised algorithm that can be used for dimensionality reduction.\nWhile PCA is one of the most widely used technique for dimensionality reduction, one must be aware of it’s limitations.\nAutoencoders are widely used for tasks other than dimensionality reduction such as:\n\nDenoising: If we pass noisy input / images as input, Autoencoder provides denoised data / images as output.\nAnamoly detection: If anamolous data is passed to the Autoencoder, the reconstruction loss would be quite high which would be indicative of anamolous data.\nImage / data generation: A special type of Autoencoders called as Variational Autoencoders (VAE) are used to generate novel data / images."
  },
  {
    "objectID": "posts/auto_encoder.html#curse-of-dimensionality-dimensionality-reduction",
    "href": "posts/auto_encoder.html#curse-of-dimensionality-dimensionality-reduction",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "Curse of Dimensionality: A daunting challenge that haunts data scientists and machine learning practitioners alike. As number of features in the dataset increases, it is very difficult to analyze and extract meaningful insights out of the data. Following are some challenges while dealing with high dimensional data:\n\nOverfitting: High dimensional datasets are prone to overfitting due to large number of parameters.\n\nIssue with distance metrics: Many machine learning algorithms rely on distance metrics (like Euclidean distance) to make decisions. In high-dimensional spaces, these metrics are often less reliable and tend to converge. In short, it is difficult to distinguish between close and far points.\n\nVisualization: Visualizing high dimensional data is very difficult which poses problem in identifying patterns / insights in data.\n\nSparsity: In high dimensions, often data tend to be sparse because of which it is difficult for algorithms to find out meaningful patterns from the data.\n\nSo, how to deal with high dimensional data and avoid curse of dimnensionality? This is where dimensionality reduction algorithms come to the rescue. One of the most popular and go-to dimensionality reduction algorithm is Principal Component Analysis (PCA).\nPCA transforms the high dimensional data it into a new set of features called principal components. One can select few features / principal components which capture maximum variance in the dataset and thereby reduce the dimensionality of the dataset. It is very important to note that PCA is a linear algorithm and therefore can not capture non-linear patterns in the data.\nMost of the real word high dimensional datasets are complex and contain non-linearities. Applications of PCA will be very limited in such complex datasets. Because of this non-linear dimensionality reduction algorithms are needed. Autoencoder is a deep learning based dimensionality reduction algorithm which is great at capturing complex non-linear patterns inside the data. In this blog, we will understand and implement Autonencoder for dimensionality reduction using Pytorch."
  },
  {
    "objectID": "posts/auto_encoder.html#what-is-autoencoder",
    "href": "posts/auto_encoder.html#what-is-autoencoder",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "Autoencoder is a neural network used for unsupervised learning. Its primary goal is to learn an efficient representation (or encoding) of input data by training the network to map the input to a lower-dimensional space and then reconstruct it back to its original form. It consists of two main components: the encoder and the decoder. The encoder compresses the input data into a compact latent representation, while the decoder reconstructs the data from this compressed representation. Apart from dimensionality reduction, Autoencoders are widely used for others tasks such as denoising, anomaly detection, feature extraction and generating new data as well.\n\nIn this blog, we will apply Autoencoder to perform dimensionality reduction of MNIST digit dataset. MNIST digit dataset contains 28X28 grayscale images of handwritten digits. In the end, we will compare the results of Autoencoder to that of PCA."
  },
  {
    "objectID": "posts/auto_encoder.html#import-packages",
    "href": "posts/auto_encoder.html#import-packages",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "We will use torchinfo package for getting summary of neural network.\nWeights and Biases will be used for logging the results of neural network training.\n\n\n!pip -q install torchinfo\n\n\nfrom google.colab import userdata\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom sklearn.decomposition import PCA\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nfrom torchinfo import summary\nfrom torch.optim.lr_scheduler import StepLR\nimport wandb\nimport plotly.express as px\nwandb.login(key=userdata.get('WB_TOKEN'))\n\nwandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\nwandb: Currently logged in as: sonawane-ravindra1. Use `wandb login --relogin` to force relogin\nwandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\n\nTrue\n\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\nif torch.cuda.is_available():\n  device = 'cuda'\nelse:\n  device = 'cpu'\n\n\ndevice\n\n'cuda'"
  },
  {
    "objectID": "posts/auto_encoder.html#load-data",
    "href": "posts/auto_encoder.html#load-data",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "We load train and test dataset using torchvision package and then convert them into dataloaders.\nTrain dataset consists of 60,000 grayscale images of handwritten digits while test dataset contains 10,000 grayscale images of handwritten digits.\n\n\ntrain_data = torchvision.datasets.MNIST(root='./data', train=True, download=True,\n                                      transform=transforms.ToTensor())\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True,\n                                              num_workers=2)\n\n\nlen(train_data)\n\n60000\n\n\n\nlen(train_dataloader)\n\n938\n\n\n\nlen(test_data)\n\n10000\n\n\n\nlen(test_dataloader)\n\n157"
  },
  {
    "objectID": "posts/auto_encoder.html#inspect-a-few-images",
    "href": "posts/auto_encoder.html#inspect-a-few-images",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "random_integers = [random.randint(0, 9999) for _ in range(9)]\n\n\nrandom_integers\n\n[1651, 5652, 80, 2806, 4588, 5011, 4612, 5760, 8162]\n\n\n\nfig, axes = plt.subplots(3, 3, figsize = (8, 5))\ncount = 0\nfor i in range(3):\n  for j in range(3):\n    image_index = random_integers[count]\n    axes[i][j].imshow(train_data[image_index][0].reshape((28, 28)), cmap='gray')\n    count = count + 1"
  },
  {
    "objectID": "posts/auto_encoder.html#create-a-neural-network-architecture-of-autoencoder",
    "href": "posts/auto_encoder.html#create-a-neural-network-architecture-of-autoencoder",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "First, We create encoder with 3 convolutional layers (with 64 filters each) and 3 fully connected layers. As you can see, the last layer in encoder has output shape of 2. The last layer in encoder is the compressed representation of the input and it is also called as latent space. The last layer in encoder is also called bottleneck.\nAfter that, we create decoder which performs opposite transformations as compared to encoder. Decoder attempts to regenerate the input by upsampling the compressed representation.\nYou can notice that we have made use of ConvTranspose2d and Unflatten layers in decoder which are inverse of Conv2d and Flatten layers used in encoder.\nGenerally, Autoencoders use MSE (Mean Square Error) loss between input and output. During training, we minimise the loss by using backpropogation. Since, we are using MSE loss, the shape of input and output must be same. In order to ensure the same shape, we have used nn.functional.interpolate method in the forward pass.\nAs it can be seen in the summary, the model has 1,331,395 parameters.\n\n\nclass AutoEncoder(nn.Module):\n  def __init__(self):\n    super(AutoEncoder, self).__init__()\n    self.encoder = nn.Sequential(\n        nn.Conv2d(1, 64, kernel_size=3, padding=1, stride=2),\n        nn.ReLU(),\n        nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2),\n        nn.ReLU(),\n        nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2),\n        nn.ReLU(),\n        nn.Flatten(),\n        nn.Linear(64 * 4 * 4, 512),\n        nn.ReLU(),\n        nn.Linear(512, 128),\n        nn.ReLU(),\n        nn.Linear(128, 2)\n    )\n\n    self.decoder = nn.Sequential(\n        nn.Linear(2, 128),\n        nn.ReLU(),\n        nn.Linear(128, 512),\n        nn.ReLU(),\n        nn.Linear(512, 64 * 4 * 4),\n        nn.Unflatten(1, (64, 4, 4)),\n        nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, output_padding=1, stride=2),\n        nn.ReLU(),\n        nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, output_padding=1, stride=2),\n        nn.ReLU(),\n        nn.ConvTranspose2d(64, 1, kernel_size=3, padding=1, output_padding=1, stride=2),\n        nn.Sigmoid()\n    )\n\n  def forward(self, x):\n    x = self.encoder(x)\n    x = self.decoder(x)\n    x = nn.functional.interpolate(x, size=(28, 28), mode='bilinear', align_corners=False)\n    return x\n\n\ntrain_data[0][0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\nmodel = AutoEncoder()\nmodel = model.to(device)\n\n\ntrain_data[0][0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\ntrain_data[0][0].unsqueeze(0).shape\n\ntorch.Size([1, 1, 28, 28])\n\n\n\nmodel.forward(train_data[0][0].unsqueeze(0).to(device)).shape\n\ntorch.Size([1, 1, 28, 28])\n\n\n\nsummary(model, input_size=(1, 1, 28, 28))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoEncoder                              [1, 1, 28, 28]            --\n├─Sequential: 1-1                        [1, 2]                    --\n│    └─Conv2d: 2-1                       [1, 64, 14, 14]           640\n│    └─ReLU: 2-2                         [1, 64, 14, 14]           --\n│    └─Conv2d: 2-3                       [1, 64, 7, 7]             36,928\n│    └─ReLU: 2-4                         [1, 64, 7, 7]             --\n│    └─Conv2d: 2-5                       [1, 64, 4, 4]             36,928\n│    └─ReLU: 2-6                         [1, 64, 4, 4]             --\n│    └─Flatten: 2-7                      [1, 1024]                 --\n│    └─Linear: 2-8                       [1, 512]                  524,800\n│    └─ReLU: 2-9                         [1, 512]                  --\n│    └─Linear: 2-10                      [1, 128]                  65,664\n│    └─ReLU: 2-11                        [1, 128]                  --\n│    └─Linear: 2-12                      [1, 2]                    258\n├─Sequential: 1-2                        [1, 1, 32, 32]            --\n│    └─Linear: 2-13                      [1, 128]                  384\n│    └─ReLU: 2-14                        [1, 128]                  --\n│    └─Linear: 2-15                      [1, 512]                  66,048\n│    └─ReLU: 2-16                        [1, 512]                  --\n│    └─Linear: 2-17                      [1, 1024]                 525,312\n│    └─Unflatten: 2-18                   [1, 64, 4, 4]             --\n│    └─ConvTranspose2d: 2-19             [1, 64, 8, 8]             36,928\n│    └─ReLU: 2-20                        [1, 64, 8, 8]             --\n│    └─ConvTranspose2d: 2-21             [1, 64, 16, 16]           36,928\n│    └─ReLU: 2-22                        [1, 64, 16, 16]           --\n│    └─ConvTranspose2d: 2-23             [1, 1, 32, 32]            577\n│    └─Sigmoid: 2-24                     [1, 1, 32, 32]            --\n==========================================================================================\nTotal params: 1,331,395\nTrainable params: 1,331,395\nNon-trainable params: 0\nTotal mult-adds (M): 16.12\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.32\nParams size (MB): 5.33\nEstimated Total Size (MB): 5.65\n=========================================================================================="
  },
  {
    "objectID": "posts/auto_encoder.html#lets-train-a-baseline-autoencoder",
    "href": "posts/auto_encoder.html#lets-train-a-baseline-autoencoder",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "We implement a standard training loop in Pytorch and report loss to weights & biases after processing of every 100 batches.\nWe can notice that after training for 10 epochs, training loss is 0.037 and test (validation) loss is 0.0348. Let’s see how this model performs in terms of dimensionality reduction as compared to PCA.\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n\ncriterion = nn.MSELoss()\n\n\nmodel = model.to(device)\n\n\nnumber_of_epochs = 10\nwandb.init(project=\"autoencoder\", entity=\"sonawane-ravindra1\",\n           config={})\nfor i in tqdm(range(number_of_epochs)):\n  model.train()\n  for batch_id, (data, _) in enumerate(train_dataloader):\n    optimizer.zero_grad()\n    data = data.to(device)\n    output = model(data)\n    loss = criterion(output, data)\n    loss.backward()\n    optimizer.step()\n    if batch_id % 100 == 0:\n      print(f\"Epoch [{i+1}/{number_of_epochs}], Batch [{batch_id+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n      wandb.log({'train_loss': loss.item()}, step=i*len(train_dataloader) + batch_id)\n\n  model.eval()\n  val_loss = 0\n  for data, _ in test_dataloader:\n    data = data.to(device)\n    output = model(data)\n    loss = criterion(output, data)\n    val_loss = val_loss + loss.item()\n\n  val_loss = val_loss / len(test_dataloader)\n  wandb.log({'val_loss': val_loss}, step=(i+1)*len(train_dataloader))\n  print(f\"Epoch [{i+1}/{number_of_epochs}], Validation Loss: {val_loss:.4f}\")\n\nwandb.finish()\n\nTracking run with wandb version 0.19.1\n\n\nRun data is saved locally in /content/wandb/run-20250111_153941-i7i1ug09\n\n\nSyncing run dark-shadow-7 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/sonawane-ravindra1/autoencoder\n\n\n View run at https://wandb.ai/sonawane-ravindra1/autoencoder/runs/i7i1ug09\n\n\n  0%|          | 0/10 [00:00&lt;?, ?it/s]\n\n\nEpoch [1/10], Batch [1/938], Loss: 0.2090\nEpoch [1/10], Batch [101/938], Loss: 0.0733\nEpoch [1/10], Batch [201/938], Loss: 0.0542\nEpoch [1/10], Batch [301/938], Loss: 0.0532\nEpoch [1/10], Batch [401/938], Loss: 0.0441\nEpoch [1/10], Batch [501/938], Loss: 0.0442\nEpoch [1/10], Batch [601/938], Loss: 0.0441\nEpoch [1/10], Batch [701/938], Loss: 0.0415\nEpoch [1/10], Batch [801/938], Loss: 0.0431\nEpoch [1/10], Batch [901/938], Loss: 0.0439\n\n\n 10%|█         | 1/10 [00:16&lt;02:26, 16.23s/it]\n\n\nEpoch [1/10], Validation Loss: 0.0421\nEpoch [2/10], Batch [1/938], Loss: 0.0373\nEpoch [2/10], Batch [101/938], Loss: 0.0439\nEpoch [2/10], Batch [201/938], Loss: 0.0397\nEpoch [2/10], Batch [301/938], Loss: 0.0431\nEpoch [2/10], Batch [401/938], Loss: 0.0392\nEpoch [2/10], Batch [501/938], Loss: 0.0434\nEpoch [2/10], Batch [601/938], Loss: 0.0405\nEpoch [2/10], Batch [701/938], Loss: 0.0395\nEpoch [2/10], Batch [801/938], Loss: 0.0385\nEpoch [2/10], Batch [901/938], Loss: 0.0394\n\n\n 20%|██        | 2/10 [00:29&lt;01:57, 14.71s/it]\n\n\nEpoch [2/10], Validation Loss: 0.0389\nEpoch [3/10], Batch [1/938], Loss: 0.0405\nEpoch [3/10], Batch [101/938], Loss: 0.0401\nEpoch [3/10], Batch [201/938], Loss: 0.0391\nEpoch [3/10], Batch [301/938], Loss: 0.0365\nEpoch [3/10], Batch [401/938], Loss: 0.0405\nEpoch [3/10], Batch [501/938], Loss: 0.0395\nEpoch [3/10], Batch [601/938], Loss: 0.0386\nEpoch [3/10], Batch [701/938], Loss: 0.0352\nEpoch [3/10], Batch [801/938], Loss: 0.0398\nEpoch [3/10], Batch [901/938], Loss: 0.0371\n\n\n 30%|███       | 3/10 [00:43&lt;01:40, 14.42s/it]\n\n\nEpoch [3/10], Validation Loss: 0.0375\nEpoch [4/10], Batch [1/938], Loss: 0.0368\nEpoch [4/10], Batch [101/938], Loss: 0.0377\nEpoch [4/10], Batch [201/938], Loss: 0.0380\nEpoch [4/10], Batch [301/938], Loss: 0.0391\nEpoch [4/10], Batch [401/938], Loss: 0.0362\nEpoch [4/10], Batch [501/938], Loss: 0.0386\nEpoch [4/10], Batch [601/938], Loss: 0.0381\nEpoch [4/10], Batch [701/938], Loss: 0.0324\nEpoch [4/10], Batch [801/938], Loss: 0.0353\nEpoch [4/10], Batch [901/938], Loss: 0.0391\n\n\n 40%|████      | 4/10 [00:56&lt;01:23, 13.88s/it]\n\n\nEpoch [4/10], Validation Loss: 0.0365\nEpoch [5/10], Batch [1/938], Loss: 0.0352\nEpoch [5/10], Batch [101/938], Loss: 0.0367\nEpoch [5/10], Batch [201/938], Loss: 0.0368\nEpoch [5/10], Batch [301/938], Loss: 0.0345\nEpoch [5/10], Batch [401/938], Loss: 0.0332\nEpoch [5/10], Batch [501/938], Loss: 0.0396\nEpoch [5/10], Batch [601/938], Loss: 0.0390\nEpoch [5/10], Batch [701/938], Loss: 0.0355\nEpoch [5/10], Batch [801/938], Loss: 0.0330\nEpoch [5/10], Batch [901/938], Loss: 0.0393\n\n\n 50%|█████     | 5/10 [01:11&lt;01:10, 14.15s/it]\n\n\nEpoch [5/10], Validation Loss: 0.0365\nEpoch [6/10], Batch [1/938], Loss: 0.0409\nEpoch [6/10], Batch [101/938], Loss: 0.0366\nEpoch [6/10], Batch [201/938], Loss: 0.0329\nEpoch [6/10], Batch [301/938], Loss: 0.0375\nEpoch [6/10], Batch [401/938], Loss: 0.0318\nEpoch [6/10], Batch [501/938], Loss: 0.0337\nEpoch [6/10], Batch [601/938], Loss: 0.0374\nEpoch [6/10], Batch [701/938], Loss: 0.0392\nEpoch [6/10], Batch [801/938], Loss: 0.0354\nEpoch [6/10], Batch [901/938], Loss: 0.0354\n\n\n 60%|██████    | 6/10 [01:25&lt;00:55, 13.89s/it]\n\n\nEpoch [6/10], Validation Loss: 0.0358\nEpoch [7/10], Batch [1/938], Loss: 0.0362\nEpoch [7/10], Batch [101/938], Loss: 0.0350\nEpoch [7/10], Batch [201/938], Loss: 0.0382\nEpoch [7/10], Batch [301/938], Loss: 0.0365\nEpoch [7/10], Batch [401/938], Loss: 0.0358\nEpoch [7/10], Batch [501/938], Loss: 0.0347\nEpoch [7/10], Batch [601/938], Loss: 0.0358\nEpoch [7/10], Batch [701/938], Loss: 0.0354\nEpoch [7/10], Batch [801/938], Loss: 0.0376\nEpoch [7/10], Batch [901/938], Loss: 0.0339\n\n\n 70%|███████   | 7/10 [01:37&lt;00:40, 13.44s/it]\n\n\nEpoch [7/10], Validation Loss: 0.0354\nEpoch [8/10], Batch [1/938], Loss: 0.0366\nEpoch [8/10], Batch [101/938], Loss: 0.0355\nEpoch [8/10], Batch [201/938], Loss: 0.0353\nEpoch [8/10], Batch [301/938], Loss: 0.0353\nEpoch [8/10], Batch [401/938], Loss: 0.0360\nEpoch [8/10], Batch [501/938], Loss: 0.0359\nEpoch [8/10], Batch [601/938], Loss: 0.0364\nEpoch [8/10], Batch [701/938], Loss: 0.0369\nEpoch [8/10], Batch [801/938], Loss: 0.0342\nEpoch [8/10], Batch [901/938], Loss: 0.0340\n\n\n 80%|████████  | 8/10 [01:50&lt;00:26, 13.29s/it]\n\n\nEpoch [8/10], Validation Loss: 0.0350\nEpoch [9/10], Batch [1/938], Loss: 0.0360\nEpoch [9/10], Batch [101/938], Loss: 0.0346\nEpoch [9/10], Batch [201/938], Loss: 0.0383\nEpoch [9/10], Batch [301/938], Loss: 0.0318\nEpoch [9/10], Batch [401/938], Loss: 0.0344\nEpoch [9/10], Batch [501/938], Loss: 0.0335\nEpoch [9/10], Batch [601/938], Loss: 0.0369\nEpoch [9/10], Batch [701/938], Loss: 0.0354\nEpoch [9/10], Batch [801/938], Loss: 0.0343\nEpoch [9/10], Batch [901/938], Loss: 0.0371\n\n\n 90%|█████████ | 9/10 [02:03&lt;00:13, 13.35s/it]\n\n\nEpoch [9/10], Validation Loss: 0.0350\nEpoch [10/10], Batch [1/938], Loss: 0.0330\nEpoch [10/10], Batch [101/938], Loss: 0.0333\nEpoch [10/10], Batch [201/938], Loss: 0.0368\nEpoch [10/10], Batch [301/938], Loss: 0.0333\nEpoch [10/10], Batch [401/938], Loss: 0.0333\nEpoch [10/10], Batch [501/938], Loss: 0.0365\nEpoch [10/10], Batch [601/938], Loss: 0.0354\nEpoch [10/10], Batch [701/938], Loss: 0.0334\nEpoch [10/10], Batch [801/938], Loss: 0.0372\nEpoch [10/10], Batch [901/938], Loss: 0.0370\n\n\n100%|██████████| 10/10 [02:17&lt;00:00, 13.75s/it]\n\n\nEpoch [10/10], Validation Loss: 0.0348\n\n\n\n\n\n\n\n\n    \n\n\n\ntrain_loss\n█▅▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▂▂▁▁▂▂▁▂▂▁▁▂▁▂▁▁▂▁▁▁▁▂\n\n\nval_loss\n█▅▄▃▃▂▂▁▁▁\n\n\n\n\n\n\n\ntrain_loss\n0.037\n\n\nval_loss\n0.03479\n\n\n\n\n\n\n View run dark-shadow-7 at: https://wandb.ai/sonawane-ravindra1/autoencoder/runs/i7i1ug09 View project at: https://wandb.ai/sonawane-ravindra1/autoencoderSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250111_153941-i7i1ug09/logs"
  },
  {
    "objectID": "posts/auto_encoder.html#save-and-load-the-model",
    "href": "posts/auto_encoder.html#save-and-load-the-model",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "We will save the trained model weights to google drive. After that, we load the weights back in another model which can be used for inference.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\nfile_path = '/content/drive/MyDrive/autoencoder_mnist_1.pth'\ntorch.save(model.state_dict(), file_path)\n\nMounted at /content/drive\n\n\n\nloaded_model = AutoEncoder()\nstate_dict = torch.load(file_path, weights_only=True)\nloaded_model.load_state_dict(state_dict)\nloaded_model.eval()\nloaded_model = loaded_model.to(device)"
  },
  {
    "objectID": "posts/auto_encoder.html#generate-images",
    "href": "posts/auto_encoder.html#generate-images",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "As we discussed earlier, Autoencoder compresses the input representation and then reconstructs it into its original form. Let’s try to visualize how the reconstruction looks like for a few examples in train and test data.\nThe Autoencoder is quite good at reconstruction of images. The generated images are blurred version of the original images. This can be attributed to simpler model / limited training.\n\n\ndef plot_original_and_generated_images(index_to_show, data, model_):\n  model_ = model_.to(device)\n  original_image = data[index_to_show][0].reshape(28, 28)\n  generated_image = model_(data[index_to_show][0].unsqueeze(0).to(device))\n  generated_image = generated_image.reshape(shape=(28, 28))\n  generated_image = generated_image.cpu().detach().numpy()\n\n  fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n  axes[0].imshow(original_image, cmap='gray')\n  axes[0].set_title('Original Image')\n\n  axes[1].imshow(generated_image, cmap='gray')\n  axes[1].set_title('Generated Image')\n\n  return fig\n\n\nindex_to_show = 10\nfig = plot_original_and_generated_images(index_to_show, train_data, loaded_model)\n\n\n\n\n\n\n\n\n\nindex_to_show = 20\nfig = plot_original_and_generated_images(index_to_show, test_data, loaded_model)\n\n\n\n\n\n\n\n\n\nindex_to_show = 100\nfig = plot_original_and_generated_images(index_to_show, test_data, loaded_model)\n\n\n\n\n\n\n\n\n\nindex_to_show = 1000\nfig = plot_original_and_generated_images(index_to_show, test_data, loaded_model)"
  },
  {
    "objectID": "posts/auto_encoder.html#lets-plot-the-latent-space",
    "href": "posts/auto_encoder.html#lets-plot-the-latent-space",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "In this section, we plot 2D latent space of 10,000 test examples. After that, we compare it with the output of PCA with 2 principal components.\nWe can observe from the plot of Latent Space that Autoencoder has performed quite great for the dimensionality reduction task. There is slight overlap between 9 and 4. The overlap can be attributed to simpler model and limited training time (we trained for only 10 epochs).\nIn addition, the issue can also be due to the latent space being too small. We can try running a model with 3 dimensional latent space and see if it removes this issue.\nOn the other hand, PCA has not been able to correctly cluster the handwritten digits. This was expected as PCA is linear technique.\n\n\nlatent_df = pd.DataFrame()\nfor data, label in test_dataloader:\n  temp_df = pd.DataFrame()\n  data = data.to(device)\n  loaded_model = loaded_model.to(device)\n  latent_represenation = loaded_model.encoder(data)\n  temp_df[['latent_x', 'latent_y']] = latent_represenation.detach().cpu().numpy()\n  temp_df['label'] = label\n  latent_df = pd.concat([temp_df, latent_df], ignore_index=True)\nlatent_df = latent_df.reset_index(drop=True)\n\n\nlatent_df.shape\n\n(10000, 3)\n\n\n\nlatent_df.head(2)\n\n\n  \n    \n\n\n\n\n\n\nlatent_x\nlatent_y\nlabel\n\n\n\n\n0\n1.059678\n1.287576\n9\n\n\n1\n8.858256\n-4.525108\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n%matplotlib inline\nlatent_df.plot(kind='scatter', x='latent_x', y='latent_y', c='label', cmap='viridis')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlen(train_data)\n\n60000\n\n\n\ntrain_data_np = train_data.data.numpy()\ntrain_data_np = train_data_np.reshape(train_data_np.shape[0], -1)\ntest_data_np = test_data.data.numpy()\ntest_data_np = test_data_np.reshape(test_data_np.shape[0], -1)\n\n\ntrain_data_np.shape\n\n(60000, 784)\n\n\n\npca_model = PCA(n_components=2)\npca_model.fit(train_data_np)\n\nPCA(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFittedPCA(n_components=2) \n\n\n\ntest_data_with_pca = pca_model.transform(test_data_np)\ntest_data_with_pca.shape\n\n(10000, 2)\n\n\n\npca_df = pd.DataFrame()\npca_df[['pca_x', 'pca_y']] = test_data_with_pca\n\nlabel_df = pd.DataFrame()\nfor _, label in test_dataloader:\n  temp_df = pd.DataFrame()\n  temp_df['label'] = label\n  label_df = pd.concat([temp_df, label_df], ignore_index=True)\n\npca_df = pd.concat([pca_df, label_df], axis = 1)\n\n\npca_df.head(2)\n\n\n  \n    \n\n\n\n\n\n\npca_x\npca_y\nlabel\n\n\n\n\n0\n-332.271169\n-747.798716\n8\n\n\n1\n20.696962\n955.584583\n4\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\npca_df.plot(kind='scatter', x='pca_x', y='pca_y', c='label', cmap='viridis')"
  },
  {
    "objectID": "posts/auto_encoder.html#autoencoder-with-3-dimensional-latent-space",
    "href": "posts/auto_encoder.html#autoencoder-with-3-dimensional-latent-space",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "We create the Autoencoder with the same architecture as that in the earlier step and only change the latent space dimension to 3.\nNext, we wrap the training of Autoencoder inside a function for easy experimentation.\nIn this experiment, we also use step Learning Rate scheduler. It will reduce the learning as we train the model which helps in finding global minima (since the step size keep on reducing as the training progresses)\n\n\nclass AutoEncoder_1(nn.Module):\n  def __init__(self):\n    super(AutoEncoder_1, self).__init__()\n    self.encoder = nn.Sequential(\n        nn.Conv2d(1, 64, kernel_size=3, padding=1, stride=2),\n        nn.ReLU(),\n        nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2),\n        nn.ReLU(),\n        nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2),\n        nn.ReLU(),\n        nn.Flatten(),\n        nn.Linear(64 * 4 * 4, 512),\n        nn.ReLU(),\n        nn.Linear(512, 128),\n        nn.ReLU(),\n        nn.Linear(128, 3)\n    )\n\n    self.decoder = nn.Sequential(\n        nn.Linear(3, 128),\n        nn.ReLU(),\n        nn.Linear(128, 512),\n        nn.ReLU(),\n        nn.Linear(512, 64 * 4 * 4),\n        nn.ReLU(),\n        nn.Unflatten(1, (64, 4, 4)),\n        nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, output_padding=1, stride=2),\n        nn.ReLU(),\n        nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, output_padding=1, stride=2),\n        nn.ReLU(),\n        nn.ConvTranspose2d(64, 1, kernel_size=3, padding=1, output_padding=1, stride=2),\n        nn.Sigmoid()\n    )\n\n  def forward(self, x):\n    x = self.encoder(x)\n    x = self.decoder(x)\n    x = nn.functional.interpolate(x, size=(28, 28), mode='bilinear', align_corners=False)\n    return x\n\n\nmodel_1 = AutoEncoder_1()\n\n\nsummary(model_1, input_size = (1, 1, 28, 28))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoEncoder_1                            [1, 1, 28, 28]            --\n├─Sequential: 1-1                        [1, 3]                    --\n│    └─Conv2d: 2-1                       [1, 64, 14, 14]           640\n│    └─ReLU: 2-2                         [1, 64, 14, 14]           --\n│    └─Conv2d: 2-3                       [1, 64, 7, 7]             36,928\n│    └─ReLU: 2-4                         [1, 64, 7, 7]             --\n│    └─Conv2d: 2-5                       [1, 64, 4, 4]             36,928\n│    └─ReLU: 2-6                         [1, 64, 4, 4]             --\n│    └─Flatten: 2-7                      [1, 1024]                 --\n│    └─Linear: 2-8                       [1, 512]                  524,800\n│    └─ReLU: 2-9                         [1, 512]                  --\n│    └─Linear: 2-10                      [1, 128]                  65,664\n│    └─ReLU: 2-11                        [1, 128]                  --\n│    └─Linear: 2-12                      [1, 3]                    387\n├─Sequential: 1-2                        [1, 1, 32, 32]            --\n│    └─Linear: 2-13                      [1, 128]                  512\n│    └─ReLU: 2-14                        [1, 128]                  --\n│    └─Linear: 2-15                      [1, 512]                  66,048\n│    └─ReLU: 2-16                        [1, 512]                  --\n│    └─Linear: 2-17                      [1, 1024]                 525,312\n│    └─ReLU: 2-18                        [1, 1024]                 --\n│    └─Unflatten: 2-19                   [1, 64, 4, 4]             --\n│    └─ConvTranspose2d: 2-20             [1, 64, 8, 8]             36,928\n│    └─ReLU: 2-21                        [1, 64, 8, 8]             --\n│    └─ConvTranspose2d: 2-22             [1, 64, 16, 16]           36,928\n│    └─ReLU: 2-23                        [1, 64, 16, 16]           --\n│    └─ConvTranspose2d: 2-24             [1, 1, 32, 32]            577\n│    └─Sigmoid: 2-25                     [1, 1, 32, 32]            --\n==========================================================================================\nTotal params: 1,331,652\nTrainable params: 1,331,652\nNon-trainable params: 0\nTotal mult-adds (M): 16.12\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.32\nParams size (MB): 5.33\nEstimated Total Size (MB): 5.65\n==========================================================================================\n\n\n\ndef train_model(model, train_dataloader, test_dataloader, number_of_epochs, learning_rate, config={}):\n  if wandb.run is not None:\n    wandb.finish()\n  wandb.init(project=\"autoencoder\", entity=\"sonawane-ravindra1\",\n             config=config, reinit=True)\n  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n  scheduler = StepLR(optimizer, step_size=2, gamma=0.5)\n  criterion = nn.MSELoss()\n  for i in tqdm(range(number_of_epochs)):\n    model.train()\n    for batch_id, (data, _) in enumerate(train_dataloader):\n      optimizer.zero_grad()\n      data = data.to(device)\n      output = model(data)\n      loss = criterion(output, data)\n      loss.backward()\n      optimizer.step()\n      if batch_id % 100 == 0:\n        print(f\"Epoch [{i+1}/{number_of_epochs}], Batch [{batch_id+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n        wandb.log({'train_loss': loss.item(),\n                   'learning_rate': optimizer.param_groups[0]['lr']}, step=i*len(train_dataloader) + batch_id)\n\n    model.eval()\n    val_loss = 0\n    for data, _ in test_dataloader:\n      data = data.to(device)\n      output = model(data)\n      loss = criterion(output, data)\n      val_loss = val_loss + loss.item()\n\n    val_loss = val_loss / len(test_dataloader)\n    wandb.log({'val_loss': val_loss}, step=(i+1)*len(train_dataloader))\n    print(f\"Epoch [{i+1}/{number_of_epochs}], Validation Loss: {val_loss:.4f}\")\n    scheduler.step()\n\n  wandb.finish()\n  return model\n\n\nmodel_1 = train_model(model_1, train_dataloader, test_dataloader, number_of_epochs=10,\n                      learning_rate=0.001, config={})\n\n\n\n\n    \n\n\n\nlearning_rate\n▁▁▁▁▁▁▁▁▁▁\n\n\ntrain_loss\n█▂▂▁▁▁▁▁▁▁\n\n\n\n\n\n\n\nlearning_rate\n0.001\n\n\ntrain_loss\n0.03555\n\n\n\n\n\n\n View run driven-morning-9 at: https://wandb.ai/sonawane-ravindra1/autoencoder/runs/idx5qrs2 View project at: https://wandb.ai/sonawane-ravindra1/autoencoderSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250111_155635-idx5qrs2/logs\n\n\nTracking run with wandb version 0.19.1\n\n\nRun data is saved locally in /content/wandb/run-20250111_155755-3zb2ks0n\n\n\nSyncing run clean-darkness-10 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/sonawane-ravindra1/autoencoder\n\n\n View run at https://wandb.ai/sonawane-ravindra1/autoencoder/runs/3zb2ks0n\n\n\n  0%|          | 0/10 [00:00&lt;?, ?it/s]\n\n\nEpoch [1/10], Batch [1/938], Loss: 0.2287\nEpoch [1/10], Batch [101/938], Loss: 0.0722\nEpoch [1/10], Batch [201/938], Loss: 0.0599\nEpoch [1/10], Batch [301/938], Loss: 0.0550\nEpoch [1/10], Batch [401/938], Loss: 0.0487\nEpoch [1/10], Batch [501/938], Loss: 0.0459\nEpoch [1/10], Batch [601/938], Loss: 0.0441\nEpoch [1/10], Batch [701/938], Loss: 0.0420\nEpoch [1/10], Batch [801/938], Loss: 0.0428\nEpoch [1/10], Batch [901/938], Loss: 0.0391\n\n\n 10%|█         | 1/10 [00:14&lt;02:10, 14.45s/it]\n\n\nEpoch [1/10], Validation Loss: 0.0386\nEpoch [2/10], Batch [1/938], Loss: 0.0382\nEpoch [2/10], Batch [101/938], Loss: 0.0351\nEpoch [2/10], Batch [201/938], Loss: 0.0375\nEpoch [2/10], Batch [301/938], Loss: 0.0378\nEpoch [2/10], Batch [401/938], Loss: 0.0343\nEpoch [2/10], Batch [501/938], Loss: 0.0352\nEpoch [2/10], Batch [601/938], Loss: 0.0360\nEpoch [2/10], Batch [701/938], Loss: 0.0400\nEpoch [2/10], Batch [801/938], Loss: 0.0368\nEpoch [2/10], Batch [901/938], Loss: 0.0334\n\n\n 20%|██        | 2/10 [00:27&lt;01:48, 13.59s/it]\n\n\nEpoch [2/10], Validation Loss: 0.0342\nEpoch [3/10], Batch [1/938], Loss: 0.0329\nEpoch [3/10], Batch [101/938], Loss: 0.0368\nEpoch [3/10], Batch [201/938], Loss: 0.0304\nEpoch [3/10], Batch [301/938], Loss: 0.0319\nEpoch [3/10], Batch [401/938], Loss: 0.0370\nEpoch [3/10], Batch [501/938], Loss: 0.0341\nEpoch [3/10], Batch [601/938], Loss: 0.0367\nEpoch [3/10], Batch [701/938], Loss: 0.0343\nEpoch [3/10], Batch [801/938], Loss: 0.0313\nEpoch [3/10], Batch [901/938], Loss: 0.0298\n\n\n 30%|███       | 3/10 [00:40&lt;01:34, 13.51s/it]\n\n\nEpoch [3/10], Validation Loss: 0.0328\nEpoch [4/10], Batch [1/938], Loss: 0.0346\nEpoch [4/10], Batch [101/938], Loss: 0.0332\nEpoch [4/10], Batch [201/938], Loss: 0.0330\nEpoch [4/10], Batch [301/938], Loss: 0.0324\nEpoch [4/10], Batch [401/938], Loss: 0.0321\nEpoch [4/10], Batch [501/938], Loss: 0.0341\nEpoch [4/10], Batch [601/938], Loss: 0.0306\nEpoch [4/10], Batch [701/938], Loss: 0.0333\nEpoch [4/10], Batch [801/938], Loss: 0.0319\nEpoch [4/10], Batch [901/938], Loss: 0.0319\n\n\n 40%|████      | 4/10 [00:55&lt;01:22, 13.82s/it]\n\n\nEpoch [4/10], Validation Loss: 0.0322\nEpoch [5/10], Batch [1/938], Loss: 0.0338\nEpoch [5/10], Batch [101/938], Loss: 0.0334\nEpoch [5/10], Batch [201/938], Loss: 0.0336\nEpoch [5/10], Batch [301/938], Loss: 0.0286\nEpoch [5/10], Batch [401/938], Loss: 0.0347\nEpoch [5/10], Batch [501/938], Loss: 0.0310\nEpoch [5/10], Batch [601/938], Loss: 0.0310\nEpoch [5/10], Batch [701/938], Loss: 0.0335\nEpoch [5/10], Batch [801/938], Loss: 0.0324\nEpoch [5/10], Batch [901/938], Loss: 0.0296\n\n\n 50%|█████     | 5/10 [01:09&lt;01:10, 14.02s/it]\n\n\nEpoch [5/10], Validation Loss: 0.0310\nEpoch [6/10], Batch [1/938], Loss: 0.0292\nEpoch [6/10], Batch [101/938], Loss: 0.0289\nEpoch [6/10], Batch [201/938], Loss: 0.0346\nEpoch [6/10], Batch [301/938], Loss: 0.0308\nEpoch [6/10], Batch [401/938], Loss: 0.0305\nEpoch [6/10], Batch [501/938], Loss: 0.0363\nEpoch [6/10], Batch [601/938], Loss: 0.0279\nEpoch [6/10], Batch [701/938], Loss: 0.0316\nEpoch [6/10], Batch [801/938], Loss: 0.0299\nEpoch [6/10], Batch [901/938], Loss: 0.0293\n\n\n 60%|██████    | 6/10 [01:23&lt;00:56, 14.04s/it]\n\n\nEpoch [6/10], Validation Loss: 0.0308\nEpoch [7/10], Batch [1/938], Loss: 0.0309\nEpoch [7/10], Batch [101/938], Loss: 0.0281\nEpoch [7/10], Batch [201/938], Loss: 0.0323\nEpoch [7/10], Batch [301/938], Loss: 0.0318\nEpoch [7/10], Batch [401/938], Loss: 0.0308\nEpoch [7/10], Batch [501/938], Loss: 0.0306\nEpoch [7/10], Batch [601/938], Loss: 0.0303\nEpoch [7/10], Batch [701/938], Loss: 0.0311\nEpoch [7/10], Batch [801/938], Loss: 0.0276\nEpoch [7/10], Batch [901/938], Loss: 0.0299\n\n\n 70%|███████   | 7/10 [01:40&lt;00:45, 15.04s/it]\n\n\nEpoch [7/10], Validation Loss: 0.0306\nEpoch [8/10], Batch [1/938], Loss: 0.0286\nEpoch [8/10], Batch [101/938], Loss: 0.0298\nEpoch [8/10], Batch [201/938], Loss: 0.0307\nEpoch [8/10], Batch [301/938], Loss: 0.0291\nEpoch [8/10], Batch [401/938], Loss: 0.0283\nEpoch [8/10], Batch [501/938], Loss: 0.0259\nEpoch [8/10], Batch [601/938], Loss: 0.0289\nEpoch [8/10], Batch [701/938], Loss: 0.0307\nEpoch [8/10], Batch [801/938], Loss: 0.0284\nEpoch [8/10], Batch [901/938], Loss: 0.0265\n\n\n 80%|████████  | 8/10 [01:54&lt;00:29, 14.72s/it]\n\n\nEpoch [8/10], Validation Loss: 0.0304\nEpoch [9/10], Batch [1/938], Loss: 0.0295\nEpoch [9/10], Batch [101/938], Loss: 0.0314\nEpoch [9/10], Batch [201/938], Loss: 0.0318\nEpoch [9/10], Batch [301/938], Loss: 0.0295\nEpoch [9/10], Batch [401/938], Loss: 0.0301\nEpoch [9/10], Batch [501/938], Loss: 0.0321\nEpoch [9/10], Batch [601/938], Loss: 0.0330\nEpoch [9/10], Batch [701/938], Loss: 0.0280\nEpoch [9/10], Batch [801/938], Loss: 0.0299\nEpoch [9/10], Batch [901/938], Loss: 0.0288\n\n\n 90%|█████████ | 9/10 [02:07&lt;00:14, 14.17s/it]\n\n\nEpoch [9/10], Validation Loss: 0.0302\nEpoch [10/10], Batch [1/938], Loss: 0.0279\nEpoch [10/10], Batch [101/938], Loss: 0.0297\nEpoch [10/10], Batch [201/938], Loss: 0.0277\nEpoch [10/10], Batch [301/938], Loss: 0.0312\nEpoch [10/10], Batch [401/938], Loss: 0.0282\nEpoch [10/10], Batch [501/938], Loss: 0.0309\nEpoch [10/10], Batch [601/938], Loss: 0.0329\nEpoch [10/10], Batch [701/938], Loss: 0.0300\nEpoch [10/10], Batch [801/938], Loss: 0.0292\nEpoch [10/10], Batch [901/938], Loss: 0.0310\n\n\n100%|██████████| 10/10 [02:21&lt;00:00, 14.12s/it]\n\n\nEpoch [10/10], Validation Loss: 0.0302\n\n\n\n\n\n\n\n\n    \n\n\n\nlearning_rate\n██████████▄▄▄▄▄▄▄▄▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\ntrain_loss\n█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nval_loss\n█▄▃▃▂▂▁▁▁▁\n\n\n\n\n\n\n\nlearning_rate\n6e-05\n\n\ntrain_loss\n0.03103\n\n\nval_loss\n0.03018\n\n\n\n\n\n\n View run clean-darkness-10 at: https://wandb.ai/sonawane-ravindra1/autoencoder/runs/3zb2ks0n View project at: https://wandb.ai/sonawane-ravindra1/autoencoderSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250111_155755-3zb2ks0n/logs"
  },
  {
    "objectID": "posts/auto_encoder.html#save-and-load-the-model-1",
    "href": "posts/auto_encoder.html#save-and-load-the-model-1",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "from google.colab import drive\ndrive.mount('/content/drive')\nfile_path = '/content/drive/MyDrive/autoencoder_mnist_2.pth'\ntorch.save(model_1.state_dict(), file_path)\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\nloaded_model_1 = AutoEncoder_1()\nstate_dict = torch.load(file_path, weights_only=True)\nloaded_model_1.load_state_dict(state_dict)\nloaded_model_1 = loaded_model_1.to(device)\nloaded_model_1.eval()\n\nAutoEncoder_1(\n  (encoder): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (5): ReLU()\n    (6): Flatten(start_dim=1, end_dim=-1)\n    (7): Linear(in_features=1024, out_features=512, bias=True)\n    (8): ReLU()\n    (9): Linear(in_features=512, out_features=128, bias=True)\n    (10): ReLU()\n    (11): Linear(in_features=128, out_features=3, bias=True)\n  )\n  (decoder): Sequential(\n    (0): Linear(in_features=3, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=1024, bias=True)\n    (5): ReLU()\n    (6): Unflatten(dim=1, unflattened_size=(64, 4, 4))\n    (7): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (8): ReLU()\n    (9): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (10): ReLU()\n    (11): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (12): Sigmoid()\n  )\n)"
  },
  {
    "objectID": "posts/auto_encoder.html#lets-plot-the-latent-space-1",
    "href": "posts/auto_encoder.html#lets-plot-the-latent-space-1",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "As we can see, the clusters are well separated in latent space.\nPCA with 3 principal components is also not able to correctly cluster the digits.\n\n\nlatent_df_1 = pd.DataFrame()\nfor data, label in test_dataloader:\n  temp_df = pd.DataFrame()\n  data = data.to(device)\n  latent_represenation = loaded_model_1.encoder(data)\n  temp_df[['latent_x', 'latent_y', 'latent_z']] = latent_represenation.detach().cpu().numpy()\n  temp_df['label'] = label\n  latent_df_1 = pd.concat([temp_df, latent_df_1], ignore_index=True)\nlatent_df_1 = latent_df_1.reset_index(drop=True)\n\n\nlatent_df_1.head(2)\n\n\n  \n    \n\n\n\n\n\n\nlatent_x\nlatent_y\nlatent_z\nlabel\n\n\n\n\n0\n1.033129\n-0.914918\n-1.792805\n3\n\n\n1\n0.271146\n-0.705786\n-1.259117\n3\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nfig = px.scatter_3d(x=latent_df_1['latent_x'], y=latent_df_1['latent_y'],\n                    z=latent_df_1['latent_z'], color=latent_df_1['label'],\n                    width=800, height=600,\n                    opacity=0.7, )\nfig.show()\n\n\n\npca_model_1 = PCA(n_components=3)\npca_model_1.fit(train_data_np)\n\nPCA(n_components=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFittedPCA(n_components=3) \n\n\n\ntest_data_with_pca_1 = pca_model_1.transform(test_data_np)\n\n\ntest_data_with_pca_1.shape\n\n(10000, 3)\n\n\n\npca_df_1 = pd.DataFrame()\npca_df_1[['pca_x', 'pca_y', 'pca_z']] = test_data_with_pca_1\npca_df_1.head(2)\n\n\n  \n    \n\n\n\n\n\n\npca_x\npca_y\npca_z\n\n\n\n\n0\n-332.271169\n-747.798716\n42.844819\n\n\n1\n20.696962\n955.584583\n152.715517\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\npca_df_1 = pd.concat([pca_df_1, label_df], axis = 1)\n\n\npca_df_1.head(2)\n\n\n  \n    \n\n\n\n\n\n\npca_x\npca_y\npca_z\nlabel\n\n\n\n\n0\n-332.271169\n-747.798716\n42.844819\n8\n\n\n1\n20.696962\n955.584583\n152.715517\n4\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nfig = px.scatter_3d(x=pca_df_1['pca_x'], y=pca_df_1['pca_y'], z=pca_df_1['pca_z'], color=pca_df_1['label'])\nfig.show()"
  },
  {
    "objectID": "posts/auto_encoder.html#conclusion",
    "href": "posts/auto_encoder.html#conclusion",
    "title": "Dimensionality Reduction using Autoencoders",
    "section": "",
    "text": "Autoencoders is a deep learning based unsupervised algorithm that can be used for dimensionality reduction.\nWhile PCA is one of the most widely used technique for dimensionality reduction, one must be aware of it’s limitations.\nAutoencoders are widely used for tasks other than dimensionality reduction such as:\n\nDenoising: If we pass noisy input / images as input, Autoencoder provides denoised data / images as output.\nAnamoly detection: If anamolous data is passed to the Autoencoder, the reconstruction loss would be quite high which would be indicative of anamolous data.\nImage / data generation: A special type of Autoencoders called as Variational Autoencoders (VAE) are used to generate novel data / images."
  },
  {
    "objectID": "posts/preference_alignment.html",
    "href": "posts/preference_alignment.html",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "",
    "text": "There are generally 3 stages in the LLM development lifecycle:\n\nPre-training: This is the foundation of LLM development where the model is trained on massive text dataset to predict next word in a sequence. The output of this step is a base model which has developed broad understanding of language. However, the model will not be of much help as it may just predict next token / word which might not make much sense. That’s why the next step of Supervised Fine-tuning is required.\nSupervised Fine-tuning (SFT): SFT is further training of base LLM on a smaller and more specific dataset. The dataset typicallly consists of input-output pairs. During SFT, model learns to generate outputs that are more relevant to the task / end user. Large foundation models are typically trained to follow instructions using a specific type of SFT called as instruction tuning.\nPreference Alignment: Building on the SFT model, preference alignment further refines the LLM’s behavior to ensure it aligns with human preferences or values. You must have noticed that ChatGPT (or any other foundational LLM model) deny answering harmful questions. This behaviour of the model is achieved after refining the model through Performance Alignment methods.\n\nTraditionally, Preference Alignment is achieved using Reinforcement Learning with Human Feedback (RLHF). RLHF involves training a reward model using reinforcement learning which helps model to align with human preferences. RLHF is quite complex as it requires dealing with complex reinforcement learning algorithms which are unstable. Thankfully, there are other techniques apart from RLHF which can be used for preference alignment.\nIn this blog, we will dive into hands-on implementation of one such technique which is Direct Preference Optimization (DPO)."
  },
  {
    "objectID": "posts/preference_alignment.html#what-is-preference-alignment-of-large-language-models",
    "href": "posts/preference_alignment.html#what-is-preference-alignment-of-large-language-models",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "",
    "text": "There are generally 3 stages in the LLM development lifecycle:\n\nPre-training: This is the foundation of LLM development where the model is trained on massive text dataset to predict next word in a sequence. The output of this step is a base model which has developed broad understanding of language. However, the model will not be of much help as it may just predict next token / word which might not make much sense. That’s why the next step of Supervised Fine-tuning is required.\nSupervised Fine-tuning (SFT): SFT is further training of base LLM on a smaller and more specific dataset. The dataset typicallly consists of input-output pairs. During SFT, model learns to generate outputs that are more relevant to the task / end user. Large foundation models are typically trained to follow instructions using a specific type of SFT called as instruction tuning.\nPreference Alignment: Building on the SFT model, preference alignment further refines the LLM’s behavior to ensure it aligns with human preferences or values. You must have noticed that ChatGPT (or any other foundational LLM model) deny answering harmful questions. This behaviour of the model is achieved after refining the model through Performance Alignment methods.\n\nTraditionally, Preference Alignment is achieved using Reinforcement Learning with Human Feedback (RLHF). RLHF involves training a reward model using reinforcement learning which helps model to align with human preferences. RLHF is quite complex as it requires dealing with complex reinforcement learning algorithms which are unstable. Thankfully, there are other techniques apart from RLHF which can be used for preference alignment.\nIn this blog, we will dive into hands-on implementation of one such technique which is Direct Preference Optimization (DPO)."
  },
  {
    "objectID": "posts/preference_alignment.html#dpo",
    "href": "posts/preference_alignment.html#dpo",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "DPO",
    "text": "DPO\nDPO models preference alignment as a classification problem on preference data. Preference dataset contains positive and negative pairs of prompt completion / generation. Following figure from DPO paper summarizes the difference between DPO and traditional RLHF:"
  },
  {
    "objectID": "posts/preference_alignment.html#import-packages",
    "href": "posts/preference_alignment.html#import-packages",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "Import Packages",
    "text": "Import Packages\n\nfrom datasets import load_dataset\nfrom trl import DPOConfig, DPOTrainer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model, PeftModel\nfrom trl import extract_prompt\nimport wandb"
  },
  {
    "objectID": "posts/preference_alignment.html#load-model-and-tokenizer",
    "href": "posts/preference_alignment.html#load-model-and-tokenizer",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "Load Model and Tokenizer",
    "text": "Load Model and Tokenizer\nIn this tutorial, we will use SmolLM2-360M-Instruct which will be finetuned using DPO for preference alignment. We are using a small model because of resource constraints. However, the same technique can be applied to large open models like Llama.\n\nmodel_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)"
  },
  {
    "objectID": "posts/preference_alignment.html#load-dataset",
    "href": "posts/preference_alignment.html#load-dataset",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "Load Dataset",
    "text": "Load Dataset\nWe will use a truthfullness DPO dataset from Huggingface (jondurbin/truthy-dpo-v0.1). The dataset consists of 1016 examples. Each example consists of a prompt and corresponding chosen & rejected responses. The datasets in this format are called DPO datasets. There are quite a lot of DPO datasets on Huggingface which can be explored for preference alignment of models.\n\ndataset_path = \"jondurbin/truthy-dpo-v0.1\"\n\n\ntrain_dataset = load_dataset(dataset_path, split=\"train\")\n\n\n\n\n\n\n\n\n\n\n\nlen(train_dataset)\n\n1016\n\n\n\ntrain_dataset[0]\n\n{'id': '04c275bf738fd391b7fe25e25fe7bed3',\n 'source': 'truthy_dpo',\n 'system': 'You are an unbiased, uncensored, helpful assistant.',\n 'prompt': \"What's the nearest national park to you?\",\n 'chosen': \"As an AI, I don't have a physical location, so I can't provide the distance to the nearest national park.\",\n 'rejected': \"I don't have access to the user's location, so I can't determine the nearest national park.\"}"
  },
  {
    "objectID": "posts/preference_alignment.html#generate-output-with-baseline-model",
    "href": "posts/preference_alignment.html#generate-output-with-baseline-model",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "Generate output with baseline model",
    "text": "Generate output with baseline model\n\nIn this section, we generate outputs from baseline model using different prompts from training dataset.\nWe have to apply chat template of the model before passing the prompts to model. If this step is skipped, the model fails to generate the outputs.\nAfter that, we have created a function which generates the output when model and prompts are provided. This function will be useful while testing the model after preference alignment step.\n\n\ninput_prompt = train_dataset[1]['prompt']\ninput_prompt\n\n'Do you possess the ability to navigate or move within a physical environment?'\n\n\n\nsystem_prompt = \"You are an unbiased, uncensored, helpful assistant.\"\n\n\ndevice = 'cuda'\nmodel = model.to(device)\n\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": system_prompt\n    },\n     {\"role\": \"user\",\n     \"content\": input_prompt}]\n\n\nmessages\n\n[{'role': 'system',\n  'content': 'You are an unbiased, uncensored, helpful assistant.'},\n {'role': 'user',\n  'content': 'Do you possess the ability to navigate or move within a physical environment?'}]\n\n\n\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\n\n\nprint(input_text)\n\n&lt;|im_start|&gt;system\nYou are an unbiased, uncensored, helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nDo you possess the ability to navigate or move within a physical environment?&lt;|im_end|&gt;\n\n\n\n\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n\n\ninputs\n\n{'input_ids': tensor([[    1,  9690,   198,  2683,   359,   354, 36417,    28,  4424,   581,\n          1851,    28,  5356, 11173,    30,     2,   198,     1,  4093,   198,\n          6248,   346,  5204,   260,  2470,   288,  6776,   355,  1485,  1127,\n           253,  2099,  1357,    47,     2,   198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n\n\n\nprompt_length = inputs['input_ids'].shape[1]\nprompt_length\n\n36\n\n\n\noutputs = model.generate(**inputs, max_new_tokens=100, temperature=0.2, top_p=0.9, \n                         do_sample=True)\n\n\noutputs\n\ntensor([[    1,  9690,   198,  2683,   359,   354, 36417,    28,  4424,   581,\n          1851,    28,  5356, 11173,    30,     2,   198,     1,  4093,   198,\n          6248,   346,  5204,   260,  2470,   288,  6776,   355,  1485,  1127,\n           253,  2099,  1357,    47,     2,   198,     1,   520,  9531,   198,\n            57,  5248,  2724,   288,  4237,  3629,   281, 10178,   284,  4138,\n          1127,   253,  2099,  1357,    28,   564,   339,  5248,   441,   253,\n          2099,  4313,    30,   339,   416,  1538,  5783,    28,  8939,    28,\n           284,   724,   351,  4381,    28,   564,   339,  1326,   982,   457,\n           260,  2470,   288,  8582,  1485,   355,  2298,   351,   260,  1357,\n            30,     2]], device='cuda:0')\n\n\n\noutput_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\nprint(output_text)\n\nassistant\nI'm designed to assist users in navigating and moving within a physical environment, but I'm not a physical presence. I can provide guidance, directions, and help with tasks, but I don't have the ability to physically move or interact with the environment.\n\n\n\ndef get_completion(model_, input_prompt, system_prompt, max_new_tokens=100, \n                   temperature=0.2, top_p=0.9):\n    messages = [\n    {\n        \"role\": \"system\",\n        \"content\": system_prompt\n    },\n     {\"role\": \"user\",\n     \"content\": input_prompt}]\n\n    input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n    prompt_length = inputs['input_ids'].shape[1]\n    outputs = model_.generate(**inputs, max_new_tokens=max_new_tokens, \n                              temperature=temperature, top_p=top_p, do_sample=True)\n    output_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n    return output_text\n\n\nget_completion(model, 'How are you?', system_prompt)\n\n\"assistant\\nI'm fine, thank you for asking.\""
  },
  {
    "objectID": "posts/preference_alignment.html#implicit-prompt-to-explicit-prompt-conversion",
    "href": "posts/preference_alignment.html#implicit-prompt-to-explicit-prompt-conversion",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "Implicit Prompt to Explicit Prompt Conversion",
    "text": "Implicit Prompt to Explicit Prompt Conversion\n\nDPO documentation recommends using explicit prompts. This section applies various processing steps to convert the dataset to the one with explicit prompts.\n\n\ndef preprocess_data(examples):\n  chosen_list = []\n  rejected_list = []\n\n  for i in range(len(examples['prompt'])):\n      chosen_messages = [\n          {\"role\": \"system\", \"content\": examples['system'][i]},\n          {\"role\": \"user\", \"content\": examples['prompt'][i]},\n          {\"role\": \"assistant\", \"content\": examples['chosen'][i]}\n      ]\n      chosen_list.append(chosen_messages)\n\n      rejected_messages = [\n          {\"role\": \"system\", \"content\": examples['system'][i]},\n          {\"role\": \"user\", \"content\": examples['prompt'][i]},\n          {\"role\": \"assistant\", \"content\": examples['rejected'][i]}\n      ]\n      rejected_list.append(rejected_messages)\n\n  return {'chosen': chosen_list, 'rejected': rejected_list}\n\n\ntrain_dataset = train_dataset.map(\n    preprocess_data, batched=True, remove_columns=train_dataset.column_names\n)\n\n\n\n\n\nlen(train_dataset)\n\n1016\n\n\n\ntrain_dataset[0]\n\n{'chosen': [{'content': 'You are an unbiased, uncensored, helpful assistant.',\n   'role': 'system'},\n  {'content': \"What's the nearest national park to you?\", 'role': 'user'},\n  {'content': \"As an AI, I don't have a physical location, so I can't provide the distance to the nearest national park.\",\n   'role': 'assistant'}],\n 'rejected': [{'content': 'You are an unbiased, uncensored, helpful assistant.',\n   'role': 'system'},\n  {'content': \"What's the nearest national park to you?\", 'role': 'user'},\n  {'content': \"I don't have access to the user's location, so I can't determine the nearest national park.\",\n   'role': 'assistant'}]}\n\n\n\ntrain_dataset = train_dataset.map(extract_prompt)\n\n\n\n\n\nlen(train_dataset)\n\n1016\n\n\n\ntrain_dataset[0]\n\n{'chosen': [{'content': \"As an AI, I don't have a physical location, so I can't provide the distance to the nearest national park.\",\n   'role': 'assistant'}],\n 'rejected': [{'content': \"I don't have access to the user's location, so I can't determine the nearest national park.\",\n   'role': 'assistant'}],\n 'prompt': [{'content': 'You are an unbiased, uncensored, helpful assistant.',\n   'role': 'system'},\n  {'content': \"What's the nearest national park to you?\", 'role': 'user'}]}"
  },
  {
    "objectID": "posts/preference_alignment.html#dpo-training",
    "href": "posts/preference_alignment.html#dpo-training",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "DPO Training",
    "text": "DPO Training\n\nWe will use LoRA (Low Rank Adaptation) to reduce the number of training parameters. We use LoRA with rank 16 and dropout of 0.05.\nWe use DPOConfig class from trl to specify training arguements. Note that we have used gradient accumulation and gradient checkpointing to optimize memory.\nWe have also enabled mixed precision training by using fp16=True.\nWe will train the model for 10000 steps (~20 Epochs) and log trainig information to Weights and Biases platform.\n\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n\ntraining_args = DPOConfig(output_dir=\"SmolLM_DPO\",\n                          logging_steps=50,\n                          per_device_train_batch_size=1,\n                          gradient_accumulation_steps=2,\n                          gradient_checkpointing=True,\n                          max_steps=10000,\n                          fp16=True,\n                          report_to=\"wandb\",\n                          run_name=\"dpo_training_run_04_01_2025\"\n                         )\n\n\nimport gc\nimport torch\n\ngc.collect()\ntorch.cuda.empty_cache()\n\n\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nwandb_api = user_secrets.get_secret(\"wandb_api\")\n\nwandb.login(key=wandb_api)\n\nwandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\nwandb: Currently logged in as: sonawane-ravindra1. Use `wandb login --relogin` to force relogin\nwandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\n\nTrue\n\n\n\ntrainer = DPOTrainer(model=model,\n                     args=training_args,\n                     processing_class=tokenizer,\n                     train_dataset=train_dataset,\n                     peft_config=peft_config)\n\n/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n\n\n\ntrainer.train()\n\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n\n\n\n    \n      \n      \n      [10000/10000 2:13:21, Epoch 19/20]\n    \n    \n\n\n\nStep\nTraining Loss\n\n\n\n\n50\n0.692900\n\n\n100\n0.692400\n\n\n150\n0.690500\n\n\n200\n0.688100\n\n\n250\n0.687200\n\n\n300\n0.684400\n\n\n350\n0.681400\n\n\n400\n0.680000\n\n\n450\n0.677900\n\n\n500\n0.674300\n\n\n550\n0.672300\n\n\n600\n0.669400\n\n\n650\n0.665000\n\n\n700\n0.666200\n\n\n750\n0.663600\n\n\n800\n0.661400\n\n\n850\n0.656800\n\n\n900\n0.652500\n\n\n950\n0.651500\n\n\n1000\n0.645300\n\n\n1050\n0.642700\n\n\n1100\n0.643000\n\n\n1150\n0.643100\n\n\n1200\n0.643200\n\n\n1250\n0.625300\n\n\n1300\n0.627800\n\n\n1350\n0.634200\n\n\n1400\n0.630200\n\n\n1450\n0.614600\n\n\n1500\n0.616000\n\n\n1550\n0.620700\n\n\n1600\n0.613400\n\n\n1650\n0.611600\n\n\n1700\n0.611300\n\n\n1750\n0.600100\n\n\n1800\n0.611600\n\n\n1850\n0.594400\n\n\n1900\n0.596300\n\n\n1950\n0.592500\n\n\n2000\n0.581800\n\n\n2050\n0.592800\n\n\n2100\n0.580700\n\n\n2150\n0.583900\n\n\n2200\n0.574700\n\n\n2250\n0.577000\n\n\n2300\n0.571800\n\n\n2350\n0.576200\n\n\n2400\n0.561200\n\n\n2450\n0.565000\n\n\n2500\n0.552500\n\n\n2550\n0.557500\n\n\n2600\n0.554000\n\n\n2650\n0.550700\n\n\n2700\n0.543400\n\n\n2750\n0.542900\n\n\n2800\n0.557800\n\n\n2850\n0.550100\n\n\n2900\n0.535100\n\n\n2950\n0.540100\n\n\n3000\n0.533000\n\n\n3050\n0.533200\n\n\n3100\n0.528700\n\n\n3150\n0.519800\n\n\n3200\n0.525800\n\n\n3250\n0.523500\n\n\n3300\n0.511900\n\n\n3350\n0.517800\n\n\n3400\n0.508900\n\n\n3450\n0.512400\n\n\n3500\n0.510800\n\n\n3550\n0.508700\n\n\n3600\n0.503600\n\n\n3650\n0.473900\n\n\n3700\n0.507300\n\n\n3750\n0.511000\n\n\n3800\n0.502300\n\n\n3850\n0.473700\n\n\n3900\n0.493700\n\n\n3950\n0.488600\n\n\n4000\n0.490700\n\n\n4050\n0.479700\n\n\n4100\n0.450500\n\n\n4150\n0.471400\n\n\n4200\n0.467400\n\n\n4250\n0.444900\n\n\n4300\n0.479300\n\n\n4350\n0.478300\n\n\n4400\n0.463000\n\n\n4450\n0.468400\n\n\n4500\n0.480500\n\n\n4550\n0.477000\n\n\n4600\n0.428500\n\n\n4650\n0.448800\n\n\n4700\n0.446100\n\n\n4750\n0.493100\n\n\n4800\n0.413700\n\n\n4850\n0.432800\n\n\n4900\n0.447700\n\n\n4950\n0.448900\n\n\n5000\n0.448200\n\n\n5050\n0.461600\n\n\n5100\n0.440700\n\n\n5150\n0.434600\n\n\n5200\n0.421900\n\n\n5250\n0.411700\n\n\n5300\n0.437900\n\n\n5350\n0.436900\n\n\n5400\n0.422800\n\n\n5450\n0.422200\n\n\n5500\n0.417200\n\n\n5550\n0.437100\n\n\n5600\n0.454300\n\n\n5650\n0.424100\n\n\n5700\n0.415300\n\n\n5750\n0.406200\n\n\n5800\n0.427800\n\n\n5850\n0.423100\n\n\n5900\n0.399200\n\n\n5950\n0.390800\n\n\n6000\n0.411200\n\n\n6050\n0.407200\n\n\n6100\n0.415300\n\n\n6150\n0.397000\n\n\n6200\n0.399000\n\n\n6250\n0.387400\n\n\n6300\n0.405600\n\n\n6350\n0.380900\n\n\n6400\n0.401300\n\n\n6450\n0.368700\n\n\n6500\n0.405500\n\n\n6550\n0.438500\n\n\n6600\n0.418900\n\n\n6650\n0.354100\n\n\n6700\n0.381000\n\n\n6750\n0.422500\n\n\n6800\n0.380900\n\n\n6850\n0.394400\n\n\n6900\n0.385500\n\n\n6950\n0.390400\n\n\n7000\n0.366100\n\n\n7050\n0.418700\n\n\n7100\n0.400400\n\n\n7150\n0.372200\n\n\n7200\n0.366000\n\n\n7250\n0.372500\n\n\n7300\n0.404300\n\n\n7350\n0.387300\n\n\n7400\n0.389300\n\n\n7450\n0.365100\n\n\n7500\n0.368300\n\n\n7550\n0.370500\n\n\n7600\n0.372700\n\n\n7650\n0.373000\n\n\n7700\n0.379000\n\n\n7750\n0.378800\n\n\n7800\n0.395400\n\n\n7850\n0.368500\n\n\n7900\n0.377500\n\n\n7950\n0.385700\n\n\n8000\n0.364700\n\n\n8050\n0.347300\n\n\n8100\n0.336800\n\n\n8150\n0.381300\n\n\n8200\n0.392100\n\n\n8250\n0.357100\n\n\n8300\n0.368500\n\n\n8350\n0.354200\n\n\n8400\n0.352300\n\n\n8450\n0.352100\n\n\n8500\n0.389400\n\n\n8550\n0.368600\n\n\n8600\n0.380800\n\n\n8650\n0.367500\n\n\n8700\n0.341200\n\n\n8750\n0.379900\n\n\n8800\n0.362000\n\n\n8850\n0.365600\n\n\n8900\n0.341500\n\n\n8950\n0.348200\n\n\n9000\n0.359400\n\n\n9050\n0.331900\n\n\n9100\n0.400300\n\n\n9150\n0.364700\n\n\n9200\n0.401100\n\n\n9250\n0.375100\n\n\n9300\n0.343500\n\n\n9350\n0.334200\n\n\n9400\n0.359000\n\n\n9450\n0.332500\n\n\n9500\n0.388500\n\n\n9550\n0.342000\n\n\n9600\n0.357400\n\n\n9650\n0.350000\n\n\n9700\n0.334800\n\n\n9750\n0.339600\n\n\n9800\n0.354400\n\n\n9850\n0.406500\n\n\n9900\n0.331100\n\n\n9950\n0.367000\n\n\n10000\n0.360200\n\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n\n\nTrainOutput(global_step=10000, training_loss=0.4737377624511719, metrics={'train_runtime': 8002.32, 'train_samples_per_second': 2.499, 'train_steps_per_second': 1.25, 'total_flos': 0.0, 'train_loss': 0.4737377624511719, 'epoch': 19.68503937007874})"
  },
  {
    "objectID": "posts/preference_alignment.html#inspect-training-logs",
    "href": "posts/preference_alignment.html#inspect-training-logs",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "Inspect Training Logs",
    "text": "Inspect Training Logs\n\nWe see that the training loss has decreased from 0.7 to 0.4 (which is good).\nHowever, we also need to examine 2 more plots (rewards of chosen and rejected prompts).\nAs we can see from the plots, rewards of chosen prompts is increasing and that of rejected prompts is decreasing. This indicates that the model after training is more likely to produce responses similar to chosen prompts."
  },
  {
    "objectID": "posts/preference_alignment.html#save-and-load-the-fine-tuned-model",
    "href": "posts/preference_alignment.html#save-and-load-the-fine-tuned-model",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "Save and Load the Fine-tuned Model",
    "text": "Save and Load the Fine-tuned Model\n\nWe will save the LoRA adapters of the fine-tuned model in a local directory.\nAfter that, we load the LoRA adapters and combine them with the baseline model.\nYou can notice that the fine-tuned model has several LoRA adapters in it.\n\n\nsave_directory = 'dpo_model'\n\n\ntrainer.model.save_pretrained(save_directory=save_directory)\n\n\nfrom peft import PeftModel\n\nfinetuned_model = PeftModel.from_pretrained(model, save_directory)\n\n\nfinetuned_model\n\nPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(49152, 960, padding_idx=2)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=960, out_features=960, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=960, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=960, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear(in_features=960, out_features=320, bias=False)\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=960, out_features=320, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=960, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=320, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear(in_features=960, out_features=960, bias=False)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n              (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n              (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((960,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n    )\n  )\n)"
  },
  {
    "objectID": "posts/preference_alignment.html#generate-responses-from-fine-tuned-model",
    "href": "posts/preference_alignment.html#generate-responses-from-fine-tuned-model",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "Generate Responses from Fine-tuned Model",
    "text": "Generate Responses from Fine-tuned Model\n\nWe can notice that responses from both the models are nearly identical. This is because of less training data (only 1000 examples) and training for less duration (due to resource constraints)\nHowever, we learned about how to apply DPO technique to ensure that the LLM models adhere to human preferences.\n\n\nsystem_prompt = train_dataset[0]['prompt'][0]['content']\ninput_prompt = train_dataset[0]['prompt'][1]['content']\nprint('Input Prompt is:')\nprint(input_prompt)\nprint()\nprint('Output of the finetuned model:')\nprint(get_completion(finetuned_model, input_prompt, system_prompt))\nprint()\nprint('Output of the baseline model')\nprint(get_completion(model, input_prompt, system_prompt))\n\nInput Prompt is:\nWhat's the nearest national park to you?\n\nOutput of the finetuned model:\nassistant\nI'm sorry, but as an AI, I don't have the ability to provide real-time location-based information. I'm designed to provide general knowledge and information, not specific location-based information. I recommend using a GPS device or a mapping service for that.\n\nOutput of the baseline model\nassistant\nI'm here to help you with any questions or inquiries you may have. I'm not affiliated with any national parks, so I'm not limited to their boundaries. If you have a specific question, feel free to ask, and I'll do my best to provide a helpful answer."
  },
  {
    "objectID": "posts/preference_alignment.html#conclusion",
    "href": "posts/preference_alignment.html#conclusion",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "Conclusion",
    "text": "Conclusion\n\nAlignment to human preferences / values is very important for LLM models to be safe and reliable.\nThe preference alignment has been traditionally performed using RLHF. However, RLHF is complex and quite unstable. DPO is one technique that is widely used to perform preference alignment which is simple, stable and computationally efficient.\nTRL framework developed by Huggingface provides easy APIs for implementation of DPO and many more preference alignment algorithms."
  },
  {
    "objectID": "posts/preference_alignment.html#references",
    "href": "posts/preference_alignment.html#references",
    "title": "Preference Alignment using Direct Preference Optimization (DPO)",
    "section": "References",
    "text": "References\n\nDPO Paper\nDPO Trainer documentation in trl library\nDataset used for training"
  }
]