{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLZMO3Tb6nX6"
   },
   "source": [
    "---\n",
    "title: \"Supervised Fine-Tuning Part-1\"\n",
    "date: \"2024-12-22\"\n",
    "categories: [supervised fine-tuning, LLM]\n",
    "format:\n",
    "  html: default\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning with the `trl` Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNSyTuiu7Lrn"
   },
   "source": [
    "## Introduction: What is Supervised Fine-Tuning?\n",
    "> Before understanding what SFT is, we should understand what pre-training is. Pre-training involves training a model (generally a transformer) on a large corpus of text. Pre-training allows models to learn generalizable knowledge, grammar, semantics etc. However, the model is hardly usable after pre-training; as the model lacks task specific expertize.\n",
    "\n",
    "  > That's where Supervised Training plays a part. Supervised Fine-Tuning is used to adapt a pre-trained model to a specific task. It involves training the model on a labeled dataset, where the model learns to predict the correct label for each input.\n",
    "\n",
    "- In this article, we will load a pre-trained model from HuggingFace and finetune it on a specific dataset about Python programming\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9Jd_oZEAx1q"
   },
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "id": "44fwNcOwiLVs",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install trl transformers trl datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPZSswcrEwVR"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "z5BdjW9woo74"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v57jFSFDF2mK"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9L7qKkaEzoj"
   },
   "source": [
    "## Load a Pretrained Model\n",
    "> We will load `SmolLM2-135M` model and it's tokenizer from HuggingFace. After that, we will generate output of the model on a specific input prompt related to Python (we ask the model to generate code for finding square root of a number)\n",
    "\n",
    "  > As we can see, the model did not provide correct output. The model generate irrelevant text and keep repeating that. Let's see how finetuning this model on task-specific dataset will help in increasing the model correctness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "o_6r6HFWowO_"
   },
   "outputs": [],
   "source": [
    "model_name = 'HuggingFaceTB/SmolLM2-135M'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRFwj36CptFV",
    "outputId": "c39a7ac8-e3d4-49f4-e0dc-229fd7c56deb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me a Python code for finding square root of a number.\n",
      "\n",
      "## Python Program to Find Square Root of a Number\n",
      "\n",
      "Letâ€™s see how to find square root of a number in Python.\n",
      "\n",
      "``````# Python program to find square root of a number\n",
      "\n",
      "# This program will print the square root of a number\n",
      "\n",
      "# using the built-in function square root\n",
      "\n",
      "# This program will print the square root of a number\n",
      "\n",
      "# using the built-in function square root\n",
      "\n",
      "# This program will print the square root of a number\n",
      "\n",
      "# using the built-in function square\n"
     ]
    }
   ],
   "source": [
    "input = 'Give me a Python code for finding square root of a number'\n",
    "tokenized_input = tokenizer(input, return_tensors='pt')\n",
    "output = model.generate(**tokenized_input, max_length=128)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dscrIAqGo9K"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "XepPaleuo0zl"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'jtatman/python-code-dataset-500k'\n",
    "ds = load_dataset(dataset_path, split='train[:3%]')\n",
    "ds = ds.train_test_split(test_size=0.1)\n",
    "train_dataset = ds['train']\n",
    "test_dataset = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fs2m6T3pqIMO",
    "outputId": "4e8f326c-0335-4769-a6b0-ea7bad4458fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15106"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6n29VphqLf9",
    "outputId": "e932a5b6-0712-4b40-c478-56306298abcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1679"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "k_QT6ayFpEE-"
   },
   "outputs": [],
   "source": [
    "def formatting_function(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        individual_prompt = \"###System: {}\\n###Instruction: {}\\n ###Output: {}\".format(example['system'][i],\n",
    "                                                                             example['instruction'][i],\n",
    "                                                                             example['output'][i])\n",
    "        output_texts.append(individual_prompt)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "pj-hwLq6pF2k"
   },
   "outputs": [],
   "source": [
    "response_template = \" ###Output:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "mhV2zU51pHcF"
   },
   "outputs": [],
   "source": [
    "trainer_config = SFTConfig(\n",
    "    output_dir='.\\code_finetuned_mode',\n",
    "    max_steps=400,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=10,\n",
    "    ignore_data_skip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "svYZZmLDhZdi",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "08f8a7fa-d8b4-4c1f-ee4f-1924963933f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 23:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.839400</td>\n",
       "      <td>0.743556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.731545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.715300</td>\n",
       "      <td>0.723117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.635300</td>\n",
       "      <td>0.717372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.720100</td>\n",
       "      <td>0.713097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.680100</td>\n",
       "      <td>0.710268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.711100</td>\n",
       "      <td>0.707956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.831900</td>\n",
       "      <td>0.707255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:169: UserWarning: Could not find response key ` ###Output:` in the following instance: ###System: You are a Python code architect, reviewing and designing scalable and efficient code\n",
      "###Instruction: Using the given information, write a function to calculate a 10-digit ISBN number. The ISBN number should be generated by combining various elements from the given information in a specific order.\n",
      "\n",
      "Here are the steps to generate the ISBN number:\n",
      "\n",
      "1. Take the first three letters of the author's last name (in this case, \"Doe\") and convert them to uppercase. If the author's last name has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"DOE\".\n",
      "\n",
      "2. Take the first three letters of the book title (in this case, \"Advanced\") and convert them to uppercase. If the title has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"ADV\".\n",
      "\n",
      "3. Take the first letter of the publisher's name (in this case, \"X\") and convert it to uppercase.\n",
      "\n",
      "4. Take the last two digits of the publication year (in this case, \"22\").\n",
      "\n",
      "5. Concatenate all the elements obtained in steps 1 to 4. The resulting string would be \"DOEADVX22\".\n",
      "\n",
      "6. Calculate the checksum digit of the ISBN number. To do this, multiply each digit of the concatenated string from step 5 by its position (starting from 1) and sum them up. For example, for the string \"DOEADVX22\", the calculation would be:\n",
      "   (1 * D) + (2 * O) + (3 * E) + (4 * A) + (5 * D) + (6 * V) + (7 * X) + (8 * 2) + (9 * 2) + (10 * 2)\n",
      "   Convert the result to modulo 11, and the remainder will be the checksum digit. If the remainder is 10, the checksum digit should be \"X\".\n",
      "\n",
      "7. Append the checksum digit obtained in step 6 to the end of the concatenated string from step 5. The resulting string would be the 10-digit ISBN number.\n",
      "\n",
      "Write a function in your preferred programming language that implements the above steps and returns the calculated 10-digit ISBN number.\n",
      "\n",
      "#Rewritten Test - Increased Difficulty#\n",
      "\n",
      "Using the given information, write a function to calculate a 10-digit ISBN number. The ISBN number should be generated by combining various elements from the given information in a specific order.\n",
      "\n",
      "Here are the steps to generate the ISBN number:\n",
      "\n",
      "1. Take the first three letters of the author's last name (in this case, \"Doe\") and convert them to uppercase. If the author's last name has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"DOE\".\n",
      "\n",
      "2. Take the first three letters of the book title (in this case, \"Advanced\") and convert them to uppercase. If the title has less than three letters, pad it with \"X\" until it reaches three letters. In this case, the result would be \"ADV\".\n",
      "\n",
      "3. Take the first letter of the publisher's name (in this case, \"X\") and convert it to uppercase.\n",
      "\n",
      "4. Take the last two digits of the publication year (in this case, \"22\").\n",
      "\n",
      "5. Concatenate all the elements obtained in steps 1 to 4. The resulting string would be \"DOEADVX22\".\n",
      "\n",
      "6. Calculate the checksum digit of the ISBN number. To do this, multiply each digit of the concatenated string from step 5 by its position (starting from 1) and sum them up. For example, for the string \"DOEADVX22\", the calculation would be:\n",
      "   (1 * D) + (2 * O) + (3 * E) + (4 * A) + (5 * D) + (6 * V) + (7 * X) + (8 * 2) + (9 * 2) + (10 * 2)\n",
      "   Convert the result to modulo 11, and the remainder will be the checksum digit. If the remainder is 10, the checksum digit should be \"X\".\n",
      "\n",
      "7. Append the checksum digit obtained in step 6 to the end of the concatenated string from step 5. The resulting string would be the 10-digit ISBN number.\n",
      "\n",
      "Write a function in your preferred programming language that implements the above steps and returns the calculated 10-digit ISBN number.\n",
      "\n",
      "Example Input:\n",
      "Author Last Name: \"Doe\"\n",
      "Book Title. This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.7337326669692993, metrics={'train_runtime': 1419.3064, 'train_samples_per_second': 0.564, 'train_steps_per_second': 0.282, 'total_flos': 341935195820544.0, 'train_loss': 0.7337326669692993, 'epoch': 0.052959089103667416})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=trainer_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    formatting_func=formatting_function,\n",
    "    data_collator=collator\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Pz6OaXRc1ssS"
   },
   "outputs": [],
   "source": [
    "finetune_name = 'code_model_finetuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "-nRhF79giFI3"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(f'./{finetune_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AE39IVop1199"
   },
   "source": [
    "## Load the Finetuned Model and perform prediction with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "BK1c2_rN1zvM"
   },
   "outputs": [],
   "source": [
    "finetuned_model = AutoModelForCausalLM.from_pretrained(f'./{finetune_name}', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tujqXQyb2YUB",
    "outputId": "edb989f9-2a79-4ebd-adea-155f79b20ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me a Python code for finding square root of a number.\n",
      "\n",
      "Here's the code:\n",
      "\n",
      "```python\n",
      "def square_root(num):\n",
      "    return num ** 0.5\n",
      "\n",
      "print(square_root(10))  # Output: 5\n",
      "print(square_root(20))  # Output: 10\n",
      "print(square_root(30))  # Output: 40\n",
      "```\n",
      "\n",
      "In this code, we define a function `square_root` that takes a number as input and returns its square root. We then call this function with `1\n"
     ]
    }
   ],
   "source": [
    "input = 'Give me a Python code for finding square root of a number'\n",
    "tokenized_input = tokenizer(input, return_tensors='pt')\n",
    "finetuned_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "output = finetuned_model.generate(**tokenized_input, max_length=128)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHWsXyzT2iaP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX5Q6pSN4JkB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "O9Jd_oZEAx1q",
    "zPZSswcrEwVR",
    "z9L7qKkaEzoj"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
